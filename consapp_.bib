Prog
/////////////////////////////////
@book{Joyce2019,
author={{Joyce, P.}},
Title={{Numerical C: Applied Computational Programming with Case Studies}},
year="2019",
edition={1},
publisher="Apress",
address="Berkeley, CA",
abstract="The C programming language was created in the 1970s, yet it is still in extensive use today and is the basis of many other languages. For this reason I have used C as the language for the solution of the numerical problems demonstrated in this book.",
isbn="978-1-4842-5064-8",
doi="10.1007/978-1-4842-5064-8"
}
@book{Gonzalez2024,
author={{Gonzalez-Morris, G., & Horton, I.}},
Title={{Beginning C: From Beginner to Pro}},
year="2024",
edition={7},
publisher="Apress",
address="Berkeley, CA",
isbn="979-8-8688-0148-8",
doi="10.1007/979-8-8688-0149-5"
}
@Inbook{Kaier1990,
author={{Kaier, E.}},
title="Referenz zu MS-DOS",
bookTitle={{Informatik: Referenzbuch. Mit den vollständigen Befehlslisten zu MS-DOS, Turbo Pascal, dBase und Multiplan}},
year="1990",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="1--12",
abstract="DOSSHELL als Befehl: Mit DOSSHELL wird die Men{\"u}-Oberfl{\"a}che von MS-DOS von der Befehlszeilen-Oberfl{\"a}che aus gestartet. Beispiel.",
isbn="978-3-322-89035-1",
doi="10.1007/978-3-322-89035-1_1",
url="https://doi.org/10.1007/978-3-322-89035-1_1"
}
@Inbook{Herrmann2001,
author={{Herrmann, D.}},
title="System-Programmierung (MS-DOS)",
bookTitle={{Effektiv Programmieren in C und C++: Eine aktuelle Einführung mit Beispielen aus Mathematik, Naturwissenschaft und Technik}},
year="2001",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="295--318",
abstract="Obwohl die in diesem Abschnitt verwendeten Schl{\"u}sselw{\"o}rter MS-DOS-spezifisch sind und daher nicht der ANSI C-Norm entsprechen, soll an einigen Beispielen gezeigt werden, wie man in C Zugang zum MS-DOS-Betriebssystem erh{\"a}lt. Eine vollst{\"a}ndige Darstellung ist im Rahmen des Buchs nat{\"u}rlich nicht m{\"o}glich. Es wird hier auf die Literatur [131 bzw. [22] verwiesen.",
isbn="978-3-322-94365-1",
doi="10.1007/978-3-322-94365-1_16",
url="https://doi.org/10.1007/978-3-322-94365-1_16"
}
@Inbook{Gerlach2019,
author={{Gerlach, S.}},
title={{Programmieren in C}},
bookTitle={{Computerphysik: Einführung, Beispiele und Anwendungen}},
year="2019",
publisher="Springer",
address="Berlin, Heidelberg",
pages="31--54",
abstract="Ein wichtiger Teil der Computerphysik ist die Implementierung eines Programms, d. h. das Programmieren selbst. Es gibt zwar Simulationspakete und Umgebungen, die spezielle Probleme ohne Programmierung l{\"o}sen k{\"o}nnen, aber erst mit der Programmierung besteht die M{\"o}glichkeit, eigene Probleme zu implementieren und zu l{\"o}sen. In diesem Kapitel werden zuerst die Grundlagen der Programmierung im Detail erl{\"a}utert. Danach folgt eine Einf{\"u}hrung in die Programmiersprache C, die eine hohe Verbreitung genie{\ss}t und auch zu gro{\ss}en Teilen in diesem Buch verwendet wird. Am Ende des Kapitels werden Hilfsmittel zur Programmierung, d.h. sog. Makefiles, Debugger und Versionsverwaltungen und deren Verwendung angesprochen.",
isbn="978-3-662-59246-5",
doi="10.1007/978-3-662-59246-5_4",
url="https://doi.org/10.1007/978-3-662-59246-5_4"
}
//
Binom
/////////////////////////////////
@Inbook{Philippou2025,
author={{Philippou, A. N., & Antzoulakos, D. L.}},
editor={{Lovric, M.}},
title="Binomial Distribution",
bookTitle="International Encyclopedia of Statistical Science",
year="2025",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="306--309",
isbn="978-3-662-69359-9",
doi="10.1007/978-3-662-69359-9_71",
url="https://doi.org/10.1007/978-3-662-69359-9_71"
}
@book{Collani2001,
title={Binomial Distribution Handbook for Scientists and Engineers},
author={{Collani, E., & Dräger, K.}},
series={Graduate Texts in Mathematics},
doi={10.1007/978-1-4612-0215-8},
publisher={Birkhäuser},
address={Boston, MA},
year={2001},
edition={1}
}
@Article{math10152680,
AUTHOR = {{García-García, J. I., Fernández Coronado, N. A., Arredondo, E. H., & Imilpán Rivera, I. A.}},
TITLE = {The Binomial Distribution: Historical Origin and Evolution of Its Problem Situations},
JOURNAL = {Mathematics},
VOLUME = {10},
YEAR = {2022},
NUMBER = {15},
ARTICLE-NUMBER = {2680},
URL = {https://www.mdpi.com/2227-7390/10/15/2680},
ISSN = {2227-7390},
ABSTRACT = {The increase in available probabilistic information and its usefulness for understanding the world has made it necessary to promote probabilistic literate citizens. For this, the binomial distribution is fundamental as one of the most important distributions for understanding random phenomena and effective decision making, and as a facilitator for the understanding of mathematical and probabilistic notions such as the normal distribution. However, to understand it effectively, it is necessary to consider how it has developed throughout history, that is, the components that gave it the form and meaning that we know today. To address this perspective, we identify the problem situations that gave origin to the binomial distribution, the operational and discursive practices developed to find solutions, and the conflicts that caused a leap in mathematical and probability heuristics, culminating in what is now known as the binomial distribution formula. As a result, we present five historical links to the binomial phenomenon where problem situations of increasing complexity were addressed: a case study using informal means (such as direct counting), the formalization of numerical patterns and constructs related to counting cases, specific probability calculus, the study and modeling of probability in variable or complex phenomena, and the use of the distribution formula as a tool to approaching notions such as the normal distribution. The periods and situations identified correspond to a required step in the design of binomial distribution learning from a historical epistemological perspective and when solving conflicts.},
DOI = {10.3390/math10152680}
}
@article{f22e64ae-c859-380d-809e-72c74d13957d,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2346943},
 abstract = {The sum of k independent and identically distributed (0, 1) variables has a binomial distribution. If the variables are identically distributed but not independent, this may be generalized to a two-parameter distribution where the k variables are assumed to have a symmetric joint distribution with no second- or higher-order "interactions". Two distinct generalizations are obtained, depending on whether the "multiplicative" or "additive" definition of "interaction" for discrete variables is used. The multiplicative generalization gives rise to a two-parameter exponential family, which naturally includes the binomial as a special case. Whereas with a beta-binomially distributed variable the variance always exceeds the corresponding binomial variance, the "additive" or "multiplicative" generalizations allow the variance to be greater or less than the corresponding binomial quantity. The properties of these two distributions are discussed, and both distributions are fitted, successfully, to data given by Skellam (1948) on the secondary association of chromosomes in Brassica.},
 author = {{Altham, P. M. E.}},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {2},
 pages = {162--167},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Two Generalizations of the Binomial Distribution},
 urldate = {2026-01-12},
 volume = {27},
 year = {1978}
}
@article{fbd8e02f-8560-36a2-a00c-e74f040810fc,
 ISSN = {0025570X, 19300980},
 URL = {http://www.jstor.org/stable/2689344},
 author = {{Goel, S. K., & Rodriguez, D. M.}},
 journal = {Mathematics Magazine},
 number = {4},
 pages = {225--228},
 publisher = {[Mathematical Association of America, Taylor & Francis, Ltd.]},
 title = {A Note on Evaluating Limits Using Riemann Sums},
 urldate = {2026-01-12},
 volume = {60},
 year = {1987}
}
@article{fa0257d0-e7e0-3c60-a790-1bc8b1adf0c3,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3215739},
 abstract = {We examine how the binomial distribution B(n, p) arises as the distribution Sn = ∑i=1n Xi of an arbitrary sequence of Bernoulli variables. It is shown that B (n, p) arises in infinitely many ways as the distribution of dependent and non-identical Bernoulli variables, and arises uniquely as that of independent Bernoulli variables. A number of illustrative examples are given. The cases B(2, p) and B(3, p) are completely analyzed to bring out some of the intrinsic properties of the binomial distribution. The conditions under which Sn follows B(n, p), given that Sn-1 is not necessarily a binomial variable, are investigated. Several natural characterizations of B(n , p), including one which relates the binomial distributions and the Poisson process, are also given.These results and characterizations lead to a better understanding of the nature of the binomial distribution and enhance the utility.},
 author = {{Vellaisamy, P.,  & Punnen, A. P.}},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {36--44},
 publisher = {Applied Probability Trust},
 title = {On the Nature of the Binomial Distribution},
 urldate = {2026-01-12},
 volume = {38},
 year = {2001}
}
//
Hyperg
/////////////////////////////////
@Inbook{Magnus1966b,
author={{Magnus, W., Oberhettinger, F., & Soni, R. P.}},
title="The hypergeometric function",
bookTitle="Formulas and Theorems for the Special Functions of Mathematical Physics",
year="1966",
publisher="Springer",
address="Berlin, Heidelberg",
pages="37--65",
abstract="The function represented by the infinite series {\$}{\$}{\backslash}sum{\backslash}limits{\_}{\{}n = 0{\}}^{\backslash}infty  {\{}{\backslash}frac{\{}{\{}{\{}{\{}(a){\}}{\_}n{\}}{\{}{\{}(b){\}}{\_}n{\}}{\}}{\}}{\{}{\{}{\{}{\{}(c){\}}{\_}n{\}}{\}}{\}}{\backslash}frac{\{}{\{}{\{}z^n{\}}{\}}{\}}{\{}{\{}n!{\}}{\}}{\}} {\$}{\$}within its circle of convergence and all the analytic continuations is called the hypergeometric function 2F1(a, b; c;z).*",
isbn="978-3-662-11761-3",
doi="10.1007/978-3-662-11761-3_2",
url="https://doi.org/10.1007/978-3-662-11761-3_2"
}
@Inbook{Sprent2025,
author={{Sprent, P.}},
editor={{Lovric, M.}},
title="The Fisher Exact Test",
bookTitle="International Encyclopedia of Statistical Science",
year="2025",
publisher="Springer",
address="Berlin, Heidelberg",
pages="961--962",
abstract="The Fisher Exact test was proposed by Fisher (1934) in the fifth edition of Statistical Methods for Research Workers. It is a test for independence as opposed to association in 2 {\texttimes} 2 contingency tables.",
isbn="978-3-662-69359-9",
doi="10.1007/978-3-662-69359-9_693",
url="https://doi.org/10.1007/978-3-662-69359-9_693"
}
@inbook{doi:https://doi.org/10.1002/9781118445112.stat04869,
author = {{Shuster, J. J.}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781118445112},
title = {Hypergeometric Distribution: Introduction},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {},
doi = {10.1002/9781118445112.stat04869},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat04869},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat04869},
year = {2014},
keywords = {binomial distribution, contingency tables, exact tests, independence, normal distribution, sampling without replacement, Yates's continuity correction},
abstract = {Abstract For a finite population of subjects of two types, suppose we select a random sample without replacement. The probability distribution of the number in the sample of one of the two types is the hypergeometric distribution. This article presents the hypergeometric distribution, summarizes its properties, discusses binomial and normal approximations, and presents a multivariate generalization.}
}
@inbook{doi:https://doi.org/10.1002/9781118445112.stat06165,
author = {{Hershberger, S. L.}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781118445112},
title = {Exact Methods for Categorical Data},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {},
doi = {10.1002/9781118445112.stat06165},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06165},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat06165},
year = {2014},
keywords = {asymptotic theory, nonparametric statistics, sampling distribution, resampling procedures, the bootstrap, Pitman test, Fisher's exact test, contingency table, hypergeometric distribution},
abstract = {Abstract Exact tests evaluate the significance of a test statistic by using the test statistic's empirically derived sampling distribution instead of a theoretical sampling distribution. Preference for exact tests may occur in situations in which the sample size is small, or when the data have distributional deficiencies. Two well-known exact tests, Pitman's test and Fisher's test, are described.}
}
@article{5d8fa06c-0d2a-391a-b97f-85223f1f3b0f,
 ISSN = {09641998, 1467985X},
 URL = {http://www.jstor.org/stable/2982890},
 abstract = {This paper reviews the problems that bedevil the selection of an appropriate test for the analysis of a 2 × 2 table. In contradiction to an earlier paper, the author now argues the case for the use of Fisher's exact test. It is noted that all test statistics for the 2 × 2 table have discrete distributions and it is suggested that it is irrational to prescribe an unattainable fixed significance level. The use of mid-P is suggested, if a formula is required for prescribing a variable tail probability. The problems of two-tail tests are discussed.},
 author = {{Upton, G. J. G.}},
 journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
 number = {3},
 pages = {395--402},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Fisher's Exact Test},
 urldate = {2026-01-12},
 volume = {155},
 year = {1992}
}
@article{202b9adf-f7a7-32c5-94dd-9f269c7f5a86,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984209},
 abstract = {A relationship is derived between the posterior probability of negative association of rows and columns of a 2 × 2 contingency table and Fisher's "exact" probability, as given in existing tables for testing the hypothesis of no association of rows and columns. The result for the 2 × 2 table is generalized to provide the posterior probability that one discrete-valued random variable is stochastically larger than another.},
 author = {{Altham, P. M. E.}},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {261--269},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Exact Bayesian Analysis of a 2 × 2 Contingency Table, and Fisher's "Exact" Significance Test},
 urldate = {2026-01-12},
 volume = {31},
 year = {1969}
}
@article{Camilli1995,
author={{Camilli, G.}},
year={1995},
title={The relationship between Fisher's exact test and Pearson's chi-square test: A bayesian perspective},
journal={Psychometrika},
page={305--312},
volume={60},
number={2},
abstract={It is demonstrated in this paper that two major tests for 2 × 2 talbes are highly related from a Bayesian perspective. Although it is well-known that Fisher's exact and Pearson's chi-square tests are asymptotically equivalent, the present analysis shows that a formal similarity also exists in small samples. The key assumption that leads to the resemblance is the presence of a continuous parameter measuring association. In particular, it is shown that Pearson's probability can be obtained by integrating a two-moment approximation to the posterior distribution of the log-odds ratio. Furthermore, Pearson's chi-square test gave an excellent approximation to the actual Bayes probability in all 2×2 tables examined, except for those with extremely disproportionate marginal frequencies.},
doi={10.1007/BF02301418}
}
//
Gamma
/////////////////////////////////
@Inbook{Koepf2014,
author={{Koepf, W.}},
title="The Gamma Function",
bookTitle="Hypergeometric Summation: An Algorithmic Approach to Summation and Special Function Identities",
year="2014",
publisher="Springer",
address="London",
pages="1--10",
abstract="Apart from the elementary transcendental functions such as the exponential and trigonometric functions and their inverses, the Gamma function is probably the most important transcendental function. It was defined by Euler to interpolate the factorials at noninteger arguments.",
isbn="978-1-4471-6464-7",
doi="10.1007/978-1-4471-6464-7_1",
url="https://doi.org/10.1007/978-1-4471-6464-7_1"
}
@Inbook{Magnus1966a,
author={{Magnus, W., Oberhettinger, F., & Soni, R. P.}},
title="The gamma function and related functions",
bookTitle="Formulas and Theorems for the Special Functions of Mathematical Physics",
year="1966",
publisher="Springer",
address="Berlin, Heidelberg",
pages="1--37",
abstract="The function $\Gamma$(z) is a meromorphic function of z with simple poles at z = −n, (n = 0, 1, 2,...) with the respective residue {\$}{\$}{\backslash}frac{\{}{\{}{\{}{\{}( - 1){\}}^n{\}}{\}}{\}}{\{}{\{}n!{\}}{\}}.{\$}{\$}.",
isbn="978-3-662-11761-3",
doi="10.1007/978-3-662-11761-3_1",
url="https://doi.org/10.1007/978-3-662-11761-3_1"
}
@article{BATIR2008187,
title = {On some properties of the gamma function},
journal = {Expositiones Mathematicae},
volume = {26},
number = {2},
pages = {187-196},
year = {2008},
issn = {0723-0869},
doi = {10.1016/j.exmath.2007.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0723086907000497},
author = {{Batir, N.}},
keywords = {Gamma function, Digamma function, Polygamma functions, Complete monotonicity, Riemann zeta-function},
abstract = {In this paper we prove a complete monotonicity theorem and establish some upper and lower bounds for the gamma function in terms of digamma and polygamma functions.}
}
@article{5bcebaa1-7494-336c-85e9-de3c540e9f64,
 ISSN = {0003486X, 19398980},
 URL = {http://www.jstor.org/stable/1968254},
 author = {{Rasch, G.}},
 journal = {Annals of Mathematics},
 number = {3},
 pages = {591--599},
 publisher = {[Annals of Mathematics, Trustees of Princeton University on Behalf of the Annals of Mathematics, Mathematics Department, Princeton University]},
 title = {Notes on the Gamma-Function},
 urldate = {2026-01-12},
 volume = {32},
 year = {1931}
}
@article{6b95b756-79df-32e7-8452-988d85f07718,
 ISSN = {0003486X, 19398980},
 URL = {http://www.jstor.org/stable/1967180},
 author = {{Gronwall, T. H.}},
 journal = {Annals of Mathematics},
 number = {2},
 pages = {35--124},
 publisher = {[Annals of Mathematics, Trustees of Princeton University on Behalf of the Annals of Mathematics, Mathematics Department, Princeton University]},
 title = {The Gamma Function in the Integral Calculus},
 urldate = {2026-01-12},
 volume = {20},
 year = {1918}
}
@article{7c7965cf-2aff-3860-a01c-d57527fbdc42,
 ISSN = {00029890, 19300972},
 URL = {https://www.jstor.org/stable/48663320},
 abstract = {Since its inception in 1894, the Monthly has printed 50 articles on the Γ function or Stirling’s asymptotic formula, including the magisterial 1959 paper by Phillip J. Davis, which won the 1963 Chauvenet prize, and the eye-opening 2000 paper by the Fields medalist Manjul Bhargava. In this article, we look back and comment on what has been said, and why, and try to guess what will be said about the Γ function in future Monthly issues. We also identify some gaps, which surprised us: phase plots, Riemann surfaces, and the functional inverse of Γ make their first appearance in the Monthly here. We also give a new elementary treatment of the asymptotics of n! and the first few terms of a new asymptotic formula for invΓ.},
 author = {{Borwein, J. M., & Corless, R. M.}},
 journal = {The American Mathematical Monthly},
 number = {5},
 pages = {400--424},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Gamma and Factorial in the Monthly},
 urldate = {2026-01-12},
 volume = {125},
 year = {2018}
}
@article{f70ecf4e-6c25-3732-9fda-1e8986c07b3c,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2309786},
 author = {{Davis, P. J.}},
 journal = {The American Mathematical Monthly},
 number = {10},
 pages = {849--869},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Leonhard Euler's Integral: A Historical Profile of the Gamma Function: In Memoriam: Milton Abramowitz},
 urldate = {2026-01-12},
 volume = {66},
 year = {1959}
}
//Pois
/////////////////////////////////
//
@Inbook{Jolicoeur1999,
author={{Jolicoeur, P.}},
title="The Poisson distribution",
bookTitle="Introduction to Biometry",
year="1999",
publisher="Springer",
address="Boston, MA",
pages="124--133",
abstract="The Poisson distribution is a discontinuous distribution which was described by the French mathematician Sim{\'e}on Denis Poisson (1781-2013;1840) in a book published in 1837. The Poisson distribution is a particular case of the binomial distribution (chapter 17) where the probability p becomes smaller and smaller (p {\textrightarrow} 0) while the exponent k becomes larger and larger (k{\textrightarrow}∞) in such a way that their product, the mean, $\mu$= kp, keeps a finite value (0 < $\mu$ = kp < ∞). The Poisson distribution might thus be defined by the following equation",
isbn="978-1-4615-4777-8",
doi="10.1007/978-1-4615-4777-8_19",
url="https://doi.org/10.1007/978-1-4615-4777-8_19"
}
@article{Zhang2025,
author={{Zhang, Y.}},
year={2025},
title={Poisson Distribution and Its Applications},
journal={Advances in Economics, Management and Political Sciences},
volume={196},
page={1--6},
doi={10.54254/2754-1169/2025.BJ24761}
}
@Inbook{Chung1974,
author={{Chung, K. L.}},
title="Poisson and Normal Distributions",
bookTitle="Elementary Probability Theory with Stochastic Processes",
year="1974",
publisher="Springer",
address="New York, NY",
pages="192--239",
abstract="The Poisson distribution is of great importance in theory and in practice. It has the added virtue of being a simple mathematical object. We could have introduced it at an earlier stage in the book, and the reader was alerted to this in {\textsection}4.4. However, the belated entrance will give it more prominence, as well as a more thorough discussion than would be possible without the benefit of the last two chapters.",
isbn="978-1-4757-3973-2",
doi="10.1007/978-1-4757-3973-2_7",
url="https://doi.org/10.1007/978-1-4757-3973-2_7"
}
@article{a2ab11c2-825b-3d03-aa88-542d1bf1629c,
 ISSN = {03063127},
 URL = {http://www.jstor.org/stable/284821},
 abstract = {Social determinists have argued that the occurrence of independent discoveries and inventions demonstrates the inevitability of techno-scientific progress. Yet the frequency of such multiples may be adequately predicted by a probabilistic model, especially the Poisson model suggested by Price. A detailed inquiry reveals that the Poisson distribution can predict almost all the observed variation in the frequency distribution of multiples collected by Merton, and by Ogburn and Thomas. This study further indicates that: (a) the number of observed multiples may be greatly underestimated, particularly those involving few independent contributors; (b) discoveries and inventions are not sufficiently probable to avoid a large proportion of total failures, and hence techno-scientific advance is to a large measure indeterminate; (c) chance or 'luck' seems to play such a major part that the 'great genius' theory is no more tenable than the social deterministic theory.},
 author = {{Simonton, D. K.}},
 journal = {Social Studies of Science},
 number = {4},
 pages = {521--532},
 publisher = {Sage Publications, Ltd.},
 title = {Independent Discovery in Science and Technology: A Closer Look at the Poisson Distribution},
 urldate = {2026-01-12},
 volume = {8},
 year = {1978}
}
@article{beaf2bb2-3f3e-3d37-837e-422ad120324f,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2333201},
 author = {{Crow, E. L.}},
 journal = {Biometrika},
 number = {3/4},
 pages = {556--559},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Mean Deviation of the Poisson Distribution},
 urldate = {2026-01-12},
 volume = {45},
 year = {1958}
}
@article{c5daa63c-82d0-3414-9df1-e64522b8f21e,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/3001466},
 author = {{Rao, C. R., & Chakravarti, I.M.}},
 journal = {Biometrics},
 number = {3},
 pages = {264--282},
 publisher = {[Wiley, International Biometric Society]},
 title = {Some Small Sample Tests of Significance for a Poisson Distribution},
 urldate = {2026-01-12},
 volume = {12},
 year = {1956}
}
@book{haight1967handbook,
  title={Handbook of the Poisson Distribution},
  author={{Haight, F. A.}},
  isbn={9788391014424},
  lccn={66028750},
  series={Operations Research Society of America. Publications in operations research},
  url={https://books.google.com/books?id=l8Y-AAAAIAAJ},
  year={1967},
  publisher={Wiley}
}
@article{Finkelshtein2025,
 author = {{Finkelshtein, D., Malyarenko, A., Mishura, Y., & Ralchenko, K.}},
 journal = {Methodology and Computing in Applied Probability},
 number = {2},
 pages = {45},
 title = {Entropies of the Poisson Distribution as Functions of Intensity: “Normal” and “Anomalous” Behavior},
 volume = {27},
 year = {2025},
doi={10.1007/s11009-025-10171-9}
}
EEG
/////////////////////////////////
@book{Grafakos2023,
title={Fundamentals of Fourier Analysis},
author={{Grafakos, L.}},
series={Graduate Texts in Mathematics},
doi={10.1007/978-3-031-56500-7},
publisher={Springer},
address={Cham},
year={2024},
edition={1}
}
@book{Brigola2025,
title={Fourier Analysis and Distributions},
author={{Brigola, R.}},
series={Texts in Applied Mathematics},
doi={10.1007/978-3-031-81311-5},
publisher={Springer},
address={Cham},
year={2025},
edition={1}
}
@book{Siuly2017,
title={EEG Signal Analysis and Classification},
author={{Siuly, S., Li, Y., & Zhang, Y.}},
series={Health Information Science},
doi={10.1007/978-3-319-47653-7},
publisher={Springer},
address={Cham},
year={2017},
edition={1}
}
@article{Zhang2023,
author={{Zhang, H., Zhou, Q.-Q., Chen, H., Hu, X.-Q., Li, W.-G., Bai, Y., Han, J.-X., Wang, Y., Liang, Z.-H., Chen, D., Cong, F.-Y., Yan, J.-Q., & Li, X.-L.}},
year={2023},
title={The applied principles of EEG analysis methods in neuroscience and clinical neurology},
journal={Military Medical Research},
page={67},
volume={10},
number={1},
abstract={Electroencephalography (EEG) is a non-invasive measurement method for brain activity. Due to its safety, high resolution, and hypersensitivity to dynamic changes in brain neural signals, EEG has aroused much interest in scientific research and medical fields. This article reviews the types of EEG signals, multiple EEG signal analysis methods, and the application of relevant methods in the neuroscience field and for diagnosing neurological diseases. First, three types of EEG signals, including time-invariant EEG, accurate event-related EEG, and random event-related EEG, are introduced. Second, five main directions for the methods of EEG analysis, including power spectrum analysis, time–frequency analysis, connectivity analysis, source localization methods, and machine learning methods, are described in the main section, along with different sub-methods and effect evaluations for solving the same problem. Finally, the application scenarios of different EEG analysis methods are emphasized, and the advantages and disadvantages of similar methods are distinguished. This article is expected to assist researchers in selecting suitable EEG analysis methods based on their research objectives, provide references for subsequent research, and summarize current issues and prospects for the future.},
doi={10.1186/s40779-023-00502-7}
}
@Inbook{Panov2023,
author={{Panov, G.}},
editor={{Stoyanov, D., Draganski, B., Brambilla, P., & Lamm, C.}},
title="Quantitative EEG Analysis: Introduction and Basic Principles",
bookTitle="Computational Neuroscience",
year="2023",
publisher="Springer US",
address="New York, NY",
pages="85--91",
abstract="The use of mathematical models for electroencephalography (EEG) analysis has been going on for many years, and currently they are starting to find a place both in clinical practice and in parallel with other methods for evaluation of brain activity. The use and interpretation of these methods are not possible without knowledge of the basic mechanisms and processes that underlie the results obtained from their application. The advantages of the quantitative methods, the processes underlying their presentation, and the optimal parameters used, such as the number of electrodes and time intervals, have been evaluated. Attention has been paid to the advantages of the individual possibilities for EEG signal analysis---absolute and relative power as well as coherence. An analysis of these defined quantities provides an opportunity to enter into the intimate mechanisms underlying specific psychopathological phenomena.",
isbn="978-1-0716-3230-7",
doi="10.1007/978-1-0716-3230-7_5",
url="https://doi.org/10.1007/978-1-0716-3230-7_5"
}
@article{FRENCH1984241,
title = {A critical review of EEG coherence studies of hemisphere function},
journal = {International Journal of Psychophysiology},
volume = {1},
number = {3},
pages = {241-254},
year = {1984},
issn = {0167-8760},
doi = {10.1016/0167-8760(84)90044-8},
url = {https://www.sciencedirect.com/science/article/pii/0167876084900448},
author = {{French, C. C, & Graham, J. B.}},
keywords = {coherence—hemisphere function—psychopathology—task effects—sex—handedness—methodology},
abstract = {Studies of hemisphere function involving the use of coherence analysis are reviewed. Despite the fact that many effects related to subject group and type of task have been reported, few seem to be reliable across studies. The assumptions which seem to underlie the use of coherence analysis are made explicit and discussed. Most investigators consider coherence levels to reflect certain ‘baseline’ patterns, possibly related to structural connectivity, upon which task and/or strategy effects may be superimposed. Use of a cephalic reference renders coherence effects virtually uninterpretable for a variety of reasons which are fully discussed.}
}
@article{KUMAR20122525,
title = {Analysis of Electroencephalography (EEG) Signals and Its Categorization–A Study},
journal = {Procedia Engineering},
volume = {38},
pages = {2525-2536},
year = {2012},
note = {INTERNATIONAL CONFERENCE ON MODELLING OPTIMIZATION AND COMPUTING},
issn = {1877-7058},
doi = {10.1016/j.proeng.2012.06.298},
url = {https://www.sciencedirect.com/science/article/pii/S1877705812022114},
author = {{Kumar, J. S., & Bhuvaneswari, P.}},
keywords = {Electroencephalography, brain signal, modality, brain states},
abstract = {Human brain consists of millions of neurons which are playing an important role for controlling behavior of human body with respect to internal/external motor/sensory stimuli. These neurons will act as information carriers between human body and brain. Understanding cognitive behaviour of brain can be done by analyzing either signals or images from the brain. Human behaviour can be visualized in terms of motor and sensory states such as, eye movement, lip movement, remembrance, attention, hand clenching etc. These states are related with specific signal frequency which helps to understand functional behavior of complex brain structure. Electroencephalography (EEG) is an efficient modality which helps to acquire brain signals corresponds to various states from the scalp surface area. These signals are generally categorized as delta, theta, alpha, beta and gamma based on signal frequencies ranges from 0.1Hz to more than 100Hz. This paper primarily focuses on EEG signals and its characterization with respect to various states of human body. It also deals with experimental setup used in EEG analysis.}
}
@inbook{Panitz_Ward_Pouliot_Keil_2024, place={Cambridge}, 
series={Cambridge Handbooks in Psychology}, title={EEG and ERP}, 
booktitle={The Cambridge Handbook of Research Methods and Statistics for the Social and Behavioral Sciences: Volume 2: Performing Research}, publisher={Cambridge University Press}, author={{Panitz, C., Ward, R. T., Pouliot, J., & Keil, A.}}, 
editor={{Edlund, J. E., & Nichols, A. L.}}, 
year={2024}, 
pages={519–544}, 
doi={10.1017/9781009000796.024},
collection={Cambridge Handbooks in Psychology}
}
@article{Guevara2011,
author={{Guevara, M., Hernández-González, M., Sanz-Martin, A., & Amezcua, C.}},
year={2011},
title={EEGcorco: a computer program to simultaneously calculate and statistically analyze EEG coherence and correlation},
journal={Journal of Biomedical Science and Engineering}, 
volume={4}, 
page={774--787},
doi={10.4236/jbise.2011.412096}
}
@article{Chiarion2023,
author={{Chiarion, G., Sparacino, L., Antonacci, Y., Faes, L., & Mesin, L.}},
year={2023},
title={Connectivity Analysis in EEG Data: A Tutorial Review of the State of the Art and Emerging Trends},
journal={Bioengineering (Basel, Switzerland)}, 
volume={10},
number={3}, 
page={372},
doi={10.3390/bioengineering10030372}
}
@book{Gerthsen1966,
title={Physik},
subtitle={Ein Lehrbuch zum Gebrauch Neben Vorlesungen},
author={{Gerthsen, C.}},
editor={{Kneser, H. O.}},
doi={10.1007/978-3-662-30201-9},
publisher={Springer},
address={Berlin, Heidelberg},
year={1966},
edition={9}
}
@book{Gerthsen2026,
title={Gerthsen Physik},
author={{Meschede, D., ed., Feld, L., Gross, R., Müller, R., Niedner-Schatteburg,  G., Schäfer, G., Sokolowski, M., Vewinger, F., Reinhard F., Werner, R. F., & Zohm, H. }},
doi={10.1007/978-3-662-30201-9},
publisher={Springer Spektrum},
address={Berlin, Heidelberg},
year={2026},
edition={26}
}
//
Romb
/////////////////////////////////
// Riemann sum //
//
@Inbook{Guillemin2008,
author={{Guillemin, V. W., & Stroock, D. W.}},
editor={{Jorgensen, P. E. T., Merrill, K. D., & Packer, J. A.}},
title="Some Riemann Sums Are Better Than Others",
bookTitle="Representations, Wavelets, and Frames: A Celebration of the Mathematical Work of Lawrence W. Baggett",
year="2008",
publisher="Birkhäuser",
address="Boston, MA",
pages="3--12",
abstract="This note contains some results obtained while ruminating about Riemann sums. We know that nothing here is truly new, but we know of no other place in which these ideas are presented in the way that they occurred to us. In particular, some of them are so elementary that we hope they will find their way into calculus texts.",
isbn="978-0-8176-4683-7",
doi="10.1007/978-0-8176-4683-7_1",
url="https://doi.org/10.1007/978-0-8176-4683-7_1"
}
@Inbook{Lewis1978,
author={{Lewis, J. T., Osgood, C. F., & Shisha, O.}},
editor={{Beckenbach, E. F.}},
title="Infinite Riemann Sums, the Simple Integral, and the Dominated Integral",
bookTitle="General Inequalities 1 / Allgemeine Ungleichungen 1: Proceedings of the First International Conference on General Inequalities held in the Mathematical Research Institute at Oberwolfach, Black Forest, May 10--14, 1976 / Abhandlung zur ersten internationalen Tagung {\"u}ber Allgemeine Ungleichungen im Mathematischen Forschungsinstitut Oberwolfach, Schwarzwald vom 10. bis 14. Mai 1976",
year="1978",
publisher="Birkhäuser",
address="Basel",
pages="233--242",
abstract="Simple integrability of a function f (defined by Haber and Shisha in [2]) is shown to be equivalent to the convergence of the infinite Riemann sum <m:math display='block'><m:mrow><m:mstyle displaystyle='true'><m:munderover><m:mo>{\&}{\#}x2211;</m:mo><m:mrow><m:mi>k</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>{\&}{\#}x221E;</m:mi></m:munderover><m:mrow><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mrow><m:msub><m:mi>{\&}{\#}x03BE;</m:mi><m:mi>k</m:mi></m:msub></m:mrow><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>(</m:mo><m:mrow><m:msub><m:mi>x</m:mi><m:mi>k</m:mi></m:msub><m:mo>{\&}{\#}x2212;</m:mo><m:msub><m:mi>x</m:mi><m:mrow><m:mi>k</m:mi><m:mo>{\&}{\#}x2212;</m:mo><m:mn>1</m:mn></m:mrow></m:msub></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow></m:mstyle></m:mrow></m:math>{\$}{\$}{\backslash}sum{\backslash}limits{\_}{\{}k = 1{\}}^{\backslash}infty  {\{}f{\backslash}left( {\{}{\{}{\backslash}xi {\_}k{\}}{\}} {\backslash}right){\backslash}left( {\{}{\{}x{\_}k{\}} - {\{}x{\_}{\{}k - 1{\}}{\}}{\}} {\backslash}right){\}} {\$}{\$}to the improper Riemann integral <m:math display='block'><m:mrow><m:mstyle displaystyle='true'><m:mrow><m:msubsup><m:mo>{\&}{\#}x222B;</m:mo><m:mn>0</m:mn><m:mi>{\&}{\#}x221E;</m:mi></m:msubsup><m:mi>f</m:mi></m:mrow></m:mstyle></m:mrow></m:math>{\$}{\$}{\backslash}int{\_}0^{\backslash}infty  f {\$}{\$}f as the gauge of the partition <m:math display='block'><m:mrow><m:msubsup><m:mrow><m:mrow><m:mo>(</m:mo><m:mrow><m:msub><m:mi>x</m:mi><m:mi>k</m:mi></m:msub></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow><m:mrow><m:mi>k</m:mi><m:mo>=</m:mo><m:mn>0</m:mn></m:mrow><m:mi>{\&}{\#}x221E;</m:mi></m:msubsup></m:mrow></m:math>{\$}{\$}{\backslash}left( {\{}{\{}x{\_}k{\}}{\}} {\backslash}right){\_}{\{}k = 0{\}}^{\backslash}infty {\$}{\$}of [0,∞)converges to O. An analogous result is obtained for dominant integrability (defined by Osgood and Shisha in [5]). Also certain results of Bromwich and Hardy [1] are recovered.",
isbn="978-3-0348-5563-1",
doi="10.1007/978-3-0348-5563-1_25",
url="https://doi.org/10.1007/978-3-0348-5563-1_25"
}
@Inbook{Hassler2007,
author={{Hassler, U.}},
title="Riemann-Integrale",
bookTitle={{Stochastische Integration und Zeitreihenmodellierung: Eine Einführung mit Anwendungen aus Finanzierung und Ökonometrie}},
year="2007",
publisher="Springer",
address="Berlin, Heidelberg",
pages="137--153",
abstract="In diesem Kapitel besch{\"a}ftigen wir uns mit stochastischen Riemann-Integralen, d. h. mit gew{\"o}hnlichen Riemann-Integralen mit einem stochastischen Prozess als Integrand1. Mathematisch sind diese Gebilde relativ wenig anspruchsvoll, sie lie{\ss}en sich pfadweise wie in der konventionellen (deterministischen) Analysis {\"u}ber stetige Funktionen definieren. Diese pfadweise Definition wird aber z. B. bei Ito-Integralen im {\"u}bern{\"a}chsten Kapitel nicht mehr m{\"o}glich sein. Daher schlagen wir hier den auch sp{\"a}ter n{\"u}tzlichen Weg ein, Integrale als Grenzwert (im quadratischen Mittel) zu definieren. Ist der stochastische Integrand speziell ein Wiener-Prozess, so folgt das Riemann-Integral einer Normalverteilung mit Erwartungswert Null und bekannter Formel f{\"u}r die Varianz. Eine Reihe von Beispielen soll die Einf{\"u}hrung erleichtern.",
isbn="978-3-540-73568-7",
doi="10.1007/978-3-540-73568-7_7",
url="https://doi.org/10.1007/978-3-540-73568-7_7"
}
@book{Torchinsky2022,
title={A Modern View of the Riemann Integral},
author={{Torchinsky, A.}},
series={Lecture Notes in Mathematics},
doi={10.1007/978-3-031-11799-2},
publisher={Springer},
address={Cham},
year={2022},
edition={1}
}
@Inbook{Forster2023,
author={{Forster, O., & Lindemann, F.}},
title="Das Riemannsche Integral",
bookTitle={{Analysis 1: Differential- und Integralrechnung einer Veränderlichen}},
year="2023",
publisher="Springer Fachmedien",
address="Wiesbaden",
pages="283--302",
abstract="Die Integration ist neben der Differentiation die wichtigste Anwendung des Grenzwertbegriffs in der Analysis. Wir definieren das Integral zun{\"a}chst f{\"u}r Treppenfunktionen, wobei noch keine Grenzwertbetrachtungen n{\"o}tig sind und der elementargeometrische Fl{\"a}cheninhalt von Rechtecken zugrundeliegt. Das Integral allgemeinerer Funktionen wird dann durch Approximation mittels Treppenfunktionen definiert. Mithilfe sog. Riemannscher Summen berechnen wir explizit die Integrale einiger elementarer Funktionen.",
isbn="978-3-658-40130-6",
doi="10.1007/978-3-658-40130-6_18",
url="https://doi.org/10.1007/978-3-658-40130-6_18"
}
@Inbook{Büchter2010,
author={{Büchter, A., & Henn, H.-W.}},
title="Grenzwerte von Riemann'schen Summen: das Integral",
bookTitle="Elementare Analysis: Von der Anschauung zur Theorie",
year="2010",
publisher="Spektrum Akademischer Verlag",
address="Heidelberg",
pages="221--235",
abstract="In diesem Kapitel wollen wir nach der Differenzial- auch die Integralrechnung auf eine mathematisch pr{\"a}zisere Grundlage stellen, sodass wir im n{\"a}chsten Kapitel -- gewisserma{\ss}en als H{\"o}hepunkt unserer Theorieentwicklung -- den Hauptsatz, der Differenzial- und Integralrechnung miteinander verbindet (vgl. 3.3), ebenso pr{\"a}zise formulieren und beweisen k{\"o}nnen.",
isbn="978-3-8274-2680-2",
doi="10.1007/978-3-8274-2680-2_6",
url="https://doi.org/10.1007/978-3-8274-2680-2_6"
}
@article{e941eb5c-b072-3fd7-a80c-cfa5c7173324,
 ISSN = {07468342, 19311346},
 URL = {https://www.jstor.org/stable/48662056},
 abstract = {It is well known that Riemann sums converge toward the integral of any Riemannintegrable function f on the segment [a, b]. In some cases this property is still valid for generalized integrals and can be useful to solve problems involving series and sums. In this note we prove (Theorem 3) that the result still holds for a generalized integral under the assumption that the function is decreasing on the interval. We give two counterexamples: the first one with a linear piecewise function, and a second one based on an integral suggested by Hardy. The last section shows recent applications of the Riemann sums in computer science and applied mathematics.},
 author = {{Truc, J.-P.}},
 journal = {The College Mathematics Journal},
 number = {2},
 pages = {pp. 123--132},
 publisher = {[Mathematical Association of America, Taylor & Francis, Ltd.]},
 title = {Riemann Sums for Generalized Integrals},
 urldate = {2026-01-12},
 volume = {50},
 year = {2019}
}
@article{fbd8e02f-8560-36a2-a00c-e74f040810fc,
 ISSN = {0025570X, 19300980},
 URL = {http://www.jstor.org/stable/2689344},
 author = {{Goel, S. K., & Rodriguez, D. M.}},
 journal = {Mathematics Magazine},
 number = {4},
 pages = {225--228},
 publisher = {[Mathematical Association of America, Taylor & Francis, Ltd.]},
 title = {A Note on Evaluating Limits Using Riemann Sums},
 urldate = {2026-01-12},
 volume = {60},
 year = {1987}
}
// Romberg extrapol //
//
@Inbook{Herrmann1983,
author={{Herrmann, D.}},
title="Romberg-Integration",
bookTitle="Numerische Mathematik --- 40 BASIC-Programme",
year="1983",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="84--86",
abstract="Die bei Programm 22 verwendete Romberg-Extrapolation l{\"a}{\ss}t sich auch auf die numerische Integration anwenden.",
isbn="978-3-322-96321-5",
doi="10.1007/978-3-322-96321-5_27",
url="https://doi.org/10.1007/978-3-322-96321-5_27"
}
@article{TALAY1990143,
title = {Romberg extrapolations for numerical schemes solving stochastic differential equations},
journal = {Structural Safety},
volume = {8},
number = {1},
pages = {143-150},
year = {1990},
issn = {0167-4730},
doi = {10.1016/0167-4730(90)90036-O},
url = {https://www.sciencedirect.com/science/article/pii/016747309090036O},
author = {{Talay, D., & Tubaro, L.}},
keywords = {Monte Carlo method, numerical analysis, probability distribution, random process, simulation},
abstract = {Let (Xt) be the solution of a stochastic differential system. We consider the following situations: computation of Eƒ(Xt) by a Monte-Carlo method (for example, computation of moments of the solution); integration of ƒ(·) with respect to the invariant probability law of (Xt) (when this process is ergodic); or of the upper Lyapunov exponent, by simulating a single trajectory. We propose to perform an extrapolation between approximate values due to first-order schemes; we show that this algorithm (simpler to implement than second-order schemes) provides a second-order accuracy, and we give results of numerical tests.}
}
@inbook{doi:https://doi.org/10.1002/9781119245476.ch7,
author={{Allen, M. B., & Isaacson, E. L}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781119245476},
title = {Numerical Integration},
booktitle = {Numerical Analysis for Applied Science},
chapter = {7},
pages = {363-401},
doi = {10.1002/9781119245476.ch7},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119245476.ch7},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119245476.ch7},
year = {2019},
keywords = {adaptive quadrature, Gauss quadrature, Legendre polynomials, numerical integration, Romberg quadrature},
abstract = {Summary Elementary techniques for computing a definite integral use the fundamental theorem of calculus. This chapter discusses more sophisticated techniques that yield accurate approximations with less computational effort. It surveys two methods for enhancing the accuracy of composite quadrature formulas. The first method, Romberg quadrature, uses approximations to the integral that have low-order accuracy to compute high-order approximations. The second approach, adaptive quadrature, encompasses a class of strategies for tailoring composite formulas to local, idiosyncratic behavior in the integrand. The chapter also explores the theory of Gauss quadrature. In doing so, it generalizes the framework based on the Legendre polynomials, to include other Gauss quadrature methods based on different orthogonal systems of polynomials. In each case, there is a specific interval of integration associated with the basic quadrature method. However, each method readily extends to more general intervals via the change-of-variables tactic.}
}
@article{30c28040-c164-336f-b877-2f793977628d,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2324787},
 author = {{von Petersdorff, T.}},
 journal = {The American Mathematical Monthly},
 number = {8},
 pages = {783--785},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {A Short Proof for Romberg Integration},
 urldate = {2026-01-11},
 volume = {100},
 year = {1993}
}
@article{Jetter1984,
author={{Jetter, K.}},
year={1984},
title={Ein kurze Anmerkung zur Romberg-Integration},
journal={Numerische Mathematik},
page={{275--281}},
volume={45},
number={2},
abstract={Subject to rather mild assumptions on the integrand, the Romberg table (theT-table) and the modified Romberg table (theU-table) yield asymptotically upper and lower bounds for the value of the integral, and the convergence of the columns of the two tables is asymptotically monotone. This is verified for arbitrary sequences of step sizes satisfying the usual condition of convergence for Romberg integration.},
doi={10.1007/BF01389471}
}
// Richardson extra //
//
@article{cc7b552a-2f22-3972-bf87-40a114018835,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2288854},
 abstract = {Simulation methods [particularly Efron's (1979) bootstrap] are being applied more and more frequently in statistical inference. Given data (X1, ..., Xn) distributed according to P, which belongs to a hypothesized model P, the basic goal is to estimate the distribution LP of a function Tn(X1, ..., Xn, P). The bootstrap presupposes the existence of an estimate P̂(X1, ..., Xn) and involves estimating Lp by the distribution L*n of Tn(X*1, ..., X*n, P̂), where (X*1, ..., X*n) is distributed according to P̂. The method is of particular interest when L*n, though known in principle, can realistically only be computed by simulation. Such computation can be expensive if n is large and Tn is complex (e.g., see the multivariate goodness-of-fit tests of Beran and Millar 1986). Even when bootstrap application to a single data set is not excessively expensive, Monte Carlo studies of the bootstrap are another matter. We propose a method based on the classical ideas of Richardson extrapolation for reducing the computational cost inherent in bootstrap simulations and Monte Carlo studies of the bootstrap, by performing simulations for statistics based on two smaller sample sizes. We study theoretically which ratio of the two small sample sizes is apt to give best results. We show how our method works for approximating the χ2, t, and smoothed binomial distributions, and for setting bootstrap percentile confidence intervals for the variance of a normal distribution with a mean of 0.},
 journal = {Journal of the American Statistical Association},
 number = {402},
 pages = {387--393},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
author={{Bickel, P. J., & Yahav, J. A.}},
 title = {Richardson Extrapolation and the Bootstrap},
 urldate = {2026-01-11},
 volume = {83},
 year = {1988}
}
@inbook{Sidi_2003, 
place={Cambridge}, 
series={Cambridge Monographs on Applied and Computational Mathematics}, 
title={The Richardson Extrapolation Process and Its Generalizations}, 
booktitle={Practical Extrapolation Methods: Theory and Applications}, 
publisher={Cambridge University Press}, 
author={{Sidi, A.}}, 
year={2003}, 
pages={19–20}, 
doi={10.1017/CBO9780511546815},
collection={Cambridge Monographs on Applied and Computational Mathematics}
}
@article{doi:10.1137/21M1397349,
author = {{Bach, F.}},
title = {On the Effectiveness of Richardson Extrapolation in Data Science},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {3},
number = {4},
pages = {1251-1277},
year = {2021},
doi = {10.1137/21M1397349},
abstract = { Richardson extrapolation is a classical technique from numerical analysis that can improve the approximation error of an estimation method by combining linearly several estimates obtained from different values of one of its hyperparameters without the need to know in details the inner structure of the original estimation method. The main goal of this paper is to study when Richardson extrapolation can be used within data science beyond the existing applications to step-size adaptations in stochastic gradient descent. We identify two situations where Richardson interpolation can be useful: (1) when the hyperparameter is the number of iterations of an existing iterative optimization algorithm with applications to averaged gradient descent and Frank--Wolfe algorithms (where we obtain asymptotically rates of \$O(1/k^2)\$ on polytopes, where \$k\$ is the number of iterations) and (2) when it is a regularization parameter with applications to Nesterov smoothing techniques for minimizing nonsmooth functions (where we obtain asymptotically rates close to \$O(1/k^2)\$ for nonsmooth functions) and kernel ridge regression. In all these cases, we show that extrapolation techniques come with no significant loss in performance but with sometimes strong gains, and we provide theoretical justifications based on asymptotic developments for such gains, as well as empirical illustrations on classical problems from machine learning. }
}
@article{DUBEAU2019100017,
title = {A remark on Richardson's extrapolation process and numerical differentiation formulae},
journal = {Journal of Computational Physics: X},
volume = {2},
pages = {100017},
year = {2019},
issn = {2590-0552},
doi = {10.1016/j.jcpx.2019.100017},
url = {https://www.sciencedirect.com/science/article/pii/S2590055219300332},
author = {{Dubeau, F.}},
keywords = {Numerical differentiation, Richardson's extrapolation process},
abstract = {Richardson's extrapolation process is a well known method to improve the order of several approximation processes. Here we observe that for numerical differentiation, Richardson's process can be applied not only to improve the order of a numerical differentiation formula but also to find in fact the original formula.}
}
@Inbook{Sidi1988,
author={{Sidi, A.}},
editor={{Brass, H., & Hämmerlin, G.}},
title="Generalizations of Richardson Extrapolation with Applications to Numerical Integration",
bookTitle="Numerical Integration III: Proceedings of the Conference held at the Mathematisches Forschungsinstitut, Oberwolfach, Nov. 8 -- 14, 1987",
year="1988",
publisher={{Birkhäuser}},
address="Basel",
pages="237--250",
abstract="The application of extrapolation (or convergence acceleration) methods to the solution of problems in numerical analysis has become very widespread in recent years. One important area in which extrapolation methods have proved to be remarkably efficient is that of numerical integration. Of the methods that have proved to be useful in this area, the Richardson extrapolation process and some of its generalizations have been the subject of intense research. It is the purpose of this paper to survey briefly the recent developments relating to the generalizations of the Richardson extrapolation process with special emphasis on their application to the numerical evaluation of finite and infinite range integrals.",
isbn="978-3-0348-6398-8",
doi="10.1007/978-3-0348-6398-8_22",
url="https://doi.org/10.1007/978-3-0348-6398-8_22"
}
@Inbook{Rannacher1987,
author={{Rannacher, R.}},
editor={{Hackbusch, W., & Witsch, K.}},
title="Richardson Extrapolation with Finite Elements",
bookTitle="Numerical Techniques in Continuum Mechanics: Proceedings of the Second GAMM-Seminar, Kiel, January 17 to 19, 1986",
year="1987",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="90--101",
abstract="In a recent paper by H. Blum, Lin Q., and the author,[l], it has been shown that the Ritz projection method with linear finite elements admits an asymptotic error expansion for certain classes of ``uniform'' meshes. This provides the theoretical justification for the use of Richardson extrapolation or related correction processes for increasing the accuracy of the scheme. The proofs are entirely based on finite element techniques and extend to situations where no ``discrete'' maximum principle is available. This is examplarily demonstrated here for the second-order Lam{\'e}-Navier system in plain linear elasticity and for a mixed formulation of the fourth-order Kirchhoff plate bending model.",
isbn="978-3-322-85997-6",
doi="10.1007/978-3-322-85997-6_9",
url="https://doi.org/10.1007/978-3-322-85997-6_9"
}
// Trapezregel //
//
@Inbook{Neher2024,
author={{Neher, M.}},
title="Numerische Integration",
bookTitle={{Numerische Mathematik: Eine anschauliche modulare Einführung}},
year="2024",
publisher="Springer",
address="Berlin, Heidelberg",
pages="195--222",
abstract="Manche Anwendungsprobleme lassen sich auf die Berechnung eines bestimmten Integrals zur{\"u}ckf{\"u}hren. Ist keine Stammfunktion des Integranden bekannt, muss die Integrationsaufgabe n{\"a}herungsweise gel{\"o}st werden. Verschiedene geeignete Verfahren werden vorgestellt und analysiert.",
isbn="978-3-662-68815-1",
doi="10.1007/978-3-662-68815-1_7",
url="https://doi.org/10.1007/978-3-662-68815-1_7"
}
@Inbook{Schwarz1997,
author={{Schwarz, H. R.}},
title="Integralberechnung",
bookTitle="Numerische Mathematik",
year="1997",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="375--411",
abstract="Manche Probleme der angewandten Mathematik f{\"u}hren auf die Berechnung von Integralen, die meistens nicht in expliziter Form dargestellt werden k{\"o}nnen. Die numerische Integralberechnung, die man kurz als Quadratur bezeichnet, spielt deshalb eine wichtige Rolle. Wir befassen uns im folgenden mit der gen{\"a}herten Berechnung von bestimmten Integralen. Unbestimmte Integrale (Stammfunktionen) werden zweckm{\"a}{\ss}ig als Anfangswertprobleme gew{\"o}hnlicher Differentialgleichungen behandelt (vgl. Kapitel 9). Von den zahlreichen Anwendungen der numerischen Quadratur nennen wir die Berechnung von Oberfl{\"a}chen, Volumina, Wahrscheinlichkeiten und Wirkungsquerschnitten, die Auswertung von Integraltransformationen und Integralen im Komplexen, die Konstruktion von konformen Abbildungen f{\"u}r Polygonbereiche nach der Formel von Schwarz-Christoffel [Hen 85], die Behandlung von Integralgleichungen etwa im Zusammenhang mit der Randelementmethode und schlie{\ss}lich die Methode der finiten Elemente [Scw91].",
isbn="978-3-663-01227-6",
doi="10.1007/978-3-663-01227-6_8",
url="https://doi.org/10.1007/978-3-663-01227-6_8"
}
@article{Abdulhameed_2021,
doi = {10.1088/1742-6596/2090/1/012104},
url = {https://doi.org/10.1088/1742-6596/2090/1/012104},
year = {2021},
month = {nov},
publisher = {IOP Publishing},
volume = {2090},
number = {1},
pages = {012104},
author = {{Abdulhameed, A. F., & Memon, Q. A.}},
title = {An improved Trapezoidal rule for numerical integration},
journal = {Journal of Physics: Conference Series},
abstract = {Numerical Methods have attracted of research community for solving engineering problems. This interest is due to its practicality and the improvement of highspeed calculations done on current century processors. The increase in numerical method tools in engineering software, such as Matlab, is an example of the increased interest. In this paper, we are present a new improved numerical integration method, that is based on the well-known trapezoidal rule. The proposed method gives a great enhancement to the trapezoidal rule and overcomes the issue of the error value when dealing with some higher order functions even when solving for a single interval. After literature review, the proposed system is mathematically explained along with error analysis. Few examples are illustrated to prove improved accuracy of the proposed method over traditional trapezoidal method.}
}
@article{IZZO2022111193,
title = {Corrected trapezoidal rules for singular implicit boundary integrals},
journal = {Journal of Computational Physics},
volume = {461},
pages = {111193},
year = {2022},
issn = {0021-9991},
doi = {10.1016/j.jcp.2022.111193},
url = {https://www.sciencedirect.com/science/article/pii/S0021999122002558},
author = {{Izzo, F., Runborg, O., & Tsai, R.}},
keywords = {Level set methods, Closest point projection, Boundary integral formulations, Singular integrals, Trapezoidal rules},
abstract = {We present new higher-order quadratures for a family of boundary integral operators re-derived using the approach introduced in Kublik et al. (2013) [7]. In this formulation, a boundary integral over a smooth, closed hypersurface is transformed into an equivalent volume integral defined in a sufficiently thin tubular neighborhood of the surface. The volumetric formulation makes it possible to use the simple trapezoidal rule on uniform Cartesian grids and relieves the need to use parameterization for developing quadrature. Consequently, typical point singularities in a layer potential extend along the surface's normal lines. We propose new higher-order corrections to the trapezoidal rule on the grid nodes around the singularities. This correction is based on local decompositions of the singularity and is dependent on the angle of approach to the singularity relative to the surface's principal curvature directions. The proposed decomposition, combined with the volumetric formulation, leads to a special quadrature error cancellation.}
}
@incollection{luke1969214,
title = {Chapter XV Trapezoidal Rule Integration Formulas},
editor = {{Luke, Y. L.}},
series = {Mathematics in Science and Engineering},
publisher = {Elsevier},
volume = {53},
pages = {214-226},
year = {1969},
booktitle = {The Special Functions and their Approximations},
issn = {0076-5392},
doi = {10.1016/S0076-5392(09)60075-8},
url = {https://www.sciencedirect.com/science/article/pii/S0076539209600758},
abstract = {Publisher Summary
This chapter discusses the trapezoidal rule integration formulas. The development of numerical integration formulas as such is not in the mainstream of the mathematical topics covered in the chapter. The modified trapezoidal rule uses functional values at points which lie midway between those points that apply for the trapezoidal rule. There is a certain class of transcendental functions defined by integrals which can be computed to high accuracy by use of these rules; with a view toward the applications, the chapter takes up this aspect of the subject. Algorithms for the evaluation of definite integrals by means of equally spaced data are closely related to the Euler-Maclaurin summation formula for which there is a vast literature. Comparison of the two formulations shows that the difference in accuracy is negligible.}
}
@article{Bujang10.1063/5.0294833,
    author = {{Bujang, N., Nasir, M. A. S., & Ijam, H. M.}},
    title = {Numerical integration of function using the modified trapezoidal rule and mean averaging method},
    journal = {AIP Conference Proceedings},
    volume = {3338},
    number = {1},
    pages = {040010},
    year = {2025},
    month = {12},
    abstract = {There are many numerical integration methods that exist with its own advantage and disadvantage. This study focusses on the Trapezoidal rule, TR. The advantage of the Trapezoidal rule is that it is simple to use and can be applied to a wide range of functions. Since the Trapezoidal rule is an approximation rule, it will yield an error. In this study, we aim to reduce the error of the Trapezoidal rule can be reduced by applying mean averaging toward Trapezoidal rule called as Trapezoidal rule with mean averaging, TRMEAN. The arithmetic mean, geometric mean, Harmonic mean, Heronian mean, Contra harmonic, Root-mean-square mean, and Centroidal mean will be implemented. Hence, we will be experimented as examples before concluding the results. The results of this study showed that the error from TRMEAN is smaller as compared to TR. The findings indicate that TRMEAN presents a promising improvement in accuracy and reliability for this task over the original TR method. However, further research and validation are essential to firmly establish these conclusions and assess their applicability to diverse datasets and situations.},
    issn = {0094-243X},
    doi = {10.1063/5.0294833},
    url = {https://doi.org/10.1063/5.0294833},
    eprint = {https://pubs.aip.org/aip/acp/article-pdf/doi/10.1063/5.0294833/20840220/040010_1_5.0294833.pdf},
}
//
Cubspl
/////////////////////////////////
@article{jarre2025cubicsplinefunctionsrevisited,
title={Cubic spline functions revisited}, 
author={{Jarre, F.}},
year={2025},
journal={arXiv},
eprint={2507.05083},
archivePrefix={arXiv},
primaryClass={math.NA},
url={https://arxiv.org/abs/2507.05083},
doi={10.48550/arXiv.2507.05083}
}
@article{JARRE2026117240,
title = {Cubic spline functions revisited},
journal = {Journal of Computational and Applied Mathematics},
volume = {478},
pages = {117240},
year = {2026},
issn = {0377-0427},
doi = {10.1016/j.cam.2025.117240},
url = {https://www.sciencedirect.com/science/article/pii/S037704272500754X},
author = {{Jarre, F.}},
keywords = {Cubic spline, Natural spline, Error estimate, Condition number},
abstract = {In this paper a fourth order asymptotically optimal error bound for a new cubic interpolating spline function, denoted by Q-spline, is derived for the case that only function values at given points are used but not any derivative information. The bound seems to be stronger than earlier error bounds for cubic spline interpolation in such setting such as the not-a-knot spline. A brief analysis of the conditioning of the end conditions of cubic spline interpolation leads to a modification of the not-a-knot spline, and some numerical examples suggest that the interpolation error of this revised not-a-knot spline generally is comparable to the near optimal Q-spline and lower than for the not-a-knot spline when the mesh size is small.}
}
@Inbook{Sarfraz2008,
author={{Sarfraz, M.}},
title="Visualization of Shaped Data by Cubic Spline Interpolation",
bookTitle="Interactive Curve Modeling: With Applications to Computer Graphics, Vision and Image Processing",
year="2008",
publisher="Springer London",
address="London",
pages="157--172",
abstract="This chapter reiterates the subject of the previous chapter. Instead of a rational cubic model, a polynomial cubic spline has been presented here for the same objective. A piecewise cubic spline has been introduced to preserve the shape of the data when it is convex, monotone or positive. The spline representation is interpolatory and applicable to the scalar valued data. The shape parameters, in the description of the cubic, have been constrained in such a way that they control the shape of the curve to avoid any noise. As far as visual smoothness is concerned, the curve scheme under discussion is GC1. Thus the continuity constraints have been relaxed from C1 to GC1.",
isbn="978-1-84628-871-5",
doi="10.1007/978-1-84628-871-5_8",
url="https://doi.org/10.1007/978-1-84628-871-5_8"
}
@InProceedings{abd10.1007/978-3-031-27199-1_3,
author={{Abduganiev, M., Azimov, R., & Muydinov, L.}},
editor={{Zaynidinov, H., Singh, M., Tiwary, U. S., & Singh, D.}},
title="Digital Processing Algorithms of Biomedical Signals Using Cubic Base Splines",
booktitle="Intelligent Human Computer Interaction",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="18--26",
abstract="In this article, the issues of digital processing and restoration of electroencephalogram (EEG) signals from biomedical signals are considered, the location of 21 sensors in the EEG apparatus along the brain, the naming of the sensors, their connection types, the use of bipolar coupling in the detection of disease symptoms, interpolation of received signals, disease symptoms. The processes of separating parts into scales have been studied. During the work, the B-spline function was selected as the most convenient mathematical model for digital processing of EEG signals, and the construction of the B-spline function was presented. Based on the constructed mathematical model, an algorithm for restoring the electroencephalogram signals by dividing the problematic parts into scales was developed, and the absolute error in the restoration of EEG signals was estimated.",
isbn="978-3-031-27199-1",
doi={10.1007/978-3-031-27199-1_3}
}
@Inbook{Farin1994,
author={{Farin, G.}},
title="Kubische Spline-Interpolation",
bookTitle="Kurven und Fl{\"a}chen im Computer Aided Geometric Design: Eine praktische Einf{\"u}hrung",
year="1994",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="104--121",
abstract="In diesem Kapitel werden wir uns mit dem popul{\"a}rsten Kurvenschema befassen: C2-stetige kubische interpolierende Splines. Wir haben schon gesehen, da{\ss} Lagrange-Interpolation keine brauchbaren Resultate liefert. Wir haben aber andererseits gesehen, da{\ss} uns durch kubische B-Spline-Kurven ein m{\"a}chtiges Werkzeug zum Modellieren an die Hand gegeben wurde; sie k{\"o}nnen auf einfache Weise komplexe Formen wiedergeben. Diese „Wiedergabe`` geschieht als Approximationsproze{\ss}, man ver{\"a}ndert das Kontrollpolygon solange, bis die gew{\"u}nschte Form gefunden ist. Wir werden sehen, wie man kubische Splines auch zur Interpolation benutzen kann, n{\"a}mlich, eine Splinekurve zu finden, die durch einen gegebenen Satz von Punkten verl{\"a}uft. Kubische Spline-Interpolation ist in die CAGD-Literatur von J. Ferguson [184] im Jahre 1964 eingef{\"u}hrt worden, w{\"a}hrend die zugrundeliegende Mathematik in der Approximationstheorie (siehe de Boor [112] oder Holladay [266]) entwickelt wurde. Eine {\"U}bersicht {\"u}ber die Geschichte der Splines ist bei Schumaker [426] zu finden.",
isbn="978-3-663-10602-9",
doi="10.1007/978-3-663-10602-9_9",
url="https://doi.org/10.1007/978-3-663-10602-9_9"
}
@Inbook{Agarwal1993,
author={{Agarwal, R. P., & Wong, P. J. Y.}},
title="Spline Interpolation",
bookTitle="Error Inequalities in Polynomial Interpolation and Their Applications",
year="1993",
publisher="Springer Netherlands",
address="Dordrecht",
pages="281--362",
abstract="Spline interpolation is an improvement over piecewise --- polynomial interpolation. It uses less information of the given function, yet furnishes smoother interpolates. The plan of this chapter is as follows: In Section6.2 we shall define the spline space Sm,$\tau$($\Delta$), and for a given function x (t) the spline and Lidstone --- spline interpolates Sm,$\tau$$\Delta$and LSm,2m−2$\Delta$x(t),respectively. Here, we shall also show that to acquire the bounds for {\textbardbl} Dk(x− Sm,$\tau$$\Delta$) {\textbardbl}∞ and {\textbardbl} Dk(x− LSm,2m−2$\Delta$x) {\textbardbl}∞ in terms of the derivatives of x(t) it is necessary to estimate several terms. While some of these terms can be estimated by the results of Chapter 5, other terms which require a different analysis for each m and $\tau$, need to be bounded. In Sections6.3,6.4 and 6.6 respectively, we shall consider the cases m = 2, $\tau$ = 2; m = 3, $\tau$ = 4 and m = 3, $\tau$ = 3. These cases correspond to cubic in the class C(2) [a, b], and quintic in the classes C(4)[a, b] and C(3)[a, b] spline interpolates. For the cases m = 3, $\tau$ = 4 and m = 3, $\tau$ = 3 in Sections 6.5 and 6.7 respectively, we shall discuss the construction of approximated quintic splines, and for these interpolates we will provide explicit error bounds in L∞ --- norm. For the cubic and quintic Lidstone --- spline interpolates error bounds in L∞ --- norm are obtained in Sections 6.8 and 6.9, respectively. In Section6.10 we shall extend Theorems 5.3.16 and 5.3.17 for the spline interpolate Sm,$\tau$$\Delta$x(t)",
isbn="978-94-011-2026-5",
doi="10.1007/978-94-011-2026-5_6",
url="https://doi.org/10.1007/978-94-011-2026-5_6"
}
@article{CHENG1991700,
title = {Interproximation: interpolation and approximation using cubic spline curves},
journal = {Computer-Aided Design},
volume = {23},
number = {10},
pages = {700-706},
year = {1991},
issn = {0010-4485},
doi = {10.1016/0010-4485(91)90023-P},
url = {https://www.sciencedirect.com/science/article/pii/001044859190023P},
author = {{Cheng, F., & Barsky, B. A.}},
keywords = {splines, interpolation, uncertain data},
abstract = {An algorithm for the construction of a cubic spline curve with relatively good shape that interpolates specified data points at some knots and passes through specified regions at some other knots is presented. The curve constructed by the algorithm has minimum energy on each of its components. This algorithm has applications in various fields, such as the reconstruction of natural phenomena where data points cannot be sampled exactly, or computer-aided modeling where some of the fitting points cannot be explicitly specified.}
}
@article{PERRIN198775,
title = {Mapping of scalp potentials by surface spline interpolation},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {66},
number = {1},
pages = {75-81},
year = {1987},
issn = {0013-4694},
doi = {10.1016/0013-4694(87)90141-6},
url = {https://www.sciencedirect.com/science/article/pii/0013469487901416},
author = {{Perrin, F., Pernier, J., Bertnard, O., Giard M. H.,  & Echallier, J. F.}},
keywords = {interpolation, spline},
abstract = {Evoked potentials and EEGs record punctuate electrical activity at electrode sites. To represent the overall potential distribution on the entire scalp it is necessary to interpolate between these sampled values. Surface splines are mathematical tools for interpolating functions of two variables. In comparison to the classical methods of interpolation, based on linear combination of the potentials of the 4 nearest electrodes, spline methods are smoother, give more precisely located extrema and converge faster toward the ‘true’ potential surface when the number of recording electrodes is increased. These advantages are at the expense of lengthier computation time.
Résumé
Les potentiels évoqués comme l'électroencéphalographie n'enregistrent l'activité électrique du scalp qu'en des points précis correspondant à l'emplacement des électrodes. Pour avoir une représentation globale de la distribution du potentiel sur le scalp, il est nécessaire d'interpoler entre ces points de mesure. Les surfaces splines permettent d'effectuer de telles interpolations bi-dimensionnelles. Par comparaison avec la méthode d'interpolation couramment utilisée basée sur la combinaison linéaire des potentiels des 4 plus proches électrodes la méthode des splines donne des surfaces plus régulières, les extrêmes sont mieux localisés et quand on augmente le nombre d'électrodes, les surfaces obtenues convergent plus rapidement vers la ‘vraie’ surface. En contrepartie le calcul de ces fonctions est plus long.}
}
@article{SOUFFLET1991393,
title = {A statistical evaluation of the main interpolation methods applied to 3-dimensional EEG mapping},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {79},
number = {5},
pages = {393-402},
year = {1991},
issn = {0013-4694},
doi = {10.1016/0013-4694(91)90204-H},
url = {https://www.sciencedirect.com/science/article/pii/001346949190204H},
author = {{Soufflet, L., Toussaint, M., Luthringer, R., Gresser, J., Minot, R., & Macher, J. P.}},
keywords = {EEG, 3D interpolation, Image synthesis},
abstract = {This paper is a review of the main interpolation methods applicable to 3-dimensional EEG mapping. The use of simple statistical comparison methods on recorded EEG maps allowed us to evaluate the qualities of interpolation methods belonging to 3 mathematical families (barycentric, polynomial, spline). A combination of a 3-dimensional representation of EEG maps and a reliable interpolation method makes it possible to obtain better spatial resolution than with standard planar mapping.}
}
@INPROCEEDINGS{5360654,
  author={{Jiang, H., & Zhao, Y.}},
  booktitle={2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery}, 
  title={The Study of Interpolation Algorithm Based on Cubic Spline in Marching Cubes Method}, 
  year={2009},
  volume={5},
  number={},
  pages={80-83},
  keywords={Interpolation;Spline;Isosurfaces;Boundary conditions;Software algorithms;Convergence;Hardware;Costs;Biomedical image processing;Rendering (computer graphics);Cubic spline;MC;interpolation},
  doi={10.1109/FSKD.2009.591}
}
@article{TOROK2025129411,
title = {Explicit forms of interpolating cubic splines and data smoothing},
journal = {Applied Mathematics and Computation},
volume = {500},
pages = {129411},
year = {2025},
issn = {0096-3003},
doi = {10.1016/j.amc.2025.129411},
url = {https://www.sciencedirect.com/science/article/pii/S0096300325001389},
author = {{Török, C., Hudák, J., Pristaš, V., & Antoni, L.}},
keywords = {Data smoothing, Cubic splines, Interpolation, Linear regression},
abstract = {We express the interpolating cubic splines of class C2 in their new, explicit forms. We construct the desired forms, the spline's Hermitian and B-spline representations for both equidistant and arbitrary nodes. During this process we demonstrate an innovative way to compute the inverse of a special class of tridiagonal matrices. Afterward, we propose the corresponding interpolating spline based linear regression models with easily interpretable coefficients suitable for smoothing data of complex structures.}
}
@article{MOON2001251,
title = {An explicit solution for the cubic spline interpolation for functions of a single variable},
journal = {Applied Mathematics and Computation},
volume = {117},
number = {2},
pages = {251-255},
year = {2001},
issn = {0096-3003},
doi = {10.1016/S0096-3003(99)00178-2},
url = {https://www.sciencedirect.com/science/article/pii/S0096300399001782},
author = {{Moon, B. S.}},
keywords = {Explicit solution, Cubic spline interpolation, B-splines, Inverse matrix},
abstract = {An algorithm for computing the cubic spline interpolation coefficients without solving the matrix equation involved is presented in this paper. It requires only O(n) multiplication or division operations for computing the inverse of the matrix compared to O(n2) or larger number of operations in the Gauss elimination method.}
}
@article{SUN2023115039,
title = {Cubic spline interpolation with optimal end conditions},
journal = {Journal of Computational and Applied Mathematics},
volume = {425},
pages = {115039},
year = {2023},
issn = {0377-0427},
doi = {10.1016/j.cam.2022.115039},
url = {https://www.sciencedirect.com/science/article/pii/S0377042722006379},
author = {{Sun, M., Lan, L., Zhu C. G., & Lei, F.}},
keywords = {Cubic spline interpolation, End conditions, Not-a-knot end condition, Interpolation error estimation},
abstract = {Traditional end conditions for cubic spline interpolation consist of values, the first or the second derivatives of interpolated functions on the boundary interpolation knots. The not-a-knot end condition proposed by de Boor (1985) is a kind of end condition of cubic spline interpolation for the practical application without the requirements of the derivatives at the end knots. However, a significant disadvantage of such end condition is that there is a sharp decrease in the accuracy of the interpolation at boundary intervals. In this paper, by changing the locations of two spline knots in not-a-knot end condition, we propose the optimal arrangement of shifted spline knots for cubic spline interpolation. The proposed scheme leads to an approximately 3.4 times increasing in the accuracy of the interpolation compared to the de Boor’s not-a-knot end condition. Furthermore, we also present the optimal end conditions of cubic spline interpolation to approximate the first and the second derivatives of interpolated functions. The representative examples show the effectiveness and the superiority of the proposed method.}
}
@article{Congedo01092002,
author = {{Congedo, M., Özen, C., & Sherlin, L.}},
title = {Notes on EEG Resampling by Natural Cubic Spline Interpolation},
journal = {Journal of Neurotherapy},
volume = {6},
number = {4},
pages = {73--80},
year = {2002},
publisher = {Routledge},
doi = {10.1300/J184v06n04_08},
abstract = {Resampling of digitized electroencephalographic data allows changing the sampling rate with minimal distortion of the signal. Useful applications of the procedure include compatibility among diverse hardware and software and the customization of data analysis. The natural cubic spline interpolation procedure is introduced in a discursive fashion. A formal presentation is provided in the appendix.}
}
@article{roy2017,
author={{Roy, V., & Shukla, S.}},
year={2017},
title={Effective EEG Motion Artifacts Elimination Based on Comparative Interpolation Analysis},
journal={Wireless Personal Communications},
pages={6441--6451},
volume={97},
number={4},
abstract={Electroencephalogram (EEG) signal is usually suffered from motion artifacts, generated randomly during signal acquisition timings. These artifacts sturdily affect the investigation and therefore, diagnosis of neural diseases from EEG signal. The artifact removal may cause loss of important information from the signal. Therefore, it is required to remove the motion artifacts and simultaneously preserve the desired information, which makes EEG artifact removal a vital task. Enhanced Empirical Mode Decomposition (EEMD) is the most widespread method used for artifact removal, as it is a data-driven based feature extraction method. In this research work the efficiency of various EEMD with different interpolation based artifact removal method have been compared. The EEMD is used to convert input single channel EEG signal to a multichannel signal, and in order to remove the randomness of motion artifact, CCA and DWT filtering were used successively. The performance of different interpolation based artifact removal methods have evaluated and results indicate that the proposed algorithm is suitable for use as a supplement to algorithms currently in use as it offers improvements in DSNR and various other performance parameters.},
doi={10.1007/s11277-017-4846-3}
}
@article{FREDENHAGEN1999182,
title = {On the Construction of Optimal Monotone Cubic Spline Interpolations},
journal = {Journal of Approximation Theory},
volume = {96},
number = {2},
pages = {182-201},
year = {1999},
issn = {0021-9045},
doi = {10.1006/jath.1998.3247},
url = {https://www.sciencedirect.com/science/article/pii/S0021904598932476},
author = {{Fredenhagen, S., Oberle, H. J., & Opfer, G.}},
abstract = {In this paper we derive necessary optimality conditions for an interpolating spline function which minimizes the Holladay approximation of the energy functional and which stays monotone if the given interpolation data are monotone. To this end optimal control theory for state-restricted optimal control problems is applied. The necessary conditions yield a complete characterization of the optimal spline. In the case of two or three interpolation knots, which we call thelocalcase, the optimality conditions are treated analytically. They reduce to polynomial equations which can very easily be solved numerically. These results are used for the construction of a numerical algorithm for the optimal monotone spline in the general (global) case via Newton's method. Here, the local optimal spline serves as a favourable initial estimation for the additional grid points of the optimal spline. Some numerical examples are presented which are constructed by FORTRAN and MATLAB programs.}
}
//
Newton
/////////////////////////////////
@Inbook{Rutishauser1990,
author={{Rutishauser, H.}},
editor={{Gutknecht, M.}},
title="Interpolation",
bookTitle="Lectures on Numerical Mathematics",
year="1990",
publisher="Birkhäuser Boston",
address="Boston, MA",
pages="128--174",
abstract="Interpolation is the art of reading between the lines of a mathematical table. It can be used to express nonelementary functions approximately in terms of the four basic arithmetic operations, thus making them accessible to computer evaluation.",
isbn="978-1-4612-3468-5",
doi="10.1007/978-1-4612-3468-5_6",
url="https://doi.org/10.1007/978-1-4612-3468-5_6"
}
@Inbook{Scherer2010,
author={{Scherer, P. O.J.}},
title="Interpolation",
bookTitle="Computational Physics: Simulation of Classical and Quantum Systems",
year="2010",
publisher="Springer",
address="Berlin, Heidelberg",
pages="15--27",
abstract="Experiments usually produce a discrete set of data points. If additional data points are needed, for instance, to draw a continuous curve or to change the sampling frequency of audio or video signals, interpolation methods are necessary. But interpolation is also helpful to develop more sophisticated numerical methods for the calculation of numerical derivatives or integrals. Polynomial interpolation is discussed in large detail together with its drawbacks. The methods by Lagrange and Newton are discussed, as well as the Neville method, which allow efficient determination and evaluation of the interpolating polynomial. For interpolation over a larger range, larger number of data spline interpolation is very useful which does not show the oscillatory behavior characteristic of polynomial interpolation. In a computer experiment both these approaches are compared. Multivariate interpolation is a necessary tool to process multidimensional data sets, for instance, for image processing. A computer experiment compares bilinear interpolation and bicubic spline interpolation.",
isbn="978-3-642-13990-1",
doi="10.1007/978-3-642-13990-1_2",
url="https://doi.org/10.1007/978-3-642-13990-1_2"
}
@article{https://doi.org/10.1155/2020/9020541,
author = {{Zou, L., Song, L., Wang, X., Weise, T., Chen, Y., & Zhang, C.}},
title = {A New Approach to Newton-Type Polynomial Interpolation with Parameters},
journal = {Mathematical Problems in Engineering},
volume = {2020},
number = {1},
pages = {9020541},
doi = {10.1155/2020/9020541},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2020/9020541},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2020/9020541},
abstract = {Newton’s interpolation is a classical polynomial interpolation approach and plays a significant role in numerical analysis and image processing. The interpolation function of most classical approaches is unique to the given data. In this paper, univariate and bivariate parameterized Newton-type polynomial interpolation methods are introduced. In order to express the divided differences tables neatly, the multiplicity of the points can be adjusted by introducing new parameters. Our new polynomial interpolation can be constructed only based on divided differences with one or multiple parameters which satisfy the interpolation conditions. We discuss the interpolation algorithm, theorem, dual interpolation, and information matrix algorithm. Since the proposed novel interpolation functions are parametric, they are not unique to the interpolation data. Therefore, its value in the interpolant region can be adjusted under unaltered interpolant data through the parameter values. Our parameterized Newton-type polynomial interpolating functions have a simple and explicit mathematical representation, and the proposed algorithms are simple and easy to calculate. Various numerical examples are given to demonstrate the efficiency of our method.},
year = {2020}
}
@inbook{doi:10.1002/9781119604570.ch4,
author={{Epperson, J. F.}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781119604570},
title = {Interpolation and Approximation},
booktitle = {An Introduction to Numerical Methods and Analysis},
chapter = {4},
pages = {101-148},
doi = {10.1002/9781119604570.ch4},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119604570.ch4},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119604570.ch4},
year = {2021},
keywords = {divided differences, Hermite interpolation, interpolation error, inverse quadratic interpolation, Lagrange interpolation, Muller's method, Newton interpolation, piecewise polynomial interpolation, tension Splines},
abstract = {Summary This chapter presents the information of interpolation and approximation for solving problems of mathematical analysis. It contains exercises and solutions that present an introduction to key concepts, a calculus review, and an updated primer on Lagrange interpolation, Newton interpolation and divided differences, interpolation error, Muller's method and inverse quadratic interpolation, Hermite interpolation, piecewise polynomial interpolation, Splines, tension Splines, and least squares concepts in approximation. The chapter features new and updated material reflecting new trends and applications in numerical methods and analysis. It is the perfect for upper-level undergraduate students in mathematics, science, and engineering courses, as well as for courses in the social sciences, medicine, and business with numerical methods and analysis components.}
}
@article{tsao1977,
author={{Tsao, N.-K.}},
year={1977},
title={Newton interpolation is efficient for approximation of linear functionals},
journal={Numerische Mathematik},
page={115--122},
volume={29},
number={1},
abstract={The Newton interpolation approach is developed for approximation of linear functionals. It is shown that in numerical interpolation and numerical differentiation, the Newton interpolation approach is more efficient than solving the Vandermonde systems.},
doi={10.1007/BF01389317}
}
//
Matrix
/////////////////////////////////
@book{saff2025,
title={Matrix Fundamentals},
subtitle={From Equation Solving to Signal Processing},
author={{Saff, E. B., & Snider, A. D.}},
doi={10.1007/978-3-031-97222-5},
publisher={Springer},
address="Cham",
edition={2},
year={2025}
}
@book{gentle2024,
title={Matrix Algebra},
subtitle={Theory, Computations and Applications in Statistics},
author={{Gentle, J. E.}},
series={Springer Texts in Statistics},
doi={10.1007/978-3-031-42144-0},
publisher={Springer},
address="Cham",
year={2024},
edition={3}
}
@Inbook{Shores2018,
author={{Shores, T. S.}},
title="MATRIX ALGEBRA",
bookTitle="Applied Linear Algebra and Matrix Analysis",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="65--180",
abstract="In Chapter 1 we used matrices and vectors as simple storage devices. In this chapter matrices and vectors take on a life of their own. We develop the arithmetic of matrices and vectors. Much of what we do is motivated by a desire to extend the ideas of ordinary arithmetic to matrices.",
isbn="978-3-319-74748-4",
doi="10.1007/978-3-319-74748-4_2",
url="https://doi.org/10.1007/978-3-319-74748-4_2"
}
@Inbook{Karpfinger2022,
author={{Karpfinger, C.}},
title="L R-Zerlegung einer Matrix",
bookTitle="H{\"o}here Mathematik in Rezepten: Begriffe, S{\"a}tze und zahlreiche Beispiele in kurzen Lerneinheiten",
year="2022",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="107--117",
abstract="Wir betrachten das Problem, zu einer invertierbaren Matrix {\$}{\$}A {\backslash}in {\backslash}mathbb {\{}R{\}}^{\{}n{\backslash}times n{\}}{\$}{\$}A∈Rn{\texttimes}nund einem Vektor {\$}{\$}b {\backslash}in {\backslash}mathbb {\{}R{\}}^n{\$}{\$}b∈Rneinen Vektor {\$}{\$}x {\backslash}in {\backslash}mathbb {\{}R{\}}^n{\$}{\$}x∈Rnmit {\$}{\$}A {\backslash}, x = b{\$}{\$}Ax=bzu bestimmen; kurz: Wir l{\"o}sen das lineare Gleichungssystem {\$}{\$}A x = b{\$}{\$}Ax=b. Formal erh{\"a}lt man die L{\"o}sung durch {\$}{\$}x = A^{\{}-1{\}} b{\$}{\$}x=A-1b. Aber die Berechnung von {\$}{\$}A^{\{}-1{\}}{\$}{\$}A-1ist bei einer gro{\ss}en Matrix A aufwendig. Die Cramer'sche Regel (siehe Rezept in Abschn. 12.3) ist aus numerischer Sicht zur Berechnung der L{\"o}sung x ungeeignet. Tats{\"a}chlich liefert das Gau{\ss}'sche Eliminationsverfahren, das wir auch in Kap. 9 zur h{\"a}ndischen L{\"o}sung eines LGS empfohlen haben, eine Zerlegung der Koeffizientenmatrix A, mit deren Hilfe es m{\"o}glich ist, ein Gleichungssystem der Form {\$}{\$}A {\backslash}, x = b{\$}{\$}Ax=bmit invertierbarem A zu l{\"o}sen. Diese sogenannte {\$}{\$}L{\backslash}, R{\$}{\$}LR-Zerlegung ist zudem numerisch gutartig. Gleichungssysteme mit bis zu etwa 10000 Zeilen und Unbekannten lassen sich auf diese Weise vorteilhaft l{\"o}sen. F{\"u}r gr{\"o}{\ss}ere Gleichungssysteme sind iterative L{\"o}sungsverfahren zu bevorzugen (siehe Kap. 71).",
isbn="978-3-662-63305-2",
doi="10.1007/978-3-662-63305-2_11",
url="https://doi.org/10.1007/978-3-662-63305-2_11"
}
@article{GRCAR2011163,
title = {How ordinary elimination became Gaussian elimination},
journal = {Historia Mathematica},
volume = {38},
number = {2},
pages = {163-218},
year = {2011},
issn = {0315-0860},
doi = {10.1016/j.hm.2010.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0315086010000376},
author = {{Grcar, J. F.}},
keywords = {Algebra before 1800, Gaussian elimination, Human computers, Least squares method, Mathematics education},
abstract = {Newton, in notes that he would rather not have seen published, described a process for solving simultaneous equations that later authors applied specifically to linear equations. This method — which Euler did not recommend, which Legendre called “ordinary,” and which Gauss called “common” — is now named after Gauss: “Gaussian” elimination. Gauss’s name became associated with elimination through the adoption, by professional computers, of a specialized notation that Gauss devised for his own least-squares calculations. The notation allowed elimination to be viewed as a sequence of arithmetic operations that were repeatedly optimized for hand computing and eventually were described by matrices.
Zusammenfassung
In Aufzeichnungen, die Newton lieber nicht der Veröffentlichung preisgegeben hätte, beschreibt er den Prozess für die Lösung von simultanen Gleichungen, den spätere Autoren speziell für lineare Gleichungen anwandten. Diese Methode — welche Euler nicht empfahl, welche Legendre “ordinaire” nannte, und welche Gauß “gewöhnlich” nannte — wird nun nach Gauß benannt: Gaußsches Eliminationsverfahren. Die Verbindung des Gaußschen Namens mit Elimination wurde dadurch hervorgebracht, dass professionelle Rechner eine Notation übernahmen, die Gauß speziell für seine eigenen Berechnungen der kleinsten Quadrate ersonnen hatte, welche zuließ, das Elimination als eine Sequenz von arithmetischen Rechenoperationen betrachtet wurde, die wiederholt für Handrechnungen optimisiert wurden und schließlich durch Matrizen beschrieben wurden.}
}



