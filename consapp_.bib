Hist
/////////////////////////////////
// Newton interpol //
@book{Newton1687,
    author={{Newton,  I.}},
    year={1687},
    title={{Philosophiae naturalis principia mathematica}},
    edition={1},
    address={Londini},
    publisher={{Jussu Societatis Regiae ac typis Josephi Streater, prostant venales apud Sam. Smith}},     url={https://books.google.at/books?id=XJwx0lnKvOgC&pg=PP2&redir_esc=y#v=onepage&q&f=false}
}
@book{taylor1715methodus,
  title={{Methodus incrementorum directa & inversa. Auctore Brook Taylor, LL. D. & Regiae Societatis Secretario}},
  author={{Taylor, B.}},
  url={https://books.google.com/books?id=iXN1xgEACAAJ},
  year={1715},
  publisher={Typis Pearsonianis: Prostant apud Gul. Innys ad Insignia Principis in Coemeterio Paulino MDCCXV},
  address={Londini}
}
// Gamma //
@misc{Bernoulli1729,
    author={{Bernoulli, D.}},
    year={1729},
    month={10},
    title={{Lettre XLVII. D. Bernoulli a Goldbach}},
    address={{St.-Petersbourg ce 6. octobre 1729}},
    url={https://commons.m.wikimedia.org/wiki/File:DanielBernoulliLetterToGoldbach-1729-10-06.jpg}
}
// Normal //
@book{DeMoivre1738,
  title={{The Doctrine of Chances: Or, A Method of Calculating the Probability of Events in Play}},
  edition={2},
  author={{de Moivre, A.}},
  url={https://books.google.com/books?id=PII\_AAAAcAAJ},
  year={1738},
  address={London},
  publisher={H. Woodfall}
}
// Fourier //
@book{Fourier1822,
    author={{Fourier, J. B. J.}},
    year={1822},
    title={{Théorie analytique de la chaleur}},
    address={A Paris},
    publisher={Chez Firmin Didot, pere et fils},
    url={https://archive.org/details/bub_gb_TDQJAAAAIAAJ/mode/1up}
}
// Riemann sum //
@article{Riemann1868,
author = {{Riemann, B.}},
journal = {Abhandlungen der Königlichen Gesellschaft der Wissenschaften in Göttingen},
pages = {87-131},
title = {{Ueber die Darstellbarkeit einer Function durch eine trigonometrische Reihe. (Mitgetheilt durch R. Dedekind)}},
url = {http://eudml.org/doc/135759},
volume = {13},
year = {1868}
}
// Chi sq //
@article{Helmert1876,
    author={{Helmert, F. R.}},
    year={1876},
    title={{Ueber die Wahrscheinlichkeit der Potenzsummen der Beobachtungsfehler und über einige damit im Zusammenhange stehende Fragen}},
    journal={{Zeitschrift für Mathematik und Physik}},
    volume={21},
    pages={192--219},
    url={https://gdz.sub.uni-goettingen.de/id/PPN599415665_0021}
}
// t //
@article{Gosset1908,
    author={{Gosset, W. S.}},
    year={1908},
    title={The probable error of a mean},
    journal={Biometrika},
    volume={6},
    number={1},
    pages={1--25},
    doi={10.2307/2331554}
}
// Rich expl //
@article{10.1098/rsta.1911.0009,
    author = {{Richardson, L. F.}},
    title = {IX. The approximate arithmetical solution by finite differences of physical problems involving differential equations, with an application to the stresses in a masonry dam},
    journal = {Philosophical Transactions of the Royal Society of London, Series A: Containing Papers of a Mathematical or Physical Character},
    volume = {210},
    number = {459-470},
    pages = {307--357},
    year = {1911},
    month = {01},
    abstract = {1. Introduction.— 1·0. The object of this paper is to develop methods where by the differential equations of physics may be applied more freely than hitherto in the approximate form of difference equations to problems concerning irregular bodies. Though very different in method, it is in purpose a continuation of a former paper by the author, on a “Freehand Graphic Way of Determining Stream Lines and Equipotentials” (‘Phil. Mag.,’February, 1908; also ‘Proc. Physical Soc.,’ London, vol. xxi.). And all that was there said, as to the need for new methods, may be taken to apply here also. In brief, analytical methods are the foundation of the whole subject, and in practice they are the most accurate when they will work, but in the integration of partial equations, with reference to irregular-shaped boundaries, their field of application is very limited.},
    issn = {0264-3952},
    doi = {10.1098/rsta.1911.0009},
    url = {https://doi.org/10.1098/rsta.1911.0009},
    eprint = {https://royalsocietypublishing.org/rsta/article-pdf/210/459-470/307/264773/rsta.1911.0009.pdf},
}
// Fisher Z //
@article{Fisher1915,
 doi={10.2307/2331838},
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2331838},
 author = {{Fisher, R. A.}},
 journal = {Biometrika},
 number = {4},
 pages = {507--521},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population},
 volume = {10},
 year = {1915}
}
// Fisher exact //
@article{Fisher1922,
    author={{Fisher, R. A.}},
    year={1922},
    title={On the Interpretation of {χ2} from Contingency Tables, and the Calculation of P},
    journal={Journal of the Royal Statistical Society},
    volume={85},
    number={1},
    pages={87--94},
    doi={10.2307/2340521}
}
// F //
@article{Fisher1924,
    author={{Fisher, R. A.}},
    year={1924},
    title={On a distribution yielding the error functions of several well-known statistics},
    journal={Proceedings International Mathematical Congress, Toronto},
    volume={2},
    pages={805--813},     url={https://repository.rothamsted.ac.uk/item/8w2q9/on-a-distribution-yielding-the-error-functions-of-several-well-known-statistics}
}
// Matrix //
@book{Cardano1545,
    author={Cardano, G.},
    year={1545},
title={{ARTIS MAGNAE, SIVE DE REGVLIS ALGEBRAICIS, LIBER VNVS}},
    publisher={ANDREAE OSIANDRO viro eruditiff},
    address={S. P. D},     url={https://web.archive.org/web/20220201093634/http://www.filosofia.unimi.it/cardano/testi/operaomnia/vol_4_s_4.pdf}
}
@article{Heisenberg1925,
    author={{Heisenberg, W.}},
    year={1925},
    title={{Über quantentheoretische Umdeutung kinematischer und mechanischer Beziehungen}},
    journal={{Zeitschrift für Physik}},
    pages={879--893},
     volume={33},
     language={de},
     abstract={In der Arbeit soll versucht werden, Grundlagen zu gewinnen für eine quantentheoretische Mechanik, die ausschließlich auf Beziehungen zwischen prinzipiell beobachtbaren Größen basiert ist.},
    url={https://doi.org/10.1007/BF01328377},
    doi={10.1007/BF01328377}
}
// Cub spl //

@article{RABUT1992149,
title = {An introduction to schoenberg's approximation},
journal = {Computers & Mathematics with Applications},
volume = {24},
number = {12},
pages = {149--175},
year = {1992},
issn = {0898-1221},
doi = {10.1016/0898-1221(92)90177-J},
url = {https://www.sciencedirect.com/science/article/pii/089812219290177J},
author = {{Rabut, C.}},
abstract = {For a given function B and a non-zero real number h, Schoenberg's approximation defines from some data (jh, yj)jϵZd the function ΣjϵZd yj B(•h − j). For people not used to this kind of approximation, this paper intends to do a summary of the main definitions, properties and utilizations of Schoenberg's approximation: we show that the main tool to handle Schoenberg's approximation is the Fourier transform of B and even more its modified version, the transfer function of B; we give conditions for convergence of ΣjϵZd f(jh) B(•h − j) when h tends to zero, and we give various ways to define various B as combinations of translates of some function ϕ (usually ϕ is either some radial function, or obtained by a tensor product of some radial function), depending on the properties we want for the associated Schoenberg's approximation. Last, we show how multi-resolution analysis, subdivision techniques, and wavelets techniques, are nicely connected to Schoenberg's approximation.}
}
@article{Schoenberg1946,
title={Contributions to the problem of approximation of equidistant data by analytic functions. Part A. On the problem of smoothing or graduation. A first class of analytic approximation formulae},
author={{Schoenberg, I. J.}},
journal={Quarterly of Applied Mathematics},
volume={4},
year={1946},
pages={45--99},
doi={10.1090/qam/15914}
}
// Romb //
@article{Romberg1955,
title={Vereinfachte numerische Integration},
author={{Romberg, W.}},
address={Trondheim},
publisher={F. Bruns Bokhandel},
year={1955},
pages={30--36},
journal={Forhandlinger / Det Kongelige Norske Videnskabers Selskab},
volume={28},
number={7},
language={ger},
note={https://katalog.ub.uni-heidelberg.de/cgi-bin/titel.cgi?katkey=68187317},
url={https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://web.eng.fiu.edu/LEVY/images/EGM5346/romberg%2520textbook%2520example.pdf&ved=2ahUKEwiSxr-u846SAxVFQvEDHd83Bj8QFnoECHMQAQ&usg=AOvVaw3vtOkKCF8WeDkOB49i_03i}
}
Allg
/////////////////////////////////
@misc{Schrausser81800920,
    url = {https://www.academia.edu/81800920},
    author = {{Schrausser, D. G.}},
    language = {de},
    title = {{ThetaWin Overview}},
    journal = {Software},
    publisher = {Academia},
    year = {2009}
}
@misc{SchrausserRG.2.2.32114.17601,
    doi = {10.13140/RG.2.2.32114.17601},
    url = {https://rgdoi.net/10.13140/RG.2.2.32114.17601},
    author = {{Schrausser, D. G.}},
    language = {en},
    title = {Development of a Parameter to Indicate the Focussation-Level of Cortical Activation},
    journal = {Method},
    publisher = {ResearchGate},
    year = {2000},
    month={05}
} 
@misc{SchrausserEEGalgorithms,
    author = {{Schrausser, D. G.}},
    language = {en},
    title = {{Spectral and Coherence Analysis: Algorithms}},
    year = {2000},
    publisher = {ResearchGate},
    journal = {Method},
    doi= {10.13140/RG.2.2.28637.90083}
}
@article{schrausser_2025_17691241,
  abstract = {Graphical user interface for Theta applications within ConsoleApp_Distribution_Functions (Schrausser, 2024), generating distributions and estimators for several parameters via bootstrap method.},
  added-at = {2025-11-23T18:11:41.000+0100},
  author = {{Schrausser, D. G.}},
  biburl = {https://www.bibsonomy.org/bibtex/299a253e1e539084acd4a02607385e9fb/schrausser},
  doi = {10.5281/zenodo.17691241},
  interhash = {6c69c4cd143e4e511b0e372a4c5e4592},
  intrahash = {99a253e1e539084acd4a02607385e9fb},
  journal = {Zenodo Software documentation},
  keywords = {bootstrapping myown probability resampling software statistics},
  month = nov,
  number = {11/23},
  pages = 1,
  publisher = {Zenodo},
  timestamp = {2025-11-23T18:13:16.000+0100},
  title = {ThetaWin},
  url = {https://doi.org/10.5281/zenodo.17691241},
  volume = 2025,
  year = 2025
}
@article{schrausser_2025_17743756,
  abstract = {Tools for calculations, bootstrapping and permutation tests, also for practice purposes when learning programming languages (C, Basic, Fortran etc.), see e.g. Halvorson and Rygmyr (1991), Chivers and Sleightholme (2018), Joyce (2019), Streib (2020) or Gonzalez-Morris and Horton (2024).},
  added-at = {2025-11-28T02:26:38.000+0100},
  author = {{Schrausser, D. G.}},
  biburl = {https://www.bibsonomy.org/bibtex/2b424a07dd1c2e798d28d6a60bd631234/schrausser},
  doi = {10.5281/zenodo.17743756},
  interhash = {c2f4285edeb5db4c709c50f0f2a4e61b},
  intrahash = {b424a07dd1c2e798d28d6a60bd631234},
  journal = {Zenodo Software documentation},
  keywords = {bootstrapping myown permutation_test programs simulation software statistics},
  month = nov,
  number = {11/28},
  pages = 1,
  publisher = {Zenodo},
  timestamp = {2025-11-28T02:26:38.000+0100},
  title = {Various Programs},
  url = {https://doi.org/10.5281/zenodo.17743756},
  volume = 2025,
  year = 2025
}
@article{schrausser_2025_17880113,
  abstract = {Graphical MS Windows user interface for ConsoleApp_DistributionFunctions.},
  added-at = {2025-12-10T15:05:51.000+0100},
  author = {{Schrausser, D. G.}},
  biburl = {https://www.bibsonomy.org/bibtex/2184be3d1de8fe4b8cdb9e2b22e981fa2/schrausser},
  doi = {10.5281/zenodo.17880113},
  interhash = {792bdced0a73182b7ca9a2650b514fc1},
  intrahash = {184be3d1de8fe4b8cdb9e2b22e981fa2},
  journal = {Zenodo Software documentation},
  keywords = {myown probability software statistics},
  language = {eng},
  month = dec,
  number = {12/10},
  pages = 1,
  publisher = {Zenodo},
  timestamp = {2025-12-10T15:05:51.000+0100},
  title = {FunktionWin: Windows Interface for distribution functions
                  },
  url = {https://doi.org/10.5281/zenodo.17880113},
  volume = 2025,
  year = 2025
}
@book{Schrausser10.31234/osf.io/rvzxa,
  abstract = {Summarizing presentation of the fundamentally most important distribution functions with distribution graphics and formulation. Examples for the calculation in SCHRAUSSER-MAT syntax, as well as an extensive collection of formulas with transformations according to parameters of interest in the appendix.},
  added-at = {2024-04-13T10:12:55.000+0200},
  author = {{Schrausser, D. G.}},
  biburl = {https://www.bibsonomy.org/bibtex/296c3affc687e22f136a69544acac0822/schrausser},
  doi = {10.31234/osf.io/rvzxa},
  interhash = {7905f1250c208c6de994640115b1dab8},
  intrahash = {96c3affc687e22f136a69544acac0822},
  keywords = {myown probability statistics},
  language = {de},
  publisher = {PsyArXiv},
  timestamp = {2024-04-13T10:14:52.000+0200},
  title = {Handbook: Distribution Functions (Verteilungs Funktionen)},
  year = 2024
}
@software{Schrausser7651660,
author={{Schrausser, D. G.}},
title = {{Schrausser/FunktionWin: Windows Interface for distribution functions}},
year = {2023},
publisher = {Zenodo},
doi = {10.5281/zenodo.7651660}
}
@software{Schrausser7659263,
author={{Schrausser, D. G.}},
title = {{Schrausser/ThetaWin: Distribution simulator}},
year = {2023},
publisher = {Zenodo},
doi = {10.5281/zenodo.7659263}
}
@software{Schrausser7664141,
author={{Schrausser, D. G.}},
title = {{Schrausser/ConsoleApp\_DistributionFunctions: Console applications for distribution functions}},
year = {2023},
publisher = {Zenodo},
doi  = {10.5281/zenodo.7664141}
}
@software{Schrausser7655046,
author={{Schrausser, D. G.}},
title = {{Schrausser/ConsoleApp\_Matrix: Console applications for matrix calculation and tools}},
year = {2023},
publisher = {Zenodo},
doi = {10.5281/zenodo.7655046}
}
@software{Schrausser7655239,
author={{Schrausser, D. G.}},
title = {{Schrausser/ConsoleApp\_Tools: Console tool applications}},
year = {2023},
publisher = {Zenodo},
doi = {10.5281/zenodo.7655239}
}
@software{Schrausser7653790,
author={{Schrausser, D. G.}},
title = {{Schrausser/ConsoleApp\_String: Console applications for string and transformation}},
year = {2023},
publisher = {Zenodo},
doi = {10.5281/zenodo.7653790}
}
@software{Schrausser7655056,
author={{Schrausser, D. G.}},
title  = {{Schrausser/ConsoleApp\_Integral: Console applications for integral and interpolation}},
year = {2023},
publisher = {Zenodo},
doi  = {10.5281/zenodo.7655056}
}
@software{schrausser_2024_10701350,
author={{Schrausser, D. G.}},
title = {Schrausser/ConsoleApp\_EEG: 2.0},
year = {2023},
publisher = {Zenodo},
doi   = {10.5281/zenodo.10701350}
}
@book{schrausser_2025_15713317,
  abstract = {Mathematical and statistical applications for HP Prime. Algorithms are presented in context with the corresponding scope of application. CAS programs (1), HP Prime User functions (2) and functions for HP Prime Applications (3) are listed in alphabetical order, a comparison to corresponding SCHRAUSSER-MAT functions is given. In addition to the source codes of the functions, raw data sets are provided for correlation- as well as resampling-methods.},
  added-at = {2025-06-23T07:05:59.000+0200},
  author = {{Schrausser, D. G.}},
  biburl = {https://www.bibsonomy.org/bibtex/296f71bca8cc6f7a017c199c26687ab04/schrausser},
  doi = {10.5281/zenodo.15713317},
  edition = 1,
  interhash = {cbaf03a9ce7d77cc5c57721d7683d6b6},
  intrahash = {96f71bca8cc6f7a017c199c26687ab04},
  keywords = {bootstrapping calculus complex_plane hp_prime myown permutation_test probability randomization_test resampling software statistics},
  language = {en},
  month = jun,
  timestamp = {2025-06-23T07:18:03.000+0200},
  title = {HP_Prime_MATH: Manual},
  url = {https://doi.org/10.5281/zenodo.15713317},
  year = 2025
}
//
Prog
/////////////////////////////////
@book{Joyce2019,
author={{Joyce, P.}},
Title={{Numerical C: Applied Computational Programming with Case Studies}},
year="2019",
edition={1},
publisher="Apress",
address="Berkeley, CA",
abstract="The C programming language was created in the 1970s, yet it is still in extensive use today and is the basis of many other languages. For this reason I have used C as the language for the solution of the numerical problems demonstrated in this book.",
isbn="978-1-4842-5064-8",
doi="10.1007/978-1-4842-5064-8"
}
@book{Gonzalez2024,
author={{Gonzalez-Morris, G., & Horton, I.}},
Title={{Beginning C: From Beginner to Pro}},
year="2024",
edition={7},
publisher="Apress",
address="Berkeley, CA",
isbn="979-8-8688-0148-8",
doi="10.1007/979-8-8688-0149-5"
}
@Inbook{Kaier1990,
author={{Kaier, E.}},
title="Referenz zu MS-DOS",
bookTitle={{Informatik: Referenzbuch. Mit den vollständigen Befehlslisten zu MS-DOS, Turbo Pascal, dBase und Multiplan}},
year="1990",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="1--12",
abstract="DOSSHELL als Befehl: Mit DOSSHELL wird die Men{\"u}-Oberfl{\"a}che von MS-DOS von der Befehlszeilen-Oberfl{\"a}che aus gestartet. Beispiel.",
isbn="978-3-322-89035-1",
doi="10.1007/978-3-322-89035-1_1",
url="https://doi.org/10.1007/978-3-322-89035-1_1"
}
@Inbook{Herrmann2001,
author={{Herrmann, D.}},
title="System-Programmierung (MS-DOS)",
bookTitle={{Effektiv Programmieren in C und C++: Eine aktuelle Einführung mit Beispielen aus Mathematik, Naturwissenschaft und Technik}},
year="2001",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="295--318",
abstract="Obwohl die in diesem Abschnitt verwendeten Schl{\"u}sselw{\"o}rter MS-DOS-spezifisch sind und daher nicht der ANSI C-Norm entsprechen, soll an einigen Beispielen gezeigt werden, wie man in C Zugang zum MS-DOS-Betriebssystem erh{\"a}lt. Eine vollst{\"a}ndige Darstellung ist im Rahmen des Buchs nat{\"u}rlich nicht m{\"o}glich. Es wird hier auf die Literatur [131 bzw. [22] verwiesen.",
isbn="978-3-322-94365-1",
doi="10.1007/978-3-322-94365-1_16",
url="https://doi.org/10.1007/978-3-322-94365-1_16"
}
@Inbook{Gerlach2019,
author={{Gerlach, S.}},
title={{Programmieren in C}},
bookTitle={{Computerphysik: Einführung, Beispiele und Anwendungen}},
year="2019",
publisher="Springer",
address="Berlin, Heidelberg",
pages="31--54",
abstract="Ein wichtiger Teil der Computerphysik ist die Implementierung eines Programms, d. h. das Programmieren selbst. Es gibt zwar Simulationspakete und Umgebungen, die spezielle Probleme ohne Programmierung l{\"o}sen k{\"o}nnen, aber erst mit der Programmierung besteht die M{\"o}glichkeit, eigene Probleme zu implementieren und zu l{\"o}sen. In diesem Kapitel werden zuerst die Grundlagen der Programmierung im Detail erl{\"a}utert. Danach folgt eine Einf{\"u}hrung in die Programmiersprache C, die eine hohe Verbreitung genie{\ss}t und auch zu gro{\ss}en Teilen in diesem Buch verwendet wird. Am Ende des Kapitels werden Hilfsmittel zur Programmierung, d.h. sog. Makefiles, Debugger und Versionsverwaltungen und deren Verwendung angesprochen.",
isbn="978-3-662-59246-5",
doi="10.1007/978-3-662-59246-5_4",
url="https://doi.org/10.1007/978-3-662-59246-5_4"
}
//
Binom
/////////////////////////////////
@Inbook{Philippou2025,
author={{Philippou, A. N., & Antzoulakos, D. L.}},
editor={{Lovric, M.}},
title="Binomial Distribution",
bookTitle="International Encyclopedia of Statistical Science",
year="2025",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="306--309",
isbn="978-3-662-69359-9",
doi="10.1007/978-3-662-69359-9_71",
url="https://doi.org/10.1007/978-3-662-69359-9_71"
}
@book{Collani2001,
title={Binomial Distribution Handbook for Scientists and Engineers},
author={{Collani, E., & Dräger, K.}},
series={Graduate Texts in Mathematics},
doi={10.1007/978-1-4612-0215-8},
publisher={Birkhäuser},
address={Boston, MA},
year={2001},
edition={1}
}
@Article{math10152680,
AUTHOR = {{García-García, J. I., Fernández Coronado, N. A., Arredondo, E. H., & Imilpán Rivera, I. A.}},
TITLE = {The Binomial Distribution: Historical Origin and Evolution of Its Problem Situations},
JOURNAL = {Mathematics},
VOLUME = {10},
YEAR = {2022},
NUMBER = {15},
ARTICLE-NUMBER = {2680},
URL = {https://www.mdpi.com/2227-7390/10/15/2680},
ISSN = {2227-7390},
ABSTRACT = {The increase in available probabilistic information and its usefulness for understanding the world has made it necessary to promote probabilistic literate citizens. For this, the binomial distribution is fundamental as one of the most important distributions for understanding random phenomena and effective decision making, and as a facilitator for the understanding of mathematical and probabilistic notions such as the normal distribution. However, to understand it effectively, it is necessary to consider how it has developed throughout history, that is, the components that gave it the form and meaning that we know today. To address this perspective, we identify the problem situations that gave origin to the binomial distribution, the operational and discursive practices developed to find solutions, and the conflicts that caused a leap in mathematical and probability heuristics, culminating in what is now known as the binomial distribution formula. As a result, we present five historical links to the binomial phenomenon where problem situations of increasing complexity were addressed: a case study using informal means (such as direct counting), the formalization of numerical patterns and constructs related to counting cases, specific probability calculus, the study and modeling of probability in variable or complex phenomena, and the use of the distribution formula as a tool to approaching notions such as the normal distribution. The periods and situations identified correspond to a required step in the design of binomial distribution learning from a historical epistemological perspective and when solving conflicts.},
DOI = {10.3390/math10152680}
}
@article{f22e64ae-c859-380d-809e-72c74d13957d,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2346943},
 abstract = {The sum of k independent and identically distributed (0, 1) variables has a binomial distribution. If the variables are identically distributed but not independent, this may be generalized to a two-parameter distribution where the k variables are assumed to have a symmetric joint distribution with no second- or higher-order "interactions". Two distinct generalizations are obtained, depending on whether the "multiplicative" or "additive" definition of "interaction" for discrete variables is used. The multiplicative generalization gives rise to a two-parameter exponential family, which naturally includes the binomial as a special case. Whereas with a beta-binomially distributed variable the variance always exceeds the corresponding binomial variance, the "additive" or "multiplicative" generalizations allow the variance to be greater or less than the corresponding binomial quantity. The properties of these two distributions are discussed, and both distributions are fitted, successfully, to data given by Skellam (1948) on the secondary association of chromosomes in Brassica.},
 author = {{Altham, P. M. E.}},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {2},
 pages = {162--167},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Two Generalizations of the Binomial Distribution},
 urldate = {2026-01-12},
 volume = {27},
 year = {1978}
}
@article{fbd8e02f-8560-36a2-a00c-e74f040810fc,
 ISSN = {0025570X, 19300980},
 URL = {http://www.jstor.org/stable/2689344},
 author = {{Goel, S. K., & Rodriguez, D. M.}},
 journal = {Mathematics Magazine},
 number = {4},
 pages = {225--228},
 publisher = {[Mathematical Association of America, Taylor & Francis, Ltd.]},
 title = {A Note on Evaluating Limits Using Riemann Sums},
 urldate = {2026-01-12},
 volume = {60},
 year = {1987}
}
@article{fa0257d0-e7e0-3c60-a790-1bc8b1adf0c3,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3215739},
 abstract = {We examine how the binomial distribution B(n, p) arises as the distribution Sn = ∑i=1n Xi of an arbitrary sequence of Bernoulli variables. It is shown that B (n, p) arises in infinitely many ways as the distribution of dependent and non-identical Bernoulli variables, and arises uniquely as that of independent Bernoulli variables. A number of illustrative examples are given. The cases B(2, p) and B(3, p) are completely analyzed to bring out some of the intrinsic properties of the binomial distribution. The conditions under which Sn follows B(n, p), given that Sn-1 is not necessarily a binomial variable, are investigated. Several natural characterizations of B(n , p), including one which relates the binomial distributions and the Poisson process, are also given.These results and characterizations lead to a better understanding of the nature of the binomial distribution and enhance the utility.},
 author = {{Vellaisamy, P.,  & Punnen, A. P.}},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {36--44},
 publisher = {Applied Probability Trust},
 title = {On the Nature of the Binomial Distribution},
 urldate = {2026-01-12},
 volume = {38},
 year = {2001}
}
//
Geom
/////////////////////////////////
@Inbook{Thomopoulos2017,
author={{Thomopoulos, N. T.}},
title="Geometric",
bookTitle="Statistical Distributions: Applications and Parameter Estimates ",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="127--133",
abstract="The geometric distribution applies when an experiment is run repeatedly until a successful outcome occurs, and the probability of a success is the same for all trials. The random variable could be defined as the number of fails till the first success, and has a range of integers zero and above. The random variable could also be labeled as the number of trials till the first success and the range is integers one and above. Both scenarios are described in the chapter. This situation occurs, for example, when a process produces units that need to meet acceptable engineering standards, and the process is repeated until an acceptable unit is produced. When the probability of a successful outcome is not known, sample data can be used to estimate the probability. Sometimes, no sample data is available, and a person of experience offers an approximation on the distribution and this data is used to estimate the probability of a successful outcome. The chapter also describes how the geometric distribution is the only discrete distribution that has a memory less property.",
isbn="978-3-319-65112-5",
doi="10.1007/978-3-319-65112-5_15",
url="https://doi.org/10.1007/978-3-319-65112-5_15"
}
@article{90db79eb-3312-380e-b218-3292416a6de2,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3213738},
 abstract = {Let X1, X2, ⋯, Xn be independent identically distributed positive integer-valued random variables with order statistics X1:n, X2:n, ⋯, Xn:n. We prove that if the random variable X2:n - X1:n is independent of the events [X1:n = m] and [X1:n = k], for fixed k > m > 1, then the Xi's are geometric. This is related to a characterization problem raised by Arnold (1980).},
 author = {{Sreehari M.}},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {209--212},
 publisher = {Applied Probability Trust},
 title = {A Characterization of the Geometric Distribution},
 urldate = {2026-01-13},
 volume = {20},
 year = {1983}
}
@article{Zijlstra_1983, 
title={Characterizations of the geometric distribution by distributional properties}, 
volume={20}, 
DOI={10.2307/3213595}, 
number={4}, 
journal={Journal of Applied Probability}, author={{Zijlstra, M.}},
 year={1983}, 
pages={843–850}
} 
//
Hyperg
/////////////////////////////////
@Inbook{Magnus1966b,
author={{Magnus, W., Oberhettinger, F., & Soni, R. P.}},
title="The hypergeometric function",
bookTitle="Formulas and Theorems for the Special Functions of Mathematical Physics",
year="1966",
publisher="Springer",
address="Berlin, Heidelberg",
pages="37--65",
abstract="The function represented by the infinite series {\$}{\$}{\backslash}sum{\backslash}limits{\_}{\{}n = 0{\}}^{\backslash}infty  {\{}{\backslash}frac{\{}{\{}{\{}{\{}(a){\}}{\_}n{\}}{\{}{\{}(b){\}}{\_}n{\}}{\}}{\}}{\{}{\{}{\{}{\{}(c){\}}{\_}n{\}}{\}}{\}}{\backslash}frac{\{}{\{}{\{}z^n{\}}{\}}{\}}{\{}{\{}n!{\}}{\}}{\}} {\$}{\$}within its circle of convergence and all the analytic continuations is called the hypergeometric function 2F1(a, b; c;z).*",
isbn="978-3-662-11761-3",
doi="10.1007/978-3-662-11761-3_2",
url="https://doi.org/10.1007/978-3-662-11761-3_2"
}
@Inbook{Sprent2025,
author={{Sprent, P.}},
editor={{Lovric, M.}},
title="The Fisher Exact Test",
bookTitle="International Encyclopedia of Statistical Science",
year="2025",
publisher="Springer",
address="Berlin, Heidelberg",
pages="961--962",
abstract="The Fisher Exact test was proposed by Fisher (1934) in the fifth edition of Statistical Methods for Research Workers. It is a test for independence as opposed to association in 2 {\texttimes} 2 contingency tables.",
isbn="978-3-662-69359-9",
doi="10.1007/978-3-662-69359-9_693",
url="https://doi.org/10.1007/978-3-662-69359-9_693"
}
@inbook{doi:https://doi.org/10.1002/9781118445112.stat04869,
author = {{Shuster, J. J.}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781118445112},
title = {Hypergeometric Distribution: Introduction},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {},
doi = {10.1002/9781118445112.stat04869},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat04869},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat04869},
year = {2014},
keywords = {binomial distribution, contingency tables, exact tests, independence, normal distribution, sampling without replacement, Yates's continuity correction},
abstract = {Abstract For a finite population of subjects of two types, suppose we select a random sample without replacement. The probability distribution of the number in the sample of one of the two types is the hypergeometric distribution. This article presents the hypergeometric distribution, summarizes its properties, discusses binomial and normal approximations, and presents a multivariate generalization.}
}
@inbook{doi:https://doi.org/10.1002/9781118445112.stat06165,
author = {{Hershberger, S. L.}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781118445112},
title = {Exact Methods for Categorical Data},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {},
doi = {10.1002/9781118445112.stat06165},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06165},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat06165},
year = {2014},
keywords = {asymptotic theory, nonparametric statistics, sampling distribution, resampling procedures, the bootstrap, Pitman test, Fisher's exact test, contingency table, hypergeometric distribution},
abstract = {Abstract Exact tests evaluate the significance of a test statistic by using the test statistic's empirically derived sampling distribution instead of a theoretical sampling distribution. Preference for exact tests may occur in situations in which the sample size is small, or when the data have distributional deficiencies. Two well-known exact tests, Pitman's test and Fisher's test, are described.}
}
@article{5d8fa06c-0d2a-391a-b97f-85223f1f3b0f,
 ISSN = {09641998, 1467985X},
 URL = {http://www.jstor.org/stable/2982890},
 abstract = {This paper reviews the problems that bedevil the selection of an appropriate test for the analysis of a 2 × 2 table. In contradiction to an earlier paper, the author now argues the case for the use of Fisher's exact test. It is noted that all test statistics for the 2 × 2 table have discrete distributions and it is suggested that it is irrational to prescribe an unattainable fixed significance level. The use of mid-P is suggested, if a formula is required for prescribing a variable tail probability. The problems of two-tail tests are discussed.},
 author = {{Upton, G. J. G.}},
 journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
 number = {3},
 pages = {395--402},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Fisher's Exact Test},
 urldate = {2026-01-12},
 volume = {155},
 year = {1992}
}
@article{202b9adf-f7a7-32c5-94dd-9f269c7f5a86,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984209},
 abstract = {A relationship is derived between the posterior probability of negative association of rows and columns of a 2 × 2 contingency table and Fisher's "exact" probability, as given in existing tables for testing the hypothesis of no association of rows and columns. The result for the 2 × 2 table is generalized to provide the posterior probability that one discrete-valued random variable is stochastically larger than another.},
 author = {{Altham, P. M. E.}},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {261--269},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Exact Bayesian Analysis of a 2 × 2 Contingency Table, and Fisher's "Exact" Significance Test},
 urldate = {2026-01-12},
 volume = {31},
 year = {1969}
}
@article{Camilli1995,
author={{Camilli, G.}},
year={1995},
title={The relationship between Fisher's exact test and Pearson's chi-square test: A bayesian perspective},
journal={Psychometrika},
page={305--312},
volume={60},
number={2},
abstract={It is demonstrated in this paper that two major tests for 2 × 2 talbes are highly related from a Bayesian perspective. Although it is well-known that Fisher's exact and Pearson's chi-square tests are asymptotically equivalent, the present analysis shows that a formal similarity also exists in small samples. The key assumption that leads to the resemblance is the presence of a continuous parameter measuring association. In particular, it is shown that Pearson's probability can be obtained by integrating a two-moment approximation to the posterior distribution of the log-odds ratio. Furthermore, Pearson's chi-square test gave an excellent approximation to the actual Bayes probability in all 2×2 tables examined, except for those with extremely disproportionate marginal frequencies.},
doi={10.1007/BF02301418}
}
//
Gamma
/////////////////////////////////
@Inbook{Koepf2014,
author={{Koepf, W.}},
title="The Gamma Function",
bookTitle="Hypergeometric Summation: An Algorithmic Approach to Summation and Special Function Identities",
year="2014",
publisher="Springer",
address="London",
pages="1--10",
abstract="Apart from the elementary transcendental functions such as the exponential and trigonometric functions and their inverses, the Gamma function is probably the most important transcendental function. It was defined by Euler to interpolate the factorials at noninteger arguments.",
isbn="978-1-4471-6464-7",
doi="10.1007/978-1-4471-6464-7_1",
url="https://doi.org/10.1007/978-1-4471-6464-7_1"
}
@Inbook{Magnus1966a,
author={{Magnus, W., Oberhettinger, F., & Soni, R. P.}},
title="The gamma function and related functions",
bookTitle="Formulas and Theorems for the Special Functions of Mathematical Physics",
year="1966",
publisher="Springer",
address="Berlin, Heidelberg",
pages="1--37",
abstract="The function $\Gamma$(z) is a meromorphic function of z with simple poles at z = −n, (n = 0, 1, 2,...) with the respective residue {\$}{\$}{\backslash}frac{\{}{\{}{\{}{\{}( - 1){\}}^n{\}}{\}}{\}}{\{}{\{}n!{\}}{\}}.{\$}{\$}.",
isbn="978-3-662-11761-3",
doi="10.1007/978-3-662-11761-3_1",
url="https://doi.org/10.1007/978-3-662-11761-3_1"
}
@article{BATIR2008187,
title = {On some properties of the gamma function},
journal = {Expositiones Mathematicae},
volume = {26},
number = {2},
pages = {187-196},
year = {2008},
issn = {0723-0869},
doi = {10.1016/j.exmath.2007.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0723086907000497},
author = {{Batir, N.}},
keywords = {Gamma function, Digamma function, Polygamma functions, Complete monotonicity, Riemann zeta-function},
abstract = {In this paper we prove a complete monotonicity theorem and establish some upper and lower bounds for the gamma function in terms of digamma and polygamma functions.}
}
@article{5bcebaa1-7494-336c-85e9-de3c540e9f64,
 ISSN = {0003486X, 19398980},
 URL = {http://www.jstor.org/stable/1968254},
 author = {{Rasch, G.}},
 journal = {Annals of Mathematics},
 number = {3},
 pages = {591--599},
 publisher = {[Annals of Mathematics, Trustees of Princeton University on Behalf of the Annals of Mathematics, Mathematics Department, Princeton University]},
 title = {Notes on the Gamma-Function},
 urldate = {2026-01-12},
 volume = {32},
 year = {1931}
}
@article{6b95b756-79df-32e7-8452-988d85f07718,
 ISSN = {0003486X, 19398980},
 URL = {http://www.jstor.org/stable/1967180},
 author = {{Gronwall, T. H.}},
 journal = {Annals of Mathematics},
 number = {2},
 pages = {35--124},
 publisher = {[Annals of Mathematics, Trustees of Princeton University on Behalf of the Annals of Mathematics, Mathematics Department, Princeton University]},
 title = {The Gamma Function in the Integral Calculus},
 urldate = {2026-01-12},
 volume = {20},
 year = {1918}
}
@article{7c7965cf-2aff-3860-a01c-d57527fbdc42,
 ISSN = {00029890, 19300972},
 URL = {https://www.jstor.org/stable/48663320},
 abstract = {Since its inception in 1894, the Monthly has printed 50 articles on the Γ function or Stirling’s asymptotic formula, including the magisterial 1959 paper by Phillip J. Davis, which won the 1963 Chauvenet prize, and the eye-opening 2000 paper by the Fields medalist Manjul Bhargava. In this article, we look back and comment on what has been said, and why, and try to guess what will be said about the Γ function in future Monthly issues. We also identify some gaps, which surprised us: phase plots, Riemann surfaces, and the functional inverse of Γ make their first appearance in the Monthly here. We also give a new elementary treatment of the asymptotics of n! and the first few terms of a new asymptotic formula for invΓ.},
 author = {{Borwein, J. M., & Corless, R. M.}},
 journal = {The American Mathematical Monthly},
 number = {5},
 pages = {400--424},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Gamma and Factorial in the Monthly},
 urldate = {2026-01-12},
 volume = {125},
 year = {2018}
}
@article{f70ecf4e-6c25-3732-9fda-1e8986c07b3c,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2309786},
 author = {{Davis, P. J.}},
 journal = {The American Mathematical Monthly},
 number = {10},
 pages = {849--869},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Leonhard Euler's Integral: A Historical Profile of the Gamma Function: In Memoriam: Milton Abramowitz},
 urldate = {2026-01-12},
 volume = {66},
 year = {1959}
}
//
Distr
/////////////////////////////////
//
// F chi² //
//
@Inbook{Jolicoeur1999,
author={{Jolicoeur, P.}},
title="The distribution of $\chi$2 (chi squared)",
bookTitle="Introduction to Biometry",
year="1999",
publisher="Springer US",
address="Boston, MA",
pages="38--39",
abstract="The distribution of $\chi$2(chi squared) is a continuous and asymmetrical distribution which ranges from 0 to + ∞ and is followed by a sum of squares of independent standardized normal variates. The $\chi$2distribution and its application to frequency tables (chapter 15) were discovered by the British biometrician Karl Pearson (1857--1936), who was considered as the ``father of biometry'' and was one of the founders of the periodical Biometrika. The $\chi$2distribution is exact when hypotheses must be tested or confidence intervals must be determined (chapter 10) concerning the parametric variance $\sigma$2of a continuous variate X which follows a normal distribution (chapter 5). Moreover, the $\chi$2distribution may be used as an approximation in many cases, including Bartlett's test of the homogeneity of variance (section 12.7), tests of hypotheses concerning frequency tables (chapter 15), tests of goodness of fit (chapters 16, 17 and 18), the binomiality (chapter 17) and Poissonianity (chapter 18) tests, etc.. Paradoxically, the approximate utilizations of the $\chi$2distribution are perhaps better known than the exact ones.",
isbn="978-1-4615-4777-8",
doi="10.1007/978-1-4615-4777-8_8",
url="https://doi.org/10.1007/978-1-4615-4777-8_8"
}
@misc{das2025newmethodscomputegeneralized,
      title={New methods to compute the generalized chi-square distribution}, 
author={{Das, A.}},
year={2025},
eprint={2404.05062},
archivePrefix={arXiv},
primaryClass={stat.CO},
url={https://arxiv.org/abs/2404.05062}, 
}
@Inbook{Chattamvelli2021,
author={{Chattamvelli, R., & Shanmugam, R.}},
title={{F Distribution}},
bookTitle="Continuous Distributions in Engineering and the Applied Sciences -- Part II",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="227--234",
isbn="978-3-031-02435-1",
doi="10.1007/978-3-031-02435-1_7",
url="https://doi.org/10.1007/978-3-031-02435-1_7"
}
@Inbook{Herrmann1984,
author={{Herrmann, D.}},
title={{F-Verteilung}},
bookTitle="Wahrscheinlichkeitsrechnung und Statistik --- 30 BASIC-Programme",
year="1984",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="40--41",
abstract="Die letzte der drei wichtigsten Test-Verteilungen ist die F-Verteilung mit der Verteilungsfunktion Die Bedeutung der F-Verteilung liegt darin, da{\ss} mit zwei $\chi$2 -verteilten Gr{\"o}{\ss}en X2 und Y2 mit den Freiheitsgraden f1 bzw. f2F-verteilt ist. Die F-Verteilung hat somit zwei Freiheitsgrade. Sie ist eine sehr allgemeine Funktion, denn sie geht f{\"u}r f1 = 1, t = {\textsurd}F in die t-Verteilung f1 = 1, f2 = ∞, z = {\textsurd}F in die Normalverteilung {\"u}ber.",
isbn="978-3-322-96320-8",
doi="10.1007/978-3-322-96320-8_21",
url="https://doi.org/10.1007/978-3-322-96320-8_21"
}
@Inbook{Behnke2006,
author={{Behnke, J., & Behnke, N.}},
title="Verteilungen, die von der Standardnormalverteilung abgeleitet werden k{\"o}nnen",
bookTitle="Grundlagen der statistischen Datenanalyse: Eine Einf{\"u}hrung f{\"u}r Politikwissenschaftler",
year="2006",
publisher="VS Verlag f{\"u}r Sozialwissenschaften",
address="Wiesbaden",
pages="344--355",
abstract="Die Standardnormalverteilung ist ohne Zweifel die wichtigste und grundlegendste Verteilungsform in der Statistik. Sie ist von fundamentaler Bedeutung f{\"u}r die Sch{\"a}tzung von Parametern, aber auch zur Konstruktion von Teststatistiken. Nicht alle Teststatistiken aber k{\"o}nnen so konstruiert werden, dass sie normalverteilt sind. Die Teststatistiken von wichtigen statistischen Tests haben oft auch andere Verteilungsformen, die allerdings aus der Standardnormalverteilung abgeleitet werden k{\"o}nnen. Die wichtigsten dieser weiteren Verteilungen sind die so genannte Chi -Verteilung und die F-Verteilung. Die Chi2 -Verteilung wird unter anderem bei der Untersuchung von Zusammenh{\"a}ngen zwischen nominalskalierten Variablen eingesetzt, aber sie kann in einem wesentlich weiteren Sinn in vielen F{\"a}llen f{\"u}r den Vergleich einer theoretischen Verteilung mit einer empirischen Verteilung herangezogen werden. Die F-Verteilung spielt vor allem f{\"u}r den Vergleich von Varianzen eine herausragende Rolle. Schon bekannt ist uns au{\ss}erdem die T-Verteilung, die im Prinzip {\"u}berall dort Verwendung findet, wo auch die Standard-normalverteilung eingesetzt werden kann, wo aber die Standardabweichung der Grundgesamtheit erst mit Hilfe der Standardabweichung der Stichprobe gesch{\"a}tzt werden muss. Wir greifen die T-Verteilung in diesem Kapitel noch einmal kurz auf, um ihre Beziehung zur Standardnormalverteilung noch etwas genauer zu kl{\"a}ren.",
isbn="978-3-531-90003-2",
doi="10.1007/978-3-531-90003-2_26",
url="https://doi.org/10.1007/978-3-531-90003-2_26"
}
@Inbook{Stange1970,
author={{Stange, K.}},
title={{Die F-Verteilung}},
bookTitle="Angewandte Statistik: Erster Teil Eindimensionale Probleme",
year="1970",
publisher="Springer",
address="Berlin, Heidelberg",
pages="357--372",
abstract="Vorgelegt seien zwei Normalverteilungen 1 und 2, die beispielsweise zu zwei Erzeugungsvorg{\"a}ngen geh{\"o}ren, mit unbekannten Varianzen $\sigma$12und $\sigma$22. Man will entscheiden, ob beide Vorg{\"a}nge mit der gleichen Varianz f{\"u}r die Merkmalwerte ablaufen. Mit anderen Worten: Es soll die Hypothese $\sigma$12= $\sigma$22getestet werden. Verf{\"u}gbar ist aus jeder Verteilung eine Zufallsprobe der Gr{\"o}{\ss}e n1 bzw. n2 mit der Varianz s12bzw. s22. Als Pr{\"u}fgr{\"o}{\ss}e, mit der man die Entscheidung f{\"a}llt, w{\"a}hlt man das Verh{\"a}ltnis s12/s22der Stichprobenvarianzen.",
isbn="978-3-642-85602-0",
doi="10.1007/978-3-642-85602-0_11",
url="https://doi.org/10.1007/978-3-642-85602-0_11"
}
@Inbook{Hafner1992,
author={{Hafner, R.}},
title="Der Chi-Quadrat-Test",
bookTitle="Statistik: f{\"u}r Sozial- und Wirtschaftswissenschaftler",
year="1992",
publisher="Springer",
address="Vienna",
pages="164--170",
abstract="Die in den Kapiteln 11, 12 und 13 behandelten statistischen Hypothesen waren Parameterhypothesen in dem Sinne, da{\ss} Aussagen {\"u}ber einzelne Verteilungsparameter (z.B.:  gepr{\"u}ft werden sollten. H{\"a}ufig hat man aber allgemeinere Aussagen {\"u}ber die den Daten zugrundeliegende Verteilung zu testen. Man m{\"o}chte zum Beispiel wissen, ob die Annahme, da{\ss} die Daten normal-verteilt sind, haltbar ist, oder ob zwei Verteilungen deutlich oder kaum verschieden sind.",
isbn="978-3-7091-3420-7",
doi="10.1007/978-3-7091-3420-7_14",
url="https://doi.org/10.1007/978-3-7091-3420-7_14"
}
@article{https://doi.org/10.1002/cem.2734,
author = {{Brereton, R. G.}},
title = {{The F distribution and its relationship to the chi squared and t distributions}},
journal = {Journal of Chemometrics},
volume = {29},
number = {11},
pages = {582--586},
doi = {10.1002/cem.2734},
url = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.2734},
eprint = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/pdf/10.1002/cem.2734},
year = {2015}
}
@inbook{doi:https://doi.org/10.1002/9781118445112.stat05856,
author = {{Selvin, S.}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781118445112},
title = {{F Distributions}},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {},
doi = {10.1002/9781118445112.stat05856},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat05856},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat05856},
year = {2014},
keywords = {-distribution, variance, probability distribution},
abstract = {Abstract The F-distribution is fundamental to much of statistical analysis, particularly the analysis of variance. The article entitled “F-Distribution” is a brief but complete description of this probability distribution. Its properties are first discussed, followed by a description of the relationship between the F-distribution and other common statistical distributions (e.g. beta, t-, chi-square, and binomial distributions). The article concludes with a typical application of this distribution to a small set of sampled observations.}
}
@article{CANAL2005803,
title = {A normal approximation for the chi-square distribution},
journal = {Computational Statistics & Data Analysis},
volume = {48},
number = {4},
pages = {803--808},
year = {2005},
issn = {0167-9473},
doi = {10.1016/j.csda.2004.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167947304001069},
author = {{Canal, L.}},
keywords = {Chi-square distribution, Maximum absolute error, Normal approximation, Power transformation},
abstract = {An accurate normal approximation for the cumulative distribution function of the chi-square distribution with n degrees of freedom is proposed. This considers a linear combination of appropriate fractional powers of chi-square. Numerical results show that the maximum absolute error associated with the new transformation is substantially lower than that found for other power transformations of a chi-square random variable for all the degrees of freedom considered (1⩽n⩽1000).}
}
@article{a71e7dcf-67b3-3b81-a0c2-17153341fab1,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2335780},
 abstract = {The distribution of the χ2 test under the null hypothesis is studied, when the parameters are estimated by the method of moments. A general formula, applicable also to other situations, is given. Three examples are studied in more detail and numerical results are given, indicating how unsafe it can be to use a χ2 distribution with a number of degrees of freedom smaller than or equal to the number of cells.},
 author = {{Molinari, L.}},
 journal = {Biometrika},
 number = {1},
 pages = {115--121},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Distribution of the Chi-Squared Test in Nonstandard Situations},
 urldate = {2026-01-13},
 volume = {64},
 year = {1977}
}
@article{b3bc6554-760a-35aa-92dd-1ef8068d343e,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2985223},
 abstract = {A procedure for interpolating in tables of the F-distribution is proposed. This procedure is already known by many statisticians, although its original source has been forgotten. A table gives an upper bound for the relative error in the interpolation between some tabulated values. An example is given.},
 author = {{Zinger, A.}},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {51--53},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {{On Interpolation in Tables of the F-Distribution}},
 urldate = {2026-01-13},
 volume = {13},
 year = {1964}
}
@article{b71582f9-54b0-3efd-95a7-a60d1005dd3b,
 ISSN = {00278424, 10916490},
 URL = {http://www.jstor.org/stable/86022},
 author = {{Wilson, E. B., & Hilferty, M. M.}},
 journal = {Proceedings of the National Academy of Sciences of the United States of America},
 number = {12},
 pages = {684--688},
 publisher = {National Academy of Sciences},
 title = {The Distribution of Chi-square},
 urldate = {2026-01-13},
 volume = {17},
 year = {1931}
}
@article{c9bb1879-be2e-3251-9489-81b3cdab8ab2,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2282687},
 abstract = {A relation is derived between the percentile points of a t-distribution with n degrees of freedom and those of an F-distribution with n and n degrees of freedom. In effect, the t-percentiles can be obtained by a simple transformation from the "diagonal" entries of an F-table.},
 author = {{Cacoullos, T.}},
 journal = {Journal of the American Statistical Association},
 number = {310},
 pages = {528--531},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {{A Relation Between t and F-Distributions}},
 urldate = {2026-01-13},
 volume = {60},
 year = {1965}
}
@article{Saunders_Moran_1978, 
title={{On the quantiles of the gamma and F distributions}}, 
volume={15}, 
DOI={10.2307/3213414}, 
number={2}, 
journal={Journal of Applied Probability}, author={{Saunders, I. W., & Moran, P. A. P.}}, year={1978}, 
pages={426--432}
}
@article{e800b171-dd42-3678-b651-8375c1fde110,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2332542},
 author = {{Patnaik, P. B.}},
 journal = {Biometrika},
 number = {1/2},
 pages = {202--232},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {{The Non-Central χ2- and F-Distribution and their Applications}},
 urldate = {2026-01-13},
 volume = {36},
 year = {1949}
}
// Fisher Z //
//
@article{CARBONELL2009780,
title = {{On the Fisher’s Z transformation of correlation random fields}},
journal = {Statistics & Probability Letters},
volume = {79},
number = {6},
pages = {780--788},
year = {2009},
issn = {0167-7152},
doi = {10.1016/j.spl.2008.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167715208005154},
author = {{Carbonell, F., Worsley, K. J., & Trujillo-Barreto, N. J.}},
abstract = {One of the most interesting problems studied in Random Field Theory (RFT) is to approximate the distribution of the maximum of a random field. This problem usually appears in a general hypothesis testing framework, where the statistics of interest are the maximum of a random field of a known distribution. In this paper, we use the RFT approach to compare two independent correlation random fields, R1 and R2. Our statistics of interest are the maximum of a random field G, resulting from the difference between the Fisher’s Z transformation of R1 and R2, respectively. The Fisher’s Z transformation guarantees a Gaussian distribution at each point of G but, unfortunately, G is not transformed into a Gaussian random field. Hence, standard results of RFT for Gaussian random fields are not longer available for G. We show here that the distribution of the maximum of G can still be approximated by the distribution of the maximum of a Gaussian random field, provided there is some correction by its spatial smoothness. Indeed, we present a general setting to obtain this correction. This is done by allowing different smoothness parameters for the components of G. Finally, the performance of our method is illustrated by means of both numerical simulations and real Electroencephalography data, recorded during a face recognition experimental paradigm.}
}
@article{67d2aa04-e169-3e9d-9c2d-31d37c595ef4,
 ISSN = {00220973, 19400683},
 URL = {http://www.jstor.org/stable/20156574},
 author = {{Hjelm, H. F., & Norris, R. C.}},
 journal = {The Journal of Experimental Education},
 number = {3},
 pages = {269--277},
 publisher = {Taylor & Francis, Ltd.},
 title = {{Empirical Study of the Efficacy of Fisher's Z-Transformation}},
 urldate = {2026-01-13},
 volume = {30},
 year = {1962}
}
@article{Bond_Richardson_2004, 
title={{Seeing the Fisher Z-transformation}}, volume={69}, 
DOI={10.1007/BF02295945}, 
number={2}, 
journal={Psychometrika}, 
author={{Bond, C. F., & Richardson, K.}}, 
year={2004}, 
pages={291–303}
}
@article{Mendoza_1993, 
title={Fisher Transformations for Correlations Corrected for Selection and Missing Data}, volume={58}, 
DOI={10.1007/BF02294830}, 
number={4}, 
journal={Psychometrika}, 
author={{Mendoza, J. L.}}, 
year={1993}, 
pages={601–615}
}
@article{fcdfa5ac-5e28-3f56-8f5f-82b584b870a3,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2332676},
 author = {{David, F. N.}},
 journal = {Biometrika},
 number = {3/4},
 pages = {394--403},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {{The Moments of the z and F Distributions}},
 urldate = {2026-01-13},
 volume = {36},
 year = {1949}
}
@article{Yang2013,
author = {{Yang, Z., Duan, Z., Wang, J., Wang, T., Song, Y., & Zhang, J.}},
year={2013},
title = {{Quadratic radical function better than fisher Z transformation}},
journal = {Transactions of Tianjin University},
pages =  {381--384},
volume = {19},
volume = {5},
abstract={A new explicit quadratic radical function is found by numerical experiments, which is simpler and has only 70.778% of the maximal distance error compared with the Fisher z transformation. Furthermore, a piecewise function is constructed for the standard normal distribution: if the independent variable falls in the interval (−1.519, 1.519), the proposed function is employed; otherwise, the Fisher z transformation is used. Compared with the Fisher z transformation, this piecewise function has only 38.206% of the total error. The new function is more exact to estimate the confidence intervals of Pearson product moment correlation coefficient and Dickinson best weights for the linear combination of forecasts},
doi={10.1007/s12209-013-1978-8}
}
//
Norm
/////////////////////////////////
//
@Inbook{Leaver1974,
author={{Leaver, R. H.,  & Thomas, T. R.}},
title="Normal Distribution",
bookTitle="Analysis and Presentation of Experimental Results",
year="1974",
publisher="Macmillan Education UK",
address="London",
pages="30--37",
abstract="There are many different frequency distributions in statistics, some of which we shall meet later in the book, but one in particular applies to the theory of errors. This is the normal or Gaussian distribution. The word `normal' is here used in its original sense of `ideal', for it was believed for a long time that this distribution was a reflection of a universal and fundamental law of nature. If certain assumptions are made it can be `proved' that the accumulation of a large number of small independent random deviations follows a normal law, but belief in the rigour of this proof is no longer widespread. There is a well-known quotation on the subject: `Everybody believes in the Gaussian law of errors; the experimenters because they think it will be proved by mathematics; and the mathematicians because they believe it has been established by observation.' However, for our present purposes we can assume that the Gaussian distribution is the best possible representation of the variation of small random errors.",
isbn="978-1-349-01942-7",
doi="10.1007/978-1-349-01942-7_4",
url="https://doi.org/10.1007/978-1-349-01942-7_4"
}
@Inbook{Mulholland1968a,
author={{Mulholland, H., & Jones, C. R.}},
title="Elementary Probability",
booktitle="Fundamentals of Statistics",
year="1968",
publisher="Springer US",
address="Boston, MA",
pages="32--58",
abstract="The phrases `it is probable that', `it is very likely that', `there is virtually no chance' are often used in conversation. They are used to describe events which could possibly happen. They represent our estimates of the probability of the events happening. They are sometimes based on personal feelings, but more often they are based on relative frequency, i.e. the proportion of times such an event has happened previously. These subjective or intuitive ideas of probability are basic and fundamental in life and they give rise to two definitions of probability which are now discussed.",
isbn="978-1-4899-6507-3",
doi="10.1007/978-1-4899-6507-3_3",
url="https://doi.org/10.1007/978-1-4899-6507-3_3"
}
@Inbook{Mulholland1968b,
author={{Mulholland, H., & Jones, C. R.}},
title="The Normal Distribution",
booktitle="Fundamentals of Statistics",
year="1968",
publisher="Springer US",
address="Boston, MA",
pages="122--138",
abstract="The normal distribution is probably the most widely studied distribution in statistics because :(a)Distributions which are approximately normal are frequently encountered, e.g. most sets of random errors follow the normal distribution.(b)The normal distribution is important as a `limiting distribution', i.e. it can be used as an approximation to other distributions (see sections 8.7 and 8.8).(c)The normal distribution is easy to use.(d)It has been shown that the results obtained by assuming a non-normal population to be normally distributed are reasonably accurate when the departure from normality is not too severe.(e)The central limit theorem shows that the means of samples of size n from any population are approximately normally distributed. The approximation improves as n gets bigger.",
isbn="978-1-4899-6507-3",
doi="10.1007/978-1-4899-6507-3_8",
url="https://doi.org/10.1007/978-1-4899-6507-3_8"
}
@Inbook{Sachs1993,
author={{Sachs, L.}},
title="Normalverteilung",
bookTitle="Statistische Methoden: Planung und Auswertung",
year="1993",
publisher="Springer",
address="Berlin, Heidelberg",
pages="41--54",
abstract="Bedeutung der Normalverteilung: Die Normalverteilung ist ein wichtiges Modell: es beschreibt die Streuung von Me{\ss}werten um ihren Mittelwert. Me{\ss}fehler sind als zuf{\"a}llige Fehler [unvermeidbar, symmetrisch, meist klein] oft angen{\"a}hert normalverteilt. Der Praktiker mu{\ss} sich damit abfinden, da{\ss} es, streng genommen, in der Empirie keine Normalverteilung gibt. Indessen lassen sich viele mehr oder weniger symmetrisch-eingipflig verteilte Beobachtungen zumindest in ihrem mittleren Bereich als angen{\"a}hert normalverteilt auffassen.",
isbn="978-3-642-77717-2",
doi="10.1007/978-3-642-77717-2_4",
url="https://doi.org/10.1007/978-3-642-77717-2_4"
}
@Inbook{Saneii2024,
author={{Saneii, S. H., & Doosti, H.}},
title="Normal Distribution",
bookTitle="Practical Biostatistics for Medical and Health Sciences",
year="2024",
publisher="Springer",
address="Singapore",
pages="115--137",
isbn="978-981-97-3083-4",
doi="10.1007/978-981-97-3083-4_6",
url="https://doi.org/10.1007/978-981-97-3083-4_6"
}
@inbook{Wesolowski2018,
author = {{Wesolowski, B., & Musselwhite Thompson, D.}},
booktitle={The SAGE Encyclopedia of Educational Research, Measurement, and Evaluation},
editor={{Frey, B. B.}},
publisher={University of Kansas},
address={USA},
year = {2018},
title = {Normal Distribution},
doi = {10.4135/9781506326139.n476}
}
@article{Benzon2021,
author = {{Benzon, B.}},
year = {2021},
month = {07},
pages = {1--16},
title = {The normal distribution, an epistemological view},
volume = {2},
journal = {St open},
doi = {10.48188/so.2.6}
}
@article{https://doi.org/10.1002/cem.1382,
author = {{Tóth, G.}},
title = {Destruction of normal distribution in small samples by centering and scaling},
journal = {Journal of Chemometrics},
volume = {25},
number = {5},
pages = {247--253},
keywords = {centering, scaling, studentization, standardization, normal distribution},
doi = {10.1002/cem.1382},
url = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.1382},
eprint = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/pdf/10.1002/cem.1382},
abstract = {Abstract It is less emphasized in scientific literature that centering and scaling of data may drastically change the original distribution of the data for small samples. The destruction of the original distribution depends on the source of the estimation of the mean (centering) and the divisor (scaling) where the latter is connected to the spread of data. In our comparative study we focus on cases, where the sample is taken from normally distributed data; the means and the standard deviations are population or sample based. We discuss six cases in transforming the data or the sample means. Most of them are studied previously, but some of them have not been theoretically investigated. The transformed data follow normal distribution in three cases, if the scaling is performed with population standard deviation. In one case, the final distribution is related to β-distribution with astonishing density functions for N = 2 Dirac-delta, N = 3 Viking helmet like and N = 4 uniform distributions. Another case is the well-known t-distribution. For one of the transformed data, we were not able to identify the general form. Here we obtained only numerical results for 3 ≤ N. The effect of the transformations was tested on experimental data representing more or less normally distributed variables. We found that transformations using the sample standard deviation were significantly less normally distributed-like than the original data for small samples, but the other transformations enhanced the normal distribution-like feature. The results point out that centering and especially scaling require consideration for small samples up to questioning the reality of subsequent data evaluation processes where normal distribution is assumed. Copyright © 2011 John Wiley \& Sons, Ltd.},
year = {2011}
}
@article{https://doi.org/10.1002/cem.2655,
author = {{Brereton, R. G.}},
title = {The normal distribution},
journal = {Journal of Chemometrics},
volume = {28},
number = {11},
pages = {789--792},
doi = {10.1002/cem.2655},
year = {2014}
}
@article{WANG2011903,
title = {A simple characterization of Student’s distributions and normal distributions},
journal = {Statistics & Probability Letters},
volume = {81},
number = {8},
pages = {903--906},
year = {2011},
issn = {0167-7152},
doi = {10.1016/j.spl.2011.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167715211001283},
author = {{Wang, J.}},
keywords = {Characterization, Order statistics, Regressional property},
abstract = {Let X1:3≤X2:3≤X3:3 be the order statistics of the independent and identically distributed random variables X1, X2, and X3. Using the regressional property E[(aX2:3−X1:3)2|X2:3=x]=E[(X3:3−aX2:3)2|X2:3=x], we characterize some distributions, including Student’s tν distributions (ν>2) and normal distributions.}
}
@incollection{199847,
title = {Chapter 3 The normal distribution},
editor = {{Massart, D. L., Vandeginste, B. G. M., Buydens, L. M. C., De Jong, S., Lewi, P. J., & Smeyers-Verbeke, J.}},
series = {Data Handling in Science and Technology},
publisher = {Elsevier},
volume = {20},
pages = {47--72},
year = {1998},
booktitle = {Handbook of Chemometrics and Qualimetrics: Part A},
issn = {0922-3487},
doi = {10.1016/S0922-3487(97)80033-0},
url = {https://www.sciencedirect.com/science/article/pii/S0922348797800330},
abstract = {Publisher Summary
This chapter focuses on the concept of normal distribution. It also describes population parameters and their estimators. To summarize the characteristics of a distribution, its moments can be used. The best known probability distribution is the normal distribution. Distributions can be non-normal and procedures or tests are needed to detect this departure from normality. A graphical procedure is described in the chapter that permits to indicate whether a distribution is normal or not. The graphical procedure applied is called the “rankit procedure.” A bacteriological example provides a clue to the way to make non-normal distributions normal— namely, by transformation. The transformation carried out in the chapter is called the “log-transformation.” Log-normal distributions are frequently found in nature, particularly when the variable studied has a natural zero (such as weight, length, etc.). In this case, simple normality around the mean could include negative values.}
}
@article{doi:10.1093/bjps/axs046,
author = {{Lyon, A.}},
title = {Why are Normal Distributions Normal?},
journal = {The British Journal for the Philosophy of Science},
volume = {65},
number = {3},
pages = {621--649},
year = {2014},
doi = {10.1093/bjps/axs046},
abstract = {It is usually supposed that the central limit theorem explains why various quantities we find in nature are approximately normally distributed—people's heights, examination grades, snowflake sizes, and so on. This sort of explanation is found in many textbooks across the sciences, particularly in biology, economics, and sociology. Contrary to this received wisdom, I argue that in many cases we are not justified in claiming that the central limit theorem explains why a particular quantity is normally distributed, and that in some cases, we are actually wrong. 1 Introduction2 Normal Distributions and the Central Limit Theorem  2.1 Normal distributions  2.2 The central limit theorem  2.3 Terminology3 Explaining Normality  3.1 Loaves of bread  3.2 Varying variances and probability densities  3.3 Tensile strengths and problems with summation  3.4 Products of factors and log-normal distributions  3.5 Transforming factors and sub-factors  3.6 Transformations of quantities  3.7 Quantitative genetics  3.8 Inference to the best explanation4 Maximum Entropy Explanations5 Conclusion }
}
@article{2e210a69-a2e7-3fa7-96e2-b30efac9ed56,
 ISSN = {0025570X, 19300980},
 URL = {http://www.jstor.org/stable/27642916},
 author = {{Stahl, S.}},
 journal = {Mathematics Magazine},
 number = {2},
 pages = {96--113},
 publisher = {Mathematical Association of America},
 title = {The Evolution of the Normal Distribution},
 urldate = {2026-01-14},
 volume = {79},
 year = {2006}
}
@article{8ccd5f0b-f3d0-3894-a051-95d6e3d4ed98,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2333749},
 author = {{Breitenberger, E.}},
 journal = {Biometrika},
 number = {1/2},
 pages = {81--88},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Analogues of the Normal Distribution on the Circle and the Sphere},
 urldate = {2026-01-14},
 volume = {50},
 year = {1963}
}
@article{57fba293-9d42-3fd4-9780-f77567f507d7,
 ISSN = {09366784},
 URL = {http://www.jstor.org/stable/40986007},
 author = {{Thome, H.}},
 journal = {Historical Social Research / Historische Sozialforschung. Supplement},
 number = {3},
 pages = {1--275},
 publisher = {GESIS - Leibniz-Institute for the Social Sciences, Center for Historical Social Research},
title = {GRUNDKURS STATISTIK FÜR HISTORIKER TEIL II: INDUKTIVE STATISTIK UND REGRESSIONSANALYSE},
 urldate = {2026-01-14},
 year = {1990}
}
@article{adfd1cdd-be2e-33b1-bf94-693f4754181e,
 ISSN = {02664666, 14694360},
 URL = {http://www.jstor.org/stable/43948011},
 abstract = {We study the asymptotic covariance function of the sample mean and quantile, and derive a new and surprising characterization of the normal distribution: the asymptotic covariance between the sample mean and quantile is constant across all quantiles, if and only if the underlying distribution is normal. This is a powerful result and facilitates statistical inference. Utilizing this result, we develop a new omnibus test for normality based on the quantile-mean covariance process. Compared to existing normality tests, the proposed testing procedure has several important attractive features. Monte Carlo evidence shows that the proposed test possesses good finite sample properties. In addition to the formal test, we suggest a graphical procedure that is easy to implement and visualize in practice. Finally, we illustrate the use of the suggested techniques with an application to stock return datasets.},
 author = {{Bera, A. K., Galvao, A. F., Wang, L., & Xiao, Z.}},
 journal = {Econometric Theory},
 number = {5},
 pages = {1216--1252},
 publisher = {Cambridge University Press},
 title = {{A New Characterisazion Of The Normal Distribution And Test For Normality}},
 urldate = {2026-01-14},
 volume = {32},
 year = {2016}
}
@inbook{Storch_Zwiers_1999, 
place={Cambridge}, 
title={Normal Density and Cumulative Distribution Function}, 
booktitle={Statistical Analysis in Climate Research}, publisher={Cambridge University Press}, 
author={{von Storch, H., & Zwiers, F. W.}}, 
year={1999}, 
doi={10.1017/CBO9780511612336},
pages={419--420}
} 
//Pois
/////////////////////////////////
//
@Inbook{Jolicoeur1999,
author={{Jolicoeur, P.}},
title="The Poisson distribution",
bookTitle="Introduction to Biometry",
year="1999",
publisher="Springer",
address="Boston, MA",
pages="124--133",
abstract="The Poisson distribution is a discontinuous distribution which was described by the French mathematician Sim{\'e}on Denis Poisson (1781-2013;1840) in a book published in 1837. The Poisson distribution is a particular case of the binomial distribution (chapter 17) where the probability p becomes smaller and smaller (p {\textrightarrow} 0) while the exponent k becomes larger and larger (k{\textrightarrow}∞) in such a way that their product, the mean, $\mu$= kp, keeps a finite value (0 < $\mu$ = kp < ∞). The Poisson distribution might thus be defined by the following equation",
isbn="978-1-4615-4777-8",
doi="10.1007/978-1-4615-4777-8_19",
url="https://doi.org/10.1007/978-1-4615-4777-8_19"
}
@article{Zhang2025,
author={{Zhang, Y.}},
year={2025},
title={Poisson Distribution and Its Applications},
journal={Advances in Economics, Management and Political Sciences},
volume={196},
page={1--6},
doi={10.54254/2754-1169/2025.BJ24761}
}
@Inbook{Chung1974,
author={{Chung, K. L.}},
title="Poisson and Normal Distributions",
bookTitle="Elementary Probability Theory with Stochastic Processes",
year="1974",
publisher="Springer",
address="New York, NY",
pages="192--239",
abstract="The Poisson distribution is of great importance in theory and in practice. It has the added virtue of being a simple mathematical object. We could have introduced it at an earlier stage in the book, and the reader was alerted to this in {\textsection}4.4. However, the belated entrance will give it more prominence, as well as a more thorough discussion than would be possible without the benefit of the last two chapters.",
isbn="978-1-4757-3973-2",
doi="10.1007/978-1-4757-3973-2_7",
url="https://doi.org/10.1007/978-1-4757-3973-2_7"
}
@article{a2ab11c2-825b-3d03-aa88-542d1bf1629c,
 ISSN = {03063127},
 URL = {http://www.jstor.org/stable/284821},
 abstract = {Social determinists have argued that the occurrence of independent discoveries and inventions demonstrates the inevitability of techno-scientific progress. Yet the frequency of such multiples may be adequately predicted by a probabilistic model, especially the Poisson model suggested by Price. A detailed inquiry reveals that the Poisson distribution can predict almost all the observed variation in the frequency distribution of multiples collected by Merton, and by Ogburn and Thomas. This study further indicates that: (a) the number of observed multiples may be greatly underestimated, particularly those involving few independent contributors; (b) discoveries and inventions are not sufficiently probable to avoid a large proportion of total failures, and hence techno-scientific advance is to a large measure indeterminate; (c) chance or 'luck' seems to play such a major part that the 'great genius' theory is no more tenable than the social deterministic theory.},
 author = {{Simonton, D. K.}},
 journal = {Social Studies of Science},
 number = {4},
 pages = {521--532},
 publisher = {Sage Publications, Ltd.},
 title = {Independent Discovery in Science and Technology: A Closer Look at the Poisson Distribution},
 urldate = {2026-01-12},
 volume = {8},
 year = {1978}
}
@article{beaf2bb2-3f3e-3d37-837e-422ad120324f,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2333201},
 author = {{Crow, E. L.}},
 journal = {Biometrika},
 number = {3/4},
 pages = {556--559},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Mean Deviation of the Poisson Distribution},
 urldate = {2026-01-12},
 volume = {45},
 year = {1958}
}
@article{c5daa63c-82d0-3414-9df1-e64522b8f21e,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/3001466},
 author = {{Rao, C. R., & Chakravarti, I.M.}},
 journal = {Biometrics},
 number = {3},
 pages = {264--282},
 publisher = {[Wiley, International Biometric Society]},
 title = {Some Small Sample Tests of Significance for a Poisson Distribution},
 urldate = {2026-01-12},
 volume = {12},
 year = {1956}
}
@book{haight1967handbook,
  title={Handbook of the Poisson Distribution},
  author={{Haight, F. A.}},
  isbn={9788391014424},
  lccn={66028750},
  series={Operations Research Society of America. Publications in operations research},
  url={https://books.google.com/books?id=l8Y-AAAAIAAJ},
  year={1967},
  publisher={Wiley}
}
@article{Finkelshtein2025,
 author = {{Finkelshtein, D., Malyarenko, A., Mishura, Y., & Ralchenko, K.}},
 journal = {Methodology and Computing in Applied Probability},
 number = {2},
 pages = {45},
 title = {Entropies of the Poisson Distribution as Functions of Intensity: “Normal” and “Anomalous” Behavior},
 volume = {27},
 year = {2025},
doi={10.1007/s11009-025-10171-9}
}
//
Eff
/////////////////////////////////
@book{Cohen1977,
    author={{Cohen, J.}},
    year={1977},
    title={Statistical Power Analysis for the Behavioral Science},
    address={Amsterdam},
    publisher={Elsevier Academic Press},
    ISBN={978-0-12-179060-8},
    url={https://doi.org/10.1016/C2013-0-10517-X}
}
@book{Cohen1988,
    author={{Cohen, J.}},
    year={1988},
    title={Statistical Power Analysis for the Behavioral Science},
    edition={2},
    address={Hillsdale, NJ},
    publisher={Lawrence Erlbaum Associates}, 
    doi={10.4324/9780203771587}
}
@article{Cohen1992,
    author={{Cohen, J.}},
    year={1992},
    title={A power primer},
    journal={Psychological Bulletin},
    volume={112},
    number={1},
    pages={155--159},
    url={https://doi.org/10.1037/0033-2909.112.1.155}
}
@Inbook{Janczyk2013,
author={{Janczyk, M., & Pfister, R.}},
title="Fehlertypen, Effektst{\"a}rken und Power",
bookTitle="Inferenzstatistik verstehen: Von A wie Signifikanztest bis Z wie Konfidenzintervall",
year="2013",
publisher="Springer",
address="Berlin, Heidelberg",
pages="77--90",
abstract="Obwohl signifikante Ergebnisse oft gew{\"u}nscht werden, sagt -- wie in diesem Kapitel aufgezeigt wird -- die blo{\ss}e Signifikanz eines Tests nichts {\"u}ber die St{\"a}rke eines Effekts aus, und man kann sich berechtigt fragen: Bedeutet statistische Signifikanz auch immer ˝inhhaltliche Relevanz˝ bzw. ˝praktische Bedeutsamkeit˝? In diesem Kapitel werden diejenigen Konzepte eingef{\"u}hrt, die zur Beantwortung dieser Frage ben{\"o}tigt werden, und begonnen wird dabei mit einer systematischen Betrachtung statistischer Entscheidungen. Im Anschluss wird Cohen's d als Ma{\ss} der Effektst{\"a}rke f{\"u}r die bisher behandelten t-Tests betrachtet. Das Konzept der Effektst{\"a}rke f{\"u}hrt schlie{\ss}lich zur Power (Testst{\"a}rke) eines Signifikanztests und der Frage nach dem optimalen Stichprobenumfang.",
isbn="978-3-642-34825-9",
doi="10.1007/978-3-642-34825-9_7",
url="https://doi.org/10.1007/978-3-642-34825-9_7"
}
@article{Brandmaier2025,
author ={{Brandmaier, A. M.}},
title ={Stop Calling Cohen's d an Effect Size},
journal ={PsyArXiv},
year ={2025},
doi={10.31234/osf.io/n9ta7_v1}
}
@article{Fritz2012,
author ={{Fritz, C. O., Morris, P. E., & Richler, J. J.}},
year ={2012},
title ={Effect size estimates: Current use, calculations, and interpretation},
journal ={Journal of Experimental Psychology: General},
volume={141},
number={1},
pages={2--18},
doi={10.1037/a0024338}
}
@ARTICLE{10.3389/fpsyg.2019.00813,
AUTHOR={{Schäfer, T., & Schwarz, M. A.}},
TITLE={The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases},
JOURNAL={Frontiers in Psychology},
VOLUME={Volume 10 - 2019},
YEAR={2019},
URL={https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.00813},
DOI={10.3389/fpsyg.2019.00813},
ISSN={1664-1078},
ABSTRACT={Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes—when is an effect small, medium, or large?—has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = .36) were much larger than effects from the latter (median r = .16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.}
}
@article{BOWRING2021117477,
title = {Confidence Sets for Cohen’s d effect size images},
journal = {NeuroImage},
volume = {226},
pages = {117477},
year = {2021},
issn = {1053-8119},
doi = {10.1016/j.neuroimage.2020.117477},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920309629},
author = {{Bowring, A., Telschow, F. J. E., Schwartzman, A., & Nichols, T. E.}},
keywords = {Confidence sets, fMRI, Task fmri, Cohen’s , Effect sizes},
abstract = {Current statistical inference methods for task-fMRI suffer from two fundamental limitations. First, the focus is solely on detection of non-zero signal or signal change, a problem that is exacerbated for large scale studies (e.g. UK Biobank, N=40,000+) where the ‘null hypothesis fallacy’ causes even trivial effects to be determined as significant. Second, for any sample size, widely used cluster inference methods only indicate regions where a null hypothesis can be rejected, without providing any notion of spatial uncertainty about the activation. In this work, we address these issues by developing spatial Confidence Sets (CSs) on clusters found in thresholded Cohen’s d effect size images. We produce an upper and lower CS to make confidence statements about brain regions where Cohen’s d effect sizes have exceeded and fallen short of a non-zero threshold, respectively. The CSs convey information about the magnitude and reliability of effect sizes that is usually given separately in a t-statistic and effect estimate map. We expand the theory developed in our previous work on CSs for %BOLD change effect maps (Bowring et al., 2019) using recent results from the bootstrapping literature. By assessing the empirical coverage with 2D and 3D Monte Carlo simulations resembling fMRI data, we find our method is accurate in sample sizes as low as N=60. We compute Cohen’s d CSs for the Human Connectome Project working memory task-fMRI data, illustrating the brain regions with a reliable Cohen’s d response for a given threshold. By comparing the CSs with results obtained from a traditional statistical voxelwise inference, we highlight the improvement in activation localization that can be gained with the Confidence Sets.}
}
@article{MILLER2011144,
title = {Exact method for computing absolute percent change in a dichotomous outcome from meta-analytic effect size: Improving impact and cost-outcome estimates},
journal = {Value in Health},
volume = {14},
number = {1},
pages = {144--151},
year = {2011},
issn = {1098-3015},
doi = {10.1016/j.jval.2010.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S1098301510000148},
author = {{Miller, T. R., Hendrie, D., & Derzon, J.}},
keywords = {Absolute change, BESD, Effect size, Meta-analysis},
abstract = {Objectives
Meta-analyses typically compute a treatment effect size (Cohen's d), which is readily converted to another common measure, the binomial effect size display (BESD). BESD is the correlation coefficient and represents a percentage difference in outcome attributable to an intervention. Both d and BESD are in arbitrary units; neither measures the absolute change resulting from intervention. The method used to estimate absolute change from BESD assumes both a 50-50 split of the outcome and a balanced design. Consequently, inaccurate assumptions underpin most meta-analytic estimates of the gain resulting from an intervention (and of its cost effectiveness). This article develops an exact formula without these assumptions.
Methods
The formula is developed algebraically from 1) the formula for the correlation coefficient represented as a 2-by-2 contingency table constructed from the relative size of the treatment and control groups and the percentage of people who would have the condition absent intervention, and 2) the BESD correlation coefficient formula showing change in success probability with treatment.
Results
Simulation reveals that BESD only approximates the reduction in the outcome an intervention might well achieve when the problem outcome occurs in 35%-65% of cases. For less common outcomes, BESD substantially overestimates the impact of an intervention. Even when BESD accurately estimates the likely percentage change in outcome, it paints a misleading picture of the proportion of cases that will achieve a positive outcome.
Conclusion
It is time to retire BESD. Our equations can also guide effect size estimation from difficult articles.}
}
@article{abb9e70f-419e-3a38-8754-462a7041475a,
 ISSN = {00220973, 19400683},
 URL = {https://www.jstor.org/stable/26594399},
 abstract = {Given the long history of discussion of issues surrounding statistical testing and effect size indices and various attempts by the American Psychological Association and by the American Educational Research Association to encourage the reporting of effect size, most journals in education and psychology have witnessed an increase in effect size reporting since 1999. Yet, effect size was often reported in three indices, namely, the unadjusted R², Cohen’s d, and η² with a simple labeling of small, medium, or large, according to Cohen’s (1969) criteria. In this article, the authors present several alternatives to Cohen’s d to help researchers conceptualize effect size beyond standardized mean differences for between-subject designs with two groups. The alternative effect size estimators are organized into a typology and are empirically contrasted with Cohen’s d in terms of purposes, usages, statistical properties, interpretability, and the potential for meta-analysis. Several sound alternatives are identified to supplement the reporting of Cohen’s d. The article concludes with a discussion of the choice of standardizers, the importance of assumptions, and the possibility of extending sound alternative effect size indices to other research contexts.},
 author = {{Peng, C.-Y. J., & Chen, L.-T.}},
 journal = {The Journal of Experimental Education},
 number = {1},
 pages = {22--50},
 publisher = {Taylor & Francis, Ltd.},
 title = {Beyond Cohen’s d: Alternative Effect Size Measures for Between-Subject Designs},
 urldate = {2026-01-14},
 volume = {82},
 year = {2014}
}
@article{d04b907b-1c2b-329b-902f-ade87a9fd109,
 ISSN = {00220973, 19400683},
 URL = {http://www.jstor.org/stable/20157436},
 abstract = {Reporting effect size plays an integral role in educational and psychological research and is required by many journals. Certainly, the best-known measure of effect size is Cohen's d, which represents a substantial improvement over using p values. But Cohen's d is known to suffer from some fundamental concerns. The author's goal was to illustrate some graphical methods aimed at addressing those concerns. These methods can be applied in a wide range of situations, including situations in which the Wilcoxon-Mann-Whitney test is used.},
 author = {{Wilcox, R. R.}},
 journal = {The Journal of Experimental Education},
 number = {4},
 pages = {353--367},
 publisher = {Taylor & Francis, Ltd.},
 title = {Graphical Methods for Assessing Effect Size: Some Alternatives to Cohen's d},
 urldate = {2026-01-14},
 volume = {74},
 year = {2006}
}
@article{Groß2024,
author = {{Groß, J., & Möller, A.}},
year = {2024},
title = {Some additional remarks on statistical properties of Cohen’s d in the presence of covariates},
journal = {Statistical Papers},
pages = {3971--3979},
volume = {65},
number = {6},
abstract = {The size of the effect of the difference in two groups with respect to a variable of interest may be estimated by the classical Cohen’s d. A recently proposed generalized estimator allows conditioning on further independent variables within the framework of a linear regression model. In this note, it is demonstrated how unbiased estimation of the effect size parameter together with a corresponding standard error may be obtained based on the non-central t distribution. The portrayed estimator may be considered as a natural generalization of the unbiased Hedges’ g. In addition, confidence interval estimation for the unknown parameter is demonstrated by applying the so-called inversion confidence interval principle. The regarded properties collapse to already known ones in case of absence of any additional independent variables. The stated remarks are illustrated with a publicly available data set.},
doi = {10.1007/s00362-023-01527-9}
}
@article{Groß2023,
author = {{Groß, J., & Möller, A.}},
year = {2023},
title = {A Note on Cohen’s d From a Partitioned Linear Regression Model},
journal = {Journal of Statistical Theory and Practice},
pages = {22},
volume = {17},
number = {2},
abstract = {In this note, we introduce a generalized formula for Cohen’s d under the presence of additional independent variables, providing a measure for the size of a possible effect concerning the size of a difference location effect of a variable in two groups. This is done by employing the so-called Frisch–Waugh–Lovell theorem in a partitioned linear regression model. The generalization is motivated by demonstrating the relationship to appropriate t and F statistics. Our discussion is further illustrated by inference about a publicly available data set.},
doi = {10.1007/s42519-023-00323-w}
}
@article{Li2016,
author={{Li, J. C.-H.}},
year = {2016},
title = {Effect size measures in a two-independent-samples case with nonnormal and nonhomogeneous data},
journal = {Behavior Research Methods},
pages = {1560--1574},
volume = {48},
number = {4},
abstract = {In psychological science, the “new statistics” refer to the new statistical practices that focus on effect size (ES) evaluation instead of conventional null-hypothesis significance testing (Cumming, Psychological Science, 25, 7–29, 2014). In a two-independent-samples scenario, Cohen’s (1988) standardized mean difference (d) is the most popular ES, but its accuracy relies on two assumptions: normality and homogeneity of variances. Five other ESs—the unscaled robust d (dr*; Hogarty & Kromrey, 2001), scaled robust d (dr; Algina, Keselman, & Penfield, Psychological Methods, 10, 317–328, 2005), point-biserial correlation (rpb; McGrath & Meyer, Psychological Methods, 11, 386–401, 2006), common-language ES (CL; Cliff, Psychological Bulletin, 114, 494–509, 1993), and nonparametric estimator for CL (Aw; Ruscio, Psychological Methods, 13, 19–30, 2008)—may be robust to violations of these assumptions, but no study has systematically evaluated their performance. Thus, in this simulation study the performance of these six ESs was examined across five factors: data distribution, sample, base rate, variance ratio, and sample size. The results showed that Awand drwere generally robust to these violations, and Awslightly outperformed dr. Implications for the use of Awand drin real-world research are discussed.},
doi = {10.3758/s13428-015-0667-z}
}
//
Boots
/////////////////////////////////
@article{Efron1979,
    author = {{Efron, B.}},
    title = {{Bootstrap Methods: Another Look at the Jackknife}},
    volume = {7},
    journal = {The Annals of Statistics},
    number = {1},
    publisher = {Institute of Mathematical Statistics},
    pages = {1--26},
    year = {1979},
    doi = {10.1214/aos/1176344552},
    URL = {https://doi.org/10.1214/aos/117634455}
}
@article{Efron1981,
    author = {{Efron, B.}},
    title = {Nonparametric estimates of standard error: The jackknife, the bootstrap and other methods},
    journal = {Biometrika},
    volume = {68},
    number = {3},
    pages = {589--599},
    year = {1981},
    month = {12},
    abstract = {We discuss several nonparametric methods for attaching a standard error to a point estimate: the jackknife, the bootstrap, half-sampling, subsampling, balanced repeated replications, the infinitesimal jackknife, influence function techniques and the delta method. The last three methods are shown to be identical. All the methods derive from the same basic idea, which is also the idea underlying the common parametric methods. Extended numerical comparisons are made for the special case of the correlation coefficient.},
    issn = {0006-3444},
    doi = {10.1093/biomet/68.3.589},
    url = {https://doi.org/10.1093/biomet/68.3.589}
}
@book{Efron1982,
    author = {{Efron, B.}},
    title = {The Jackknife, the Bootstrap and Other Resampling Plans},
    publisher = {{SIAM, Society for Industrial and Applied Mathematics}},
    year = {1982},
    doi = {10.1137/1.9781611970319},
    address = {Philadelphia},
    series   = {CBMS-NSF Regional Conference Series in Applied Mathematics, Monograph 38},
    URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611970319}
}
@Inbook{Wilcox2001,
author={{Wilcox, R. R.}},
title="The Bootstrap",
bookTitle="Fundamentals of Modern Statistical Methods: Substantially Improving Power and Accuracy",
year="2001",
publisher="Springer",
address="New York, NY",
pages="93--115",
abstract="When testing hypotheses (or computing confidence intervals) with the one-sample Student's T method described in Chapter 5, the central limit theorem tells us that Student's T performs better as the sample size increases. That is, under random sampling the discrepancy between the nominal and actual Type I error probability will go to zero as the sample size goes to infinity. But unfortunately, for reasons outlined in Section 5.3 of Chapter 5, there are realistic situations where about two hundred observations are needed to get satisfactory control over the probability of a Type I error or accurate probability coverage when computing confidence intervals. When comparing the population means of two groups of individuals, using Student's T is known to be unsatisfactory when sample sizes are small or even moderately large. In fact, it might be unsatisfactory no matter how large the sample sizes are because under general conditions it does not converge to the correct answer (Cressie and Whitford, 1986). Switching to the test statistic W given by Equation 5.3, the central limit theorem now applies under general conditions, so using W means we will converge to the correct answer as the sample sizes increase, but in some cases we again need very large sample sizes to get accurate results. (There are simple methods for improving the performance of W using what are called estimated degrees of freedom, but the improvement remains highly unsatisfactory for a wide range of situations.) Consequently, there is interest in finding methods that beat our reliance on the central limit theorem as it applies to these techniques. That is, we would like to find a method that converges to the correct answer more quickly as the sample sizes get large, and such a method is described here.",
isbn="978-1-4757-3522-2",
doi="10.1007/978-1-4757-3522-2_6",
url="https://doi.org/10.1007/978-1-4757-3522-2_6"
}
@article{politis2003,
author={{Politis. D. N.}}, 
title ={The Impact of Bootstrap Methods on Time Series Analysis},
journal={Statistical Science},
volume={18},
number ={2},
pages ={219--230},
year ={2003},
doi={10.1214/ss/1063994977}
}
@Inbook{Kenett2022,
author={{Kenett, R., Zacks, S., & Gedeck, P.}},
title="Statistical Inference and Bootstrapping",
bookTitle="Modern Statistics: A Computer-Based Approach with Python",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="139--223",
abstract="In this chapter we introduce basic concepts and methods of statistical inference. The focus is on estimating the parameters of statistical distributions and testing hypotheses about them. Problems of testing if certain distributions fit observed data are also considered.",
isbn="978-3-031-07566-7",
doi="10.1007/978-3-031-07566-7_3",
url="https://doi.org/10.1007/978-3-031-07566-7_3"
}
@Inbook{Zuev2026,
author={{Zuev, K. M.}},
title="The Bootstrap Method",
bookTitle="Fundamentals of Statistical Inference: Foundations of Data Analysis",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="115--129",
abstract="In this chapter, we will learn how to estimate the standard error of an estimate and how to construct confidence intervals for a parameter of interest. Both problems can be solved by the bootstrap method, which was inspired by the jackknife. The bootstrap, introduced in 1979 by Bradley Efron, is a general simulation-based method for measuring the uncertainty of estimates, in particular, for estimating their standard errors and constructing confidence intervals. Its beauty lies in its simplicity and universality: the bootstrap is fully automatic, requires no theoretical calculations, and is always available.",
isbn="978-3-032-03848-7",
doi="10.1007/978-3-032-03848-7_6",
url="https://doi.org/10.1007/978-3-032-03848-7_6"
}
@Article{math13182913,
AUTHOR = {{Zheng, Y., & Fan, H.}},
TITLE = {Fast Cluster Bootstrap Methods for Spatial Error Models},
JOURNAL = {Mathematics},
VOLUME = {13},
YEAR = {2025},
NUMBER = {18},
ARTICLE-NUMBER = {2913},
URL = {https://www.mdpi.com/2227-7390/13/18/2913},
ISSN = {2227-7390},
ABSTRACT = {Typically, the traditional bootstrap methods for parameter inference of spatial error models suffer from high computational costs, so this study proposes fast cluster bootstrap methods for spatial error models to deal with the dilemma. The key idea is to calculate the sufficient statistics for each cluster before performing the bootstrap loop of the spatial error model, and based on these sufficient statistics, all quantities needed for bootstrap inference can be computed. Furthermore, this study performed Monte Carlo simulations, and the result reveals that compared with traditional bootstrap methods, our proposed methods can reduce the computational cost substantially and improve the reliability for obtaining the bootstrap test statistics and confidence intervals of the parameters for spatial error models.},
DOI = {10.3390/math13182913}
}
@article{eidous2025,
author = {{Eidous, O.}},
year = {2025},
month = {09},
journal={{ResearchGate}},
pages = {},
title = {Does the Bootstrap Method Effectively Evaluate Estimators?},
doi = {10.13140/RG.2.2.12238.11846}
}
@ARTICLE{647042,
  author={{Politis, D. N.}},
  journal={IEEE Signal Processing Magazine}, 
  title={Computer-intensive methods in statistical analysis}, 
  year={1998},
  volume={15},
  number={1},
  pages={39--55},
  keywords={Statistical analysis;Sampling methods;Statistical distributions;Gaussian distribution;Shape;Density functional theory;Statistics},
  doi={10.1109/79.647042}
}
@article{KHOSRAVI2021100781,
title = {Application of bootstrap re-sampling method in statistical measurement of sustainability},
journal = {Socio-Economic Planning Sciences},
volume = {75},
pages = {100781},
year = {2021},
note = {Decision-Making for Environmental Sustainability},
issn = {0038-0121},
doi = {10.1016/j.seps.2020.100781},
url = {https://www.sciencedirect.com/science/article/pii/S0038012119303416},
author = {{Khosravi, F., Izbirak,  G., & Shavarani, S. M.}},
keywords = {Sustainability, Bootstrap Re-Sampling, Statistical performance measurement, Enablers, Barriers},
abstract = {Sustainability is of essential interest for many organizations and is defined as the ability to maintain existing resources at a certain rate or level when encountered with barriers. Factors affecting sustainability are categorized as enablers (capacities) and barriers (challenges) that have positive and negative effects on sustainability, respectively. To evaluate the status of sustainability, organizations need a measurement method to account for all the aspects of the sustainability classified into social, economic, and environmental tiers. Previously many researchers have provided indexes and measure specific to the studied field, which is not applicable to other areas. Furthermore, the proposed methods fail to cover all the aspects of sustainability. This study investigates a statistical method to measure the sustainability and the application of the bootstrap re-sampling method in order to overcome the problem with normality assumption when the sample size is not large enough and thus develop a more realistic stochastic model. The Bootstrap re-sampling method enables the unbiased estimation of population parameters such as mean and standard deviation. The proposed method is evaluated by comparing its results with those found in the literature.}
}
@incollection{HOROWITZ20013159,
title = {Chapter 52 - The Bootstrap},
editor = {{Heckman, J. J., & Leamer, E.}},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {5},
pages = {3159--3228},
year = {2001},
issn = {1573-4412},
doi = {10.1016/S1573-4412(01)05005-X},
url = {https://www.sciencedirect.com/science/article/pii/S157344120105005X},
author = {Joel L. Horowitz},
keywords = {JEL classification:, C12, C13, C15},
abstract = {The bootstrap is a method for estimating the distribution of an estimator or test statistic by resampling one’s data or a model estimated from the data. Under conditions that hold in a wide variety of econometric applications, the bootstrap provides approximations to distributions of statistics, coverage probabilities of confidence intervals, and rejection probabilities of hypothesis tests that are more accurate than the approximations of first-order asymptotic distribution theory. The reductions in the differences between true and nominal coverage or rejection probabilities can be very large. The bootstrap is a practical technique that is ready for use in applications. This chapter explains and illustrates the usefulness and limitations of the bootstrap in contexts of interest in econometrics. The chapter outlines the theory of the bootstrap, provides numerical illustrations of its performance, and gives simple instructions on how to implement the bootstrap in applications. The presentation is informal and expository. Its aim is to provide an intuitive understanding of how the bootstrap works and a feeling for its practical value in econometrics.}
}
@article{5d7fe90b-50e5-3e2c-9890-dc7b7f71d323,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/3182845},
 abstract = {The contemporary development of bootstrap methods, from the time of Efron's early articles to the present day, is well documented and widely appreciated. Likewise, the relationship of bootstrap techniques to certain early work on permutation testing, the jackknife and cross-validation is well understood. Less known, however, are the connections of the bootstrap to research on survey sampling for spatial data in the first half of the last century or to work from the 1940s to the 1970s on subsampling and resampling. In a selective way, some of these early linkages will be explored, giving emphasis to developments with which the statistics community tends to be less familiar. Particular attention will be paid to the work of P. C. Mahalanobis, whose development in the 1930s and 1940s of moving-block sampling methods for spatial data has a range of interesting features, and to contributions of other scientists who, during the next 40 years, developed half-sampling, subsampling and resampling methods.},
 author = {{Hall, P.}},
 journal = {Statistical Science},
 number = {2},
 pages = {158--167},
 publisher = {Institute of Mathematical Statistics},
 title = {A Short Prehistory of the Bootstrap},
 urldate = {2026-01-14},
 volume = {18},
 year = {2003}
}
@article{71a9795f-441b-3377-aa30-ddc5824ffbf2,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/24774569},
 abstract = {The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large data sets—which are increasingly prevalent—the calculation of bootstrap-based quantities can be prohibitively demanding computationally. Although variants such as subsampling and the m out of n bootstrap can be used in principle to reduce the cost of bootstrap computations, these methods are generally not robust to specification of tuning parameters (such as the number of subsampled data points), and they often require knowledge of the estimator's convergence rate, in contrast with the bootstrap. As an alternative, we introduce the 'bag of little bootstraps' (BLB), which is a new procedure which incorporates features of both the bootstrap and subsampling to yield a robust, computationally efficient means of assessing the quality of estimators. The BLB is well suited to modern parallel and distributed computing architectures and furthermore retains the generic applicability and statistical efficiency of the bootstrap. We demonstrate the BLB's favourable statistical performance via a theoretical analysis elucidating the procedure's properties, as well as a simulation study comparing the BLB with the bootstrap, the m out of n bootstrap and subsampling. In addition, we present results from a large-scale distributed implementation of the BLB demonstrating its computational superiority on massive data, a method for adaptively selecting the BLB's tuning parameters, an empirical study applying the BLB to several real data sets and an extension of the BLB to time series data.},
 author = {{Kleiner, A., Talwalkar, A., Sarkar, P., & Jordan, M. I.}},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {4},
 pages = {795--816},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {A scalable bootstrap for massive data},
 urldate = {2026-01-14},
 volume = {76},
 year = {2014}
}
@article{10.2307/2348962,
    author = {{Pewsey, A.}},
    title = {Exploring the Limits of Bootstrap},
    journal = {Journal of the Royal Statistical Society Series D: The Statistician},
    volume = {43},
    number = {1},
    pages = {215--216},
    year = {2018},
    month = {12},
    issn = {2515-7884},
    doi = {10.2307/2348962},
    url = {https://doi.org/10.2307/2348962},
    eprint = {https://academic.oup.com/jrsssd/article-pdf/43/1/215/49929729/jrsssd_43_1_215a.pdf},
}
@article{e841cec6-d2b9-36dd-bee3-92abf3c9e224,
 ISSN = {13684221, 1368423X},
 URL = {http://www.jstor.org/stable/23114968},
 abstract = {Consistency of the bootstrap second moments does not usually follow from the proofs of consistency of the distribution of the bootstrap. Here it is shown that the convergence of the bootstrap distribution to a normal variate implicitly defines a consistent estimator for the asymptotic second moments. The estimator is based on the L-estimation of the scale parameter of arbitrary linear combinations of the bootstrap sequence and uses Classical Minimum Distance techniques to impose the positive semi-definiteness restrictions.},
 author = {{Machado, J. A. F., & Parente, P.}},
 journal = {The Econometrics Journal},
 number = {1},
 pages = {70--78},
 publisher = {[Royal Economic Society, Wiley]},
 title = {Bootstrap estimation of covariance matrices via the percentile method},
 urldate = {2026-01-14},
 volume = {8},
 year = {2005}
}
@article{ee45c30f-1813-36a2-b2a9-57ffa08fcc80,
 ISSN = {07492170},
 URL = {http://www.jstor.org/stable/4356107},
 abstract = {We propose a unifying principle which identifies a very broad class of hypotheses and statistics for which a suitable application of the n out of n bootstrap yields asymptotically correct critical values and power for contiguous alternatives. We also show that this attractive principle can fail in situations which the m out of n bootstrap can deal with (Bickel, Götze and van Zwet, 1997) (BGvZ). We formalize the m out of n bootstrap theory for testing and show that under mild conditions, it provides correct significance level, asymptotic power under contiguous alternatives, and consistency. We conclude with simulation results supporting the asymptotics.},
 author = {{Bickel, P. J., & Ren, J.-J.}},
 journal = {Lecture Notes-Monograph Series},
 pages = {91--112},
 publisher = {Institute of Mathematical Statistics},
 title = {The Bootstrap in Hypothesis Testing},
 urldate = {2026-01-14},
 volume = {36},
 year = {2001}
}
@article{Baíllo2025,
author={{Baíllo, A., & Cárcamo, J.}},
year = {2025},
title = {Bootstrap tests for almost goodness-of-fit},
journal= {Statistics and Computing},
pages = {10},
volume= {36},
number = {1},
abstract= {We introduce the almost goodness-of-fit test, a procedure to assess whether a (parametric) model provides a good representation of the probability distribution generating the observed sample. Specifically, given a distribution function F and a parametric family $$\mathcal {G}=\{ G(\varvec{\theta }): \varvec{\theta } \in \Theta \}$$, we consider the testing problem $$ H_0: \Vert F - G(\varvec{\theta }_F) \Vert _p \ge \epsilon \quad \text {vs} \quad H_1: \Vert F - G(\varvec{\theta }_F) \Vert _p < \epsilon , $$H0:‖F-G(θF)‖p≥ϵvsH1:‖F-G(θF)‖p<ϵ,where $$\epsilon >0$$is a margin of error and $$G(\varvec{\theta }_F)$$denotes a representative of F within the parametric class. The approximate model is determined via an M-estimator of the parameters. The methodology also quantifies the percentage improvement of the proposed model relative to a non-informative (constant) benchmark. The test statistic is the $$\textrm{L}^p$$-distance between the empirical distribution function and that of the estimated model. We present two consistent, easy-to-implement, and flexible bootstrap schemes to carry out the test. The performance of the proposal is illustrated through simulation studies and analysis and real-data applications.},
doi={10.1007/s11222-025-10762-z}
}
@article{Liu2025,
author={{Liu, X. S.}},
year = {2025},
title = {Bootstrap power calculation: a flexible alternative to conventional power analysis for prospective and replication studies},
journal= {Behaviormetrika},
abstract= {Bootstrapping offers a flexible approach to estimating statistical power, when planning a new study based on a previous one. In this article, we explore both parametric and non-parametric bootstrap power calculations and demonstrate how to incorporate uncertainty about effect size in bootstrap power analysis. We use real examples to illustrate bootstrap power calculations in independent t-test and ANCOVA. Finally, we discuss the broader implications of bootstrap power calculation for a variety of research designs.},
doi={10.1007/s41237-025-00283-4}
}
//
EEG
/////////////////////////////////
@book{Grafakos2023,
title={Fundamentals of Fourier Analysis},
author={{Grafakos, L.}},
series={Graduate Texts in Mathematics},
doi={10.1007/978-3-031-56500-7},
publisher={Springer},
address={Cham},
year={2024},
edition={1}
}
@book{Brigola2025,
title={Fourier Analysis and Distributions},
author={{Brigola, R.}},
series={Texts in Applied Mathematics},
doi={10.1007/978-3-031-81311-5},
publisher={Springer},
address={Cham},
year={2025},
edition={1}
}
@book{Siuly2017,
title={EEG Signal Analysis and Classification},
author={{Siuly, S., Li, Y., & Zhang, Y.}},
series={Health Information Science},
doi={10.1007/978-3-319-47653-7},
publisher={Springer},
address={Cham},
year={2017},
edition={1}
}
@article{Zhang2023,
author={{Zhang, H., Zhou, Q.-Q., Chen, H., Hu, X.-Q., Li, W.-G., Bai, Y., Han, J.-X., Wang, Y., Liang, Z.-H., Chen, D., Cong, F.-Y., Yan, J.-Q., & Li, X.-L.}},
year={2023},
title={The applied principles of EEG analysis methods in neuroscience and clinical neurology},
journal={Military Medical Research},
page={67},
volume={10},
number={1},
abstract={Electroencephalography (EEG) is a non-invasive measurement method for brain activity. Due to its safety, high resolution, and hypersensitivity to dynamic changes in brain neural signals, EEG has aroused much interest in scientific research and medical fields. This article reviews the types of EEG signals, multiple EEG signal analysis methods, and the application of relevant methods in the neuroscience field and for diagnosing neurological diseases. First, three types of EEG signals, including time-invariant EEG, accurate event-related EEG, and random event-related EEG, are introduced. Second, five main directions for the methods of EEG analysis, including power spectrum analysis, time–frequency analysis, connectivity analysis, source localization methods, and machine learning methods, are described in the main section, along with different sub-methods and effect evaluations for solving the same problem. Finally, the application scenarios of different EEG analysis methods are emphasized, and the advantages and disadvantages of similar methods are distinguished. This article is expected to assist researchers in selecting suitable EEG analysis methods based on their research objectives, provide references for subsequent research, and summarize current issues and prospects for the future.},
doi={10.1186/s40779-023-00502-7}
}
@Inbook{Panov2023,
author={{Panov, G.}},
editor={{Stoyanov, D., Draganski, B., Brambilla, P., & Lamm, C.}},
title="Quantitative EEG Analysis: Introduction and Basic Principles",
bookTitle="Computational Neuroscience",
year="2023",
publisher="Springer US",
address="New York, NY",
pages="85--91",
abstract="The use of mathematical models for electroencephalography (EEG) analysis has been going on for many years, and currently they are starting to find a place both in clinical practice and in parallel with other methods for evaluation of brain activity. The use and interpretation of these methods are not possible without knowledge of the basic mechanisms and processes that underlie the results obtained from their application. The advantages of the quantitative methods, the processes underlying their presentation, and the optimal parameters used, such as the number of electrodes and time intervals, have been evaluated. Attention has been paid to the advantages of the individual possibilities for EEG signal analysis---absolute and relative power as well as coherence. An analysis of these defined quantities provides an opportunity to enter into the intimate mechanisms underlying specific psychopathological phenomena.",
isbn="978-1-0716-3230-7",
doi="10.1007/978-1-0716-3230-7_5",
url="https://doi.org/10.1007/978-1-0716-3230-7_5"
}
@article{FRENCH1984241,
title = {A critical review of EEG coherence studies of hemisphere function},
journal = {International Journal of Psychophysiology},
volume = {1},
number = {3},
pages = {241-254},
year = {1984},
issn = {0167-8760},
doi = {10.1016/0167-8760(84)90044-8},
url = {https://www.sciencedirect.com/science/article/pii/0167876084900448},
author = {{French, C. C, & Graham, J. B.}},
keywords = {coherence—hemisphere function—psychopathology—task effects—sex—handedness—methodology},
abstract = {Studies of hemisphere function involving the use of coherence analysis are reviewed. Despite the fact that many effects related to subject group and type of task have been reported, few seem to be reliable across studies. The assumptions which seem to underlie the use of coherence analysis are made explicit and discussed. Most investigators consider coherence levels to reflect certain ‘baseline’ patterns, possibly related to structural connectivity, upon which task and/or strategy effects may be superimposed. Use of a cephalic reference renders coherence effects virtually uninterpretable for a variety of reasons which are fully discussed.}
}
@article{KUMAR20122525,
title = {Analysis of Electroencephalography (EEG) Signals and Its Categorization–A Study},
journal = {Procedia Engineering},
volume = {38},
pages = {2525-2536},
year = {2012},
note = {INTERNATIONAL CONFERENCE ON MODELLING OPTIMIZATION AND COMPUTING},
issn = {1877-7058},
doi = {10.1016/j.proeng.2012.06.298},
url = {https://www.sciencedirect.com/science/article/pii/S1877705812022114},
author = {{Kumar, J. S., & Bhuvaneswari, P.}},
keywords = {Electroencephalography, brain signal, modality, brain states},
abstract = {Human brain consists of millions of neurons which are playing an important role for controlling behavior of human body with respect to internal/external motor/sensory stimuli. These neurons will act as information carriers between human body and brain. Understanding cognitive behaviour of brain can be done by analyzing either signals or images from the brain. Human behaviour can be visualized in terms of motor and sensory states such as, eye movement, lip movement, remembrance, attention, hand clenching etc. These states are related with specific signal frequency which helps to understand functional behavior of complex brain structure. Electroencephalography (EEG) is an efficient modality which helps to acquire brain signals corresponds to various states from the scalp surface area. These signals are generally categorized as delta, theta, alpha, beta and gamma based on signal frequencies ranges from 0.1Hz to more than 100Hz. This paper primarily focuses on EEG signals and its characterization with respect to various states of human body. It also deals with experimental setup used in EEG analysis.}
}
@inbook{Panitz_Ward_Pouliot_Keil_2024, place={Cambridge}, 
series={Cambridge Handbooks in Psychology}, title={EEG and ERP}, 
booktitle={The Cambridge Handbook of Research Methods and Statistics for the Social and Behavioral Sciences: Volume 2: Performing Research}, publisher={Cambridge University Press}, author={{Panitz, C., Ward, R. T., Pouliot, J., & Keil, A.}}, 
editor={{Edlund, J. E., & Nichols, A. L.}}, 
year={2024}, 
pages={519–544}, 
doi={10.1017/9781009000796.024},
collection={Cambridge Handbooks in Psychology}
}
@article{Guevara2011,
author={{Guevara, M., Hernández-González, M., Sanz-Martin, A., & Amezcua, C.}},
year={2011},
title={EEGcorco: a computer program to simultaneously calculate and statistically analyze EEG coherence and correlation},
journal={Journal of Biomedical Science and Engineering}, 
volume={4}, 
page={774--787},
doi={10.4236/jbise.2011.412096}
}
@article{Chiarion2023,
author={{Chiarion, G., Sparacino, L., Antonacci, Y., Faes, L., & Mesin, L.}},
year={2023},
title={Connectivity Analysis in EEG Data: A Tutorial Review of the State of the Art and Emerging Trends},
journal={Bioengineering (Basel, Switzerland)}, 
volume={10},
number={3}, 
page={372},
doi={10.3390/bioengineering10030372}
}
@book{Gerthsen1966,
title={Physik},
subtitle={Ein Lehrbuch zum Gebrauch Neben Vorlesungen},
author={{Gerthsen, C.}},
editor={{Kneser, H. O.}},
doi={10.1007/978-3-662-30201-9},
publisher={Springer},
address={Berlin, Heidelberg},
year={1966},
edition={9}
}
@book{Gerthsen2026,
title={Gerthsen Physik},
author={{Meschede, D., ed., Feld, L., Gross, R., Müller, R., Niedner-Schatteburg,  G., Schäfer, G., Sokolowski, M., Vewinger, F., Reinhard F., Werner, R. F., & Zohm, H. }},
doi={10.1007/978-3-662-30201-9},
publisher={Springer Spektrum},
address={Berlin, Heidelberg},
year={2026},
edition={26}
}
@article{NEUBAUER20091004,
title = {Intelligence and neural efficiency},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {33},
number = {7},
pages = {1004-1023},
year = {2009},
issn = {0149-7634},
doi = {10.1016/j.neubiorev.2009.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0149763409000591},
author = {{Neubauer, A. C., & Fink, A}},
keywords = {Intelligence, Neural efficiency, Training, Task complexity, Neuroscience},
abstract = {We review research on the neural efficiency hypothesis of intelligence, stating that brighter individuals display lower (more efficient) brain activation while performing cognitive tasks [Haier, R.J., Siegel, B.V., Nuechterlein, K.H., Hazlett, E., Wu, J.C., Paek, J., Browning, H.L., Buchsbaum, M.S., 1988. Cortical glucose metabolic rate correlates of abstract reasoning and attention studied with positron emission tomography. Intelligence 12, 199–217]. While most early studies confirmed this hypothesis later research has revealed contradictory evidence or has identified some moderating variables like sex, task type, task complexity or brain area. Neuroscientific training studies suggest that neural efficiency also seems to be a function of the amount and quality of learning. From integrating this evidence we conclude that neural efficiency might arise when individuals are confronted with tasks of (subjectively) low to moderate task difficulty and it is mainly observable for frontal brain areas. This is true for easier novel cognitive tasks or after sufficient practice allowing participants to develop appropriate (efficient) strategies to deal with the task. In very complex tasks more able individuals seem to invest more cortical resources resulting in positive correlations between brain usage and cognitive ability. Based on the reviewed evidence we propose future empirical approaches in this field.}
}
@article{NEUBAUER2002515,
title = {Intelligence and neural efficiency: The influence of task content and sex on the brain–IQ relationship},
journal = {Intelligence},
volume = {30},
number = {6},
pages = {515-536},
year = {2002},
issn = {0160-2896},
doi = {10.1016/S0160-2896(02)00091-0},
url = {https://www.sciencedirect.com/science/article/pii/S0160289602000910},
author = {{Neubauer, A. C., Fink, A., & Schrausser, D. G.}},
keywords = {Intelligence, EEG sex, Elementary cognitive task, Task specificity},
abstract = {In studying physiological correlates of human intelligence, new brain imaging techniques like positron emission tomography (PET) and electroencephalography (EEG) mapping methods focus on the level and topographical distribution of cortical activation. Actually, there is strong empirical evidence that more intelligent individuals display a more focused cortical activation during cognitive performance resulting in lower total brain activation than in less intelligent individuals (i.e., neural efficiency hypothesis). Former studies have used only single, homogeneous tasks and most of the studies have been performed using males. Therefore, here the influence of different task content and of sex on the relationship between intelligence and cortical activation has been tested. In a sample of 26 males and 25 females, we administered verbal, numerical, and figural versions of a well-known elementary cognitive task, the so-called Posner task. Our results suggest a comparatively low cortical activation in brighter as compared to less intelligent individuals but this expected neural efficiency pattern interacted with sex and task content: In the verbal Posner task, the females were more likely to produce cortical activation patterns in line with the neural efficiency hypothesis, whereas in the figural task, primarily the males displayed the expected inverse relationship between IQ and cortical activation.}
}
@article{MICHELOYANNIS2006273,
title = {Using graph theoretical analysis of multi channel EEG to evaluate the neural efficiency hypothesis},
journal = {Neuroscience Letters},
volume = {402},
number = {3},
pages = {273-277},
year = {2006},
issn = {0304-3940},
doi = {10.1016/j.neulet.2006.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S030439400600382X},
author = {{Micheloyannis, S., Pachou, E., Stam, C. J., Vourkas, M., Erimaki, S., & Tsirka, V.}},
keywords = {Neural efficiency, Graph theory, Small-world network, Synchronization likelihood, Working memory},
abstract = {Previous studies demonstrated that intelligence is significantly related to an impressive array of psychological, social, biological and genetic factors and that working memory (WM) can be considered as a general cognitive resource strongly related with a wide variety of higher order cognitive competencies and intelligence. Also, evaluating the WM of subjects might allow one to test the neural efficiency hypothesis (NEH). WM typically involves functional interactions between frontal and parietal cortices. We recorded EEG signals to study neuronal interactions during one WM test in individuals who had few years of formal education (LE) as compared to individuals with university degrees (UE). The two groups of individuals differed in the scores they obtained in psychological tests. To quantify the synchronization between EEG channels in several frequency bands, we evaluated the “synchronization likelihood” (SL), which takes into consideration nonlinear processes as well as linear ones. SL was then converted into graphs to estimate the distance from “small-world network” (SWN) organization, i.e., an optimally organized network that would give rise to the data. In comparison to LE subjects, those with university degrees exhibited less prominent SWN properties in most frequency bands during the WM task. This finding supports the NEH and suggests that the connections between brain areas of well-educated subjects engaged in WM tasks are not as well-organized in the sense of SWN.}
}
@article{NEUBAUER2005217,
title = {Intelligence and neural efficiency: Further evidence of the influence of task content and sex on the brain–IQ relationship},
journal = {Cognitive Brain Research},
volume = {25},
number = {1},
pages = {217-225},
year = {2005},
issn = {0926-6410},
doi = {10.1016/j.cogbrainres.2005.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S092664100500162X},
author = {{Neubauer, A. C., Grabner, R. H., Fink, A., & Neuper, C.}},
keywords = {EEG, ERD, Neural efficiency, Sex difference, Task content, Verbal IQ, Visuo-spatial IQ, Topographical difference},
abstract = {In the field of physiological study of human intelligence, strong evidence of a more efficient operation (i.e., less activation) of the brain in brighter individuals (the neural efficiency hypothesis) can be found. Most studies in this field have used single, homogeneous tasks and have not examined sex differences. In analyzing the extent of Event-related Desynchronization (ERD) in the EEG during the performance of a verbal and a visuo-spatial task, we recently found that males and females display neural efficiency primarily in the domain where they usually perform better (i.e., verbal in females and spatial in males; cf. A.C. Neubauer, A. Fink, D.G. Schrausser, Intelligence and neural efficiency: the influence of task content and sex on brain–IQ relationship. Intelligence, 30 (2002) 515–536). However, this interpretation was complicated by differences in the complexity of the two tasks. By using a verbal (semantic) and a spatial (rotation) task of comparable complexity in this research, we sought to replicate and extend our earlier findings by additionally considering the individual differences in intelligence structure and the topographical distribution over the cortex. Findings were similar to the previous study: Females (n = 35) display neural efficiency (i.e., less brain activation in brighter individuals) primarily during the verbal task, males (n = 31) in the spatial task. However, the strength of this brain activation–IQ relationship varies with the intelligence factor: In males, the highest correlations were observed for spatial IQ, in females for verbal IQ. Furthermore, the sexes displayed topographical differences of neural efficiency patterns.}
}
@inproceedings{schrausser_2001_13738772,
author = {{Schrausser, D. G., Fink, A., & Neubauer, A. C.}},
booktitle={10th Biennial Meeting of the International Society for the Study of Individual Differences (ISSID)},
title = {Intelligence and neural efficiency as determined by EEG-Coherence},
address={Edinburgh, UK},
publisher={The University of Edinburgh},
year = {2001},
doi  = {10.5281/zenodo.13738772}
}
//
Intg
/////////////////////////////////
@book{Meyberg2001,
author={{Meyberg, K., & Vachenauer, P.}},
Title="H{\"o}here Mathematik 1: Differential- und Integralrechnung Vektor- und Matrizenrechnung",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
isbn="978-3-642-56654-7",
doi="10.1007/978-3-642-56654-7_4",
url="https://doi.org/10.1007/978-3-642-56654-7"
}
@Inbook{Meyberg2001int,
author={{Meyberg, K., & Vachenauer, P.}},
title="Integration",
bookTitle="H{\"o}here Mathematik 1: Differential- und Integralrechnung Vektor- und Matrizenrechnung",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="161--211",
abstract="Mit der Integration l{\"o}st man das Umkehrproblem, aus der Ableitung f' die urspr{\"u}ngliche Funktion f zu rekonstruieren. Die wesentliche, zur L{\"o}sung f{\"u}hrende Idee stammt aus dem Mittelwertsatz der Differentialrechnung ({\textrightarrow} Kap. 3, {\textsection}2): Zu jeder Zerlegung a = x0 < x1 < ... < xn−1 < xn= x des Intervalls [a, x], auf dem f differenzierbar ist, gibt es Zwischenpunkte $\xi$ ∈ [xi−1, xi], so da{\ss} gilt{\$}{\$}f{\backslash}left( {\{}{\{}x{\_}i{\}}{\}} {\backslash}right) - f{\backslash}left( {\{}{\{}x{\_}{\{}i - 1{\}}{\}}{\}} {\backslash}right) = f'{\backslash}left( {\{}{\{}{\backslash}xi {\_}i{\}}{\}} {\backslash}right){\backslash}left( {\{}{\{}x{\_}i{\}} - {\{}x{\_}{\{}i - 1{\}}{\}}{\}} {\backslash}right){\backslash}left( {\{}i = 1,2, {\backslash}ldots ,n{\}} {\backslash}right).{\$}{\$}",
isbn="978-3-642-56654-7",
doi="10.1007/978-3-642-56654-7_4",
url="https://doi.org/10.1007/978-3-642-56654-7_4"
}
// Riemann sum //
//
@Inbook{Guillemin2008,
author={{Guillemin, V. W., & Stroock, D. W.}},
editor={{Jorgensen, P. E. T., Merrill, K. D., & Packer, J. A.}},
title="Some Riemann Sums Are Better Than Others",
bookTitle="Representations, Wavelets, and Frames: A Celebration of the Mathematical Work of Lawrence W. Baggett",
year="2008",
publisher="Birkhäuser",
address="Boston, MA",
pages="3--12",
abstract="This note contains some results obtained while ruminating about Riemann sums. We know that nothing here is truly new, but we know of no other place in which these ideas are presented in the way that they occurred to us. In particular, some of them are so elementary that we hope they will find their way into calculus texts.",
isbn="978-0-8176-4683-7",
doi="10.1007/978-0-8176-4683-7_1",
url="https://doi.org/10.1007/978-0-8176-4683-7_1"
}
@Inbook{Lewis1978,
author={{Lewis, J. T., Osgood, C. F., & Shisha, O.}},
editor={{Beckenbach, E. F.}},
title="Infinite Riemann Sums, the Simple Integral, and the Dominated Integral",
bookTitle="General Inequalities 1 / Allgemeine Ungleichungen 1: Proceedings of the First International Conference on General Inequalities held in the Mathematical Research Institute at Oberwolfach, Black Forest, May 10--14, 1976 / Abhandlung zur ersten internationalen Tagung {\"u}ber Allgemeine Ungleichungen im Mathematischen Forschungsinstitut Oberwolfach, Schwarzwald vom 10. bis 14. Mai 1976",
year="1978",
publisher="Birkhäuser",
address="Basel",
pages="233--242",
abstract="Simple integrability of a function f (defined by Haber and Shisha in [2]) is shown to be equivalent to the convergence of the infinite Riemann sum <m:math display='block'><m:mrow><m:mstyle displaystyle='true'><m:munderover><m:mo>{\&}{\#}x2211;</m:mo><m:mrow><m:mi>k</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>{\&}{\#}x221E;</m:mi></m:munderover><m:mrow><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mrow><m:msub><m:mi>{\&}{\#}x03BE;</m:mi><m:mi>k</m:mi></m:msub></m:mrow><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>(</m:mo><m:mrow><m:msub><m:mi>x</m:mi><m:mi>k</m:mi></m:msub><m:mo>{\&}{\#}x2212;</m:mo><m:msub><m:mi>x</m:mi><m:mrow><m:mi>k</m:mi><m:mo>{\&}{\#}x2212;</m:mo><m:mn>1</m:mn></m:mrow></m:msub></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow></m:mstyle></m:mrow></m:math>{\$}{\$}{\backslash}sum{\backslash}limits{\_}{\{}k = 1{\}}^{\backslash}infty  {\{}f{\backslash}left( {\{}{\{}{\backslash}xi {\_}k{\}}{\}} {\backslash}right){\backslash}left( {\{}{\{}x{\_}k{\}} - {\{}x{\_}{\{}k - 1{\}}{\}}{\}} {\backslash}right){\}} {\$}{\$}to the improper Riemann integral <m:math display='block'><m:mrow><m:mstyle displaystyle='true'><m:mrow><m:msubsup><m:mo>{\&}{\#}x222B;</m:mo><m:mn>0</m:mn><m:mi>{\&}{\#}x221E;</m:mi></m:msubsup><m:mi>f</m:mi></m:mrow></m:mstyle></m:mrow></m:math>{\$}{\$}{\backslash}int{\_}0^{\backslash}infty  f {\$}{\$}f as the gauge of the partition <m:math display='block'><m:mrow><m:msubsup><m:mrow><m:mrow><m:mo>(</m:mo><m:mrow><m:msub><m:mi>x</m:mi><m:mi>k</m:mi></m:msub></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow><m:mrow><m:mi>k</m:mi><m:mo>=</m:mo><m:mn>0</m:mn></m:mrow><m:mi>{\&}{\#}x221E;</m:mi></m:msubsup></m:mrow></m:math>{\$}{\$}{\backslash}left( {\{}{\{}x{\_}k{\}}{\}} {\backslash}right){\_}{\{}k = 0{\}}^{\backslash}infty {\$}{\$}of [0,∞)converges to O. An analogous result is obtained for dominant integrability (defined by Osgood and Shisha in [5]). Also certain results of Bromwich and Hardy [1] are recovered.",
isbn="978-3-0348-5563-1",
doi="10.1007/978-3-0348-5563-1_25",
url="https://doi.org/10.1007/978-3-0348-5563-1_25"
}
@Inbook{Hassler2007,
author={{Hassler, U.}},
title="Riemann-Integrale",
bookTitle={{Stochastische Integration und Zeitreihenmodellierung: Eine Einführung mit Anwendungen aus Finanzierung und Ökonometrie}},
year="2007",
publisher="Springer",
address="Berlin, Heidelberg",
pages="137--153",
abstract="In diesem Kapitel besch{\"a}ftigen wir uns mit stochastischen Riemann-Integralen, d. h. mit gew{\"o}hnlichen Riemann-Integralen mit einem stochastischen Prozess als Integrand1. Mathematisch sind diese Gebilde relativ wenig anspruchsvoll, sie lie{\ss}en sich pfadweise wie in der konventionellen (deterministischen) Analysis {\"u}ber stetige Funktionen definieren. Diese pfadweise Definition wird aber z. B. bei Ito-Integralen im {\"u}bern{\"a}chsten Kapitel nicht mehr m{\"o}glich sein. Daher schlagen wir hier den auch sp{\"a}ter n{\"u}tzlichen Weg ein, Integrale als Grenzwert (im quadratischen Mittel) zu definieren. Ist der stochastische Integrand speziell ein Wiener-Prozess, so folgt das Riemann-Integral einer Normalverteilung mit Erwartungswert Null und bekannter Formel f{\"u}r die Varianz. Eine Reihe von Beispielen soll die Einf{\"u}hrung erleichtern.",
isbn="978-3-540-73568-7",
doi="10.1007/978-3-540-73568-7_7",
url="https://doi.org/10.1007/978-3-540-73568-7_7"
}
@book{Torchinsky2022,
title={A Modern View of the Riemann Integral},
author={{Torchinsky, A.}},
series={Lecture Notes in Mathematics},
doi={10.1007/978-3-031-11799-2},
publisher={Springer},
address={Cham},
year={2022},
edition={1}
}
@Inbook{Forster2023,
author={{Forster, O., & Lindemann, F.}},
title="Das Riemannsche Integral",
bookTitle={{Analysis 1: Differential- und Integralrechnung einer Veränderlichen}},
year="2023",
publisher="Springer Fachmedien",
address="Wiesbaden",
pages="283--302",
abstract="Die Integration ist neben der Differentiation die wichtigste Anwendung des Grenzwertbegriffs in der Analysis. Wir definieren das Integral zun{\"a}chst f{\"u}r Treppenfunktionen, wobei noch keine Grenzwertbetrachtungen n{\"o}tig sind und der elementargeometrische Fl{\"a}cheninhalt von Rechtecken zugrundeliegt. Das Integral allgemeinerer Funktionen wird dann durch Approximation mittels Treppenfunktionen definiert. Mithilfe sog. Riemannscher Summen berechnen wir explizit die Integrale einiger elementarer Funktionen.",
isbn="978-3-658-40130-6",
doi="10.1007/978-3-658-40130-6_18",
url="https://doi.org/10.1007/978-3-658-40130-6_18"
}
@Inbook{Büchter2010,
author={{Büchter, A., & Henn, H.-W.}},
title="Grenzwerte von Riemann'schen Summen: das Integral",
bookTitle="Elementare Analysis: Von der Anschauung zur Theorie",
year="2010",
publisher="Spektrum Akademischer Verlag",
address="Heidelberg",
pages="221--235",
abstract="In diesem Kapitel wollen wir nach der Differenzial- auch die Integralrechnung auf eine mathematisch pr{\"a}zisere Grundlage stellen, sodass wir im n{\"a}chsten Kapitel -- gewisserma{\ss}en als H{\"o}hepunkt unserer Theorieentwicklung -- den Hauptsatz, der Differenzial- und Integralrechnung miteinander verbindet (vgl. 3.3), ebenso pr{\"a}zise formulieren und beweisen k{\"o}nnen.",
isbn="978-3-8274-2680-2",
doi="10.1007/978-3-8274-2680-2_6",
url="https://doi.org/10.1007/978-3-8274-2680-2_6"
}
@article{e941eb5c-b072-3fd7-a80c-cfa5c7173324,
 ISSN = {07468342, 19311346},
 URL = {https://www.jstor.org/stable/48662056},
 abstract = {It is well known that Riemann sums converge toward the integral of any Riemannintegrable function f on the segment [a, b]. In some cases this property is still valid for generalized integrals and can be useful to solve problems involving series and sums. In this note we prove (Theorem 3) that the result still holds for a generalized integral under the assumption that the function is decreasing on the interval. We give two counterexamples: the first one with a linear piecewise function, and a second one based on an integral suggested by Hardy. The last section shows recent applications of the Riemann sums in computer science and applied mathematics.},
 author = {{Truc, J.-P.}},
 journal = {The College Mathematics Journal},
 number = {2},
 pages = {pp. 123--132},
 publisher = {[Mathematical Association of America, Taylor & Francis, Ltd.]},
 title = {Riemann Sums for Generalized Integrals},
 urldate = {2026-01-12},
 volume = {50},
 year = {2019}
}
@article{fbd8e02f-8560-36a2-a00c-e74f040810fc,
 ISSN = {0025570X, 19300980},
 URL = {http://www.jstor.org/stable/2689344},
 author = {{Goel, S. K., & Rodriguez, D. M.}},
 journal = {Mathematics Magazine},
 number = {4},
 pages = {225--228},
 publisher = {[Mathematical Association of America, Taylor & Francis, Ltd.]},
 title = {A Note on Evaluating Limits Using Riemann Sums},
 urldate = {2026-01-12},
 volume = {60},
 year = {1987}
}
// Romberg extrapol //
//
@Inbook{Herrmann1983,
author={{Herrmann, D.}},
title="Romberg-Integration",
bookTitle="Numerische Mathematik --- 40 BASIC-Programme",
year="1983",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="84--86",
abstract="Die bei Programm 22 verwendete Romberg-Extrapolation l{\"a}{\ss}t sich auch auf die numerische Integration anwenden.",
isbn="978-3-322-96321-5",
doi="10.1007/978-3-322-96321-5_27",
url="https://doi.org/10.1007/978-3-322-96321-5_27"
}
@article{TALAY1990143,
title = {Romberg extrapolations for numerical schemes solving stochastic differential equations},
journal = {Structural Safety},
volume = {8},
number = {1},
pages = {143-150},
year = {1990},
issn = {0167-4730},
doi = {10.1016/0167-4730(90)90036-O},
url = {https://www.sciencedirect.com/science/article/pii/016747309090036O},
author = {{Talay, D., & Tubaro, L.}},
keywords = {Monte Carlo method, numerical analysis, probability distribution, random process, simulation},
abstract = {Let (Xt) be the solution of a stochastic differential system. We consider the following situations: computation of Eƒ(Xt) by a Monte-Carlo method (for example, computation of moments of the solution); integration of ƒ(·) with respect to the invariant probability law of (Xt) (when this process is ergodic); or of the upper Lyapunov exponent, by simulating a single trajectory. We propose to perform an extrapolation between approximate values due to first-order schemes; we show that this algorithm (simpler to implement than second-order schemes) provides a second-order accuracy, and we give results of numerical tests.}
}
@inbook{doi:https://doi.org/10.1002/9781119245476.ch7,
author={{Allen, M. B., & Isaacson, E. L}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781119245476},
title = {Numerical Integration},
booktitle = {Numerical Analysis for Applied Science},
chapter = {7},
pages = {363-401},
doi = {10.1002/9781119245476.ch7},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119245476.ch7},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119245476.ch7},
year = {2019},
keywords = {adaptive quadrature, Gauss quadrature, Legendre polynomials, numerical integration, Romberg quadrature},
abstract = {Summary Elementary techniques for computing a definite integral use the fundamental theorem of calculus. This chapter discusses more sophisticated techniques that yield accurate approximations with less computational effort. It surveys two methods for enhancing the accuracy of composite quadrature formulas. The first method, Romberg quadrature, uses approximations to the integral that have low-order accuracy to compute high-order approximations. The second approach, adaptive quadrature, encompasses a class of strategies for tailoring composite formulas to local, idiosyncratic behavior in the integrand. The chapter also explores the theory of Gauss quadrature. In doing so, it generalizes the framework based on the Legendre polynomials, to include other Gauss quadrature methods based on different orthogonal systems of polynomials. In each case, there is a specific interval of integration associated with the basic quadrature method. However, each method readily extends to more general intervals via the change-of-variables tactic.}
}
@article{30c28040-c164-336f-b877-2f793977628d,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2324787},
 author = {{von Petersdorff, T.}},
 journal = {The American Mathematical Monthly},
 number = {8},
 pages = {783--785},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {A Short Proof for Romberg Integration},
 urldate = {2026-01-11},
 volume = {100},
 year = {1993}
}
@article{Jetter1984,
author={{Jetter, K.}},
year={1984},
title={Ein kurze Anmerkung zur Romberg-Integration},
journal={Numerische Mathematik},
page={{275--281}},
volume={45},
number={2},
abstract={Subject to rather mild assumptions on the integrand, the Romberg table (theT-table) and the modified Romberg table (theU-table) yield asymptotically upper and lower bounds for the value of the integral, and the convergence of the columns of the two tables is asymptotically monotone. This is verified for arbitrary sequences of step sizes satisfying the usual condition of convergence for Romberg integration.},
doi={10.1007/BF01389471}
}
// Richardson extra //
//
@article{cc7b552a-2f22-3972-bf87-40a114018835,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2288854},
 abstract = {Simulation methods [particularly Efron's (1979) bootstrap] are being applied more and more frequently in statistical inference. Given data (X1, ..., Xn) distributed according to P, which belongs to a hypothesized model P, the basic goal is to estimate the distribution LP of a function Tn(X1, ..., Xn, P). The bootstrap presupposes the existence of an estimate P̂(X1, ..., Xn) and involves estimating Lp by the distribution L*n of Tn(X*1, ..., X*n, P̂), where (X*1, ..., X*n) is distributed according to P̂. The method is of particular interest when L*n, though known in principle, can realistically only be computed by simulation. Such computation can be expensive if n is large and Tn is complex (e.g., see the multivariate goodness-of-fit tests of Beran and Millar 1986). Even when bootstrap application to a single data set is not excessively expensive, Monte Carlo studies of the bootstrap are another matter. We propose a method based on the classical ideas of Richardson extrapolation for reducing the computational cost inherent in bootstrap simulations and Monte Carlo studies of the bootstrap, by performing simulations for statistics based on two smaller sample sizes. We study theoretically which ratio of the two small sample sizes is apt to give best results. We show how our method works for approximating the χ2, t, and smoothed binomial distributions, and for setting bootstrap percentile confidence intervals for the variance of a normal distribution with a mean of 0.},
 journal = {Journal of the American Statistical Association},
 number = {402},
 pages = {387--393},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
author={{Bickel, P. J., & Yahav, J. A.}},
 title = {Richardson Extrapolation and the Bootstrap},
 urldate = {2026-01-11},
 volume = {83},
 year = {1988}
}
@inbook{Sidi_2003, 
place={Cambridge}, 
series={Cambridge Monographs on Applied and Computational Mathematics}, 
title={The Richardson Extrapolation Process and Its Generalizations}, 
booktitle={Practical Extrapolation Methods: Theory and Applications}, 
publisher={Cambridge University Press}, 
author={{Sidi, A.}}, 
year={2003}, 
pages={19–20}, 
doi={10.1017/CBO9780511546815},
collection={Cambridge Monographs on Applied and Computational Mathematics}
}
@article{doi:10.1137/21M1397349,
author = {{Bach, F.}},
title = {On the Effectiveness of Richardson Extrapolation in Data Science},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {3},
number = {4},
pages = {1251-1277},
year = {2021},
doi = {10.1137/21M1397349},
abstract = { Richardson extrapolation is a classical technique from numerical analysis that can improve the approximation error of an estimation method by combining linearly several estimates obtained from different values of one of its hyperparameters without the need to know in details the inner structure of the original estimation method. The main goal of this paper is to study when Richardson extrapolation can be used within data science beyond the existing applications to step-size adaptations in stochastic gradient descent. We identify two situations where Richardson interpolation can be useful: (1) when the hyperparameter is the number of iterations of an existing iterative optimization algorithm with applications to averaged gradient descent and Frank--Wolfe algorithms (where we obtain asymptotically rates of \$O(1/k^2)\$ on polytopes, where \$k\$ is the number of iterations) and (2) when it is a regularization parameter with applications to Nesterov smoothing techniques for minimizing nonsmooth functions (where we obtain asymptotically rates close to \$O(1/k^2)\$ for nonsmooth functions) and kernel ridge regression. In all these cases, we show that extrapolation techniques come with no significant loss in performance but with sometimes strong gains, and we provide theoretical justifications based on asymptotic developments for such gains, as well as empirical illustrations on classical problems from machine learning. }
}
@article{DUBEAU2019100017,
title = {A remark on Richardson's extrapolation process and numerical differentiation formulae},
journal = {Journal of Computational Physics: X},
volume = {2},
pages = {100017},
year = {2019},
issn = {2590-0552},
doi = {10.1016/j.jcpx.2019.100017},
url = {https://www.sciencedirect.com/science/article/pii/S2590055219300332},
author = {{Dubeau, F.}},
keywords = {Numerical differentiation, Richardson's extrapolation process},
abstract = {Richardson's extrapolation process is a well known method to improve the order of several approximation processes. Here we observe that for numerical differentiation, Richardson's process can be applied not only to improve the order of a numerical differentiation formula but also to find in fact the original formula.}
}
@Inbook{Sidi1988,
author={{Sidi, A.}},
editor={{Brass, H., & Hämmerlin, G.}},
title="Generalizations of Richardson Extrapolation with Applications to Numerical Integration",
bookTitle="Numerical Integration III: Proceedings of the Conference held at the Mathematisches Forschungsinstitut, Oberwolfach, Nov. 8 -- 14, 1987",
year="1988",
publisher={{Birkhäuser}},
address="Basel",
pages="237--250",
abstract="The application of extrapolation (or convergence acceleration) methods to the solution of problems in numerical analysis has become very widespread in recent years. One important area in which extrapolation methods have proved to be remarkably efficient is that of numerical integration. Of the methods that have proved to be useful in this area, the Richardson extrapolation process and some of its generalizations have been the subject of intense research. It is the purpose of this paper to survey briefly the recent developments relating to the generalizations of the Richardson extrapolation process with special emphasis on their application to the numerical evaluation of finite and infinite range integrals.",
isbn="978-3-0348-6398-8",
doi="10.1007/978-3-0348-6398-8_22",
url="https://doi.org/10.1007/978-3-0348-6398-8_22"
}
@Inbook{Rannacher1987,
author={{Rannacher, R.}},
editor={{Hackbusch, W., & Witsch, K.}},
title="Richardson Extrapolation with Finite Elements",
bookTitle="Numerical Techniques in Continuum Mechanics: Proceedings of the Second GAMM-Seminar, Kiel, January 17 to 19, 1986",
year="1987",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="90--101",
abstract="In a recent paper by H. Blum, Lin Q., and the author,[l], it has been shown that the Ritz projection method with linear finite elements admits an asymptotic error expansion for certain classes of ``uniform'' meshes. This provides the theoretical justification for the use of Richardson extrapolation or related correction processes for increasing the accuracy of the scheme. The proofs are entirely based on finite element techniques and extend to situations where no ``discrete'' maximum principle is available. This is examplarily demonstrated here for the second-order Lam{\'e}-Navier system in plain linear elasticity and for a mixed formulation of the fourth-order Kirchhoff plate bending model.",
isbn="978-3-322-85997-6",
doi="10.1007/978-3-322-85997-6_9",
url="https://doi.org/10.1007/978-3-322-85997-6_9"
}
// Trapezregel //
//
@Inbook{Neher2024,
author={{Neher, M.}},
title="Numerische Integration",
bookTitle={{Numerische Mathematik: Eine anschauliche modulare Einführung}},
year="2024",
publisher="Springer",
address="Berlin, Heidelberg",
pages="195--222",
abstract="Manche Anwendungsprobleme lassen sich auf die Berechnung eines bestimmten Integrals zur{\"u}ckf{\"u}hren. Ist keine Stammfunktion des Integranden bekannt, muss die Integrationsaufgabe n{\"a}herungsweise gel{\"o}st werden. Verschiedene geeignete Verfahren werden vorgestellt und analysiert.",
isbn="978-3-662-68815-1",
doi="10.1007/978-3-662-68815-1_7",
url="https://doi.org/10.1007/978-3-662-68815-1_7"
}
@Inbook{Schwarz1997,
author={{Schwarz, H. R.}},
title="Integralberechnung",
bookTitle="Numerische Mathematik",
year="1997",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="375--411",
abstract="Manche Probleme der angewandten Mathematik f{\"u}hren auf die Berechnung von Integralen, die meistens nicht in expliziter Form dargestellt werden k{\"o}nnen. Die numerische Integralberechnung, die man kurz als Quadratur bezeichnet, spielt deshalb eine wichtige Rolle. Wir befassen uns im folgenden mit der gen{\"a}herten Berechnung von bestimmten Integralen. Unbestimmte Integrale (Stammfunktionen) werden zweckm{\"a}{\ss}ig als Anfangswertprobleme gew{\"o}hnlicher Differentialgleichungen behandelt (vgl. Kapitel 9). Von den zahlreichen Anwendungen der numerischen Quadratur nennen wir die Berechnung von Oberfl{\"a}chen, Volumina, Wahrscheinlichkeiten und Wirkungsquerschnitten, die Auswertung von Integraltransformationen und Integralen im Komplexen, die Konstruktion von konformen Abbildungen f{\"u}r Polygonbereiche nach der Formel von Schwarz-Christoffel [Hen 85], die Behandlung von Integralgleichungen etwa im Zusammenhang mit der Randelementmethode und schlie{\ss}lich die Methode der finiten Elemente [Scw91].",
isbn="978-3-663-01227-6",
doi="10.1007/978-3-663-01227-6_8",
url="https://doi.org/10.1007/978-3-663-01227-6_8"
}
@article{Abdulhameed_2021,
doi = {10.1088/1742-6596/2090/1/012104},
url = {https://doi.org/10.1088/1742-6596/2090/1/012104},
year = {2021},
month = {nov},
publisher = {IOP Publishing},
volume = {2090},
number = {1},
pages = {012104},
author = {{Abdulhameed, A. F., & Memon, Q. A.}},
title = {An improved Trapezoidal rule for numerical integration},
journal = {Journal of Physics: Conference Series},
abstract = {Numerical Methods have attracted of research community for solving engineering problems. This interest is due to its practicality and the improvement of highspeed calculations done on current century processors. The increase in numerical method tools in engineering software, such as Matlab, is an example of the increased interest. In this paper, we are present a new improved numerical integration method, that is based on the well-known trapezoidal rule. The proposed method gives a great enhancement to the trapezoidal rule and overcomes the issue of the error value when dealing with some higher order functions even when solving for a single interval. After literature review, the proposed system is mathematically explained along with error analysis. Few examples are illustrated to prove improved accuracy of the proposed method over traditional trapezoidal method.}
}
@article{IZZO2022111193,
title = {Corrected trapezoidal rules for singular implicit boundary integrals},
journal = {Journal of Computational Physics},
volume = {461},
pages = {111193},
year = {2022},
issn = {0021-9991},
doi = {10.1016/j.jcp.2022.111193},
url = {https://www.sciencedirect.com/science/article/pii/S0021999122002558},
author = {{Izzo, F., Runborg, O., & Tsai, R.}},
keywords = {Level set methods, Closest point projection, Boundary integral formulations, Singular integrals, Trapezoidal rules},
abstract = {We present new higher-order quadratures for a family of boundary integral operators re-derived using the approach introduced in Kublik et al. (2013) [7]. In this formulation, a boundary integral over a smooth, closed hypersurface is transformed into an equivalent volume integral defined in a sufficiently thin tubular neighborhood of the surface. The volumetric formulation makes it possible to use the simple trapezoidal rule on uniform Cartesian grids and relieves the need to use parameterization for developing quadrature. Consequently, typical point singularities in a layer potential extend along the surface's normal lines. We propose new higher-order corrections to the trapezoidal rule on the grid nodes around the singularities. This correction is based on local decompositions of the singularity and is dependent on the angle of approach to the singularity relative to the surface's principal curvature directions. The proposed decomposition, combined with the volumetric formulation, leads to a special quadrature error cancellation.}
}
@incollection{luke1969214,
title = {Chapter XV Trapezoidal Rule Integration Formulas},
editor = {{Luke, Y. L.}},
series = {Mathematics in Science and Engineering},
publisher = {Elsevier},
volume = {53},
pages = {214-226},
year = {1969},
booktitle = {The Special Functions and their Approximations},
issn = {0076-5392},
doi = {10.1016/S0076-5392(09)60075-8},
url = {https://www.sciencedirect.com/science/article/pii/S0076539209600758},
abstract = {Publisher Summary
This chapter discusses the trapezoidal rule integration formulas. The development of numerical integration formulas as such is not in the mainstream of the mathematical topics covered in the chapter. The modified trapezoidal rule uses functional values at points which lie midway between those points that apply for the trapezoidal rule. There is a certain class of transcendental functions defined by integrals which can be computed to high accuracy by use of these rules; with a view toward the applications, the chapter takes up this aspect of the subject. Algorithms for the evaluation of definite integrals by means of equally spaced data are closely related to the Euler-Maclaurin summation formula for which there is a vast literature. Comparison of the two formulations shows that the difference in accuracy is negligible.}
}
@article{Bujang10.1063/5.0294833,
    author = {{Bujang, N., Nasir, M. A. S., & Ijam, H. M.}},
    title = {Numerical integration of function using the modified trapezoidal rule and mean averaging method},
    journal = {AIP Conference Proceedings},
    volume = {3338},
    number = {1},
    pages = {040010},
    year = {2025},
    month = {12},
    abstract = {There are many numerical integration methods that exist with its own advantage and disadvantage. This study focusses on the Trapezoidal rule, TR. The advantage of the Trapezoidal rule is that it is simple to use and can be applied to a wide range of functions. Since the Trapezoidal rule is an approximation rule, it will yield an error. In this study, we aim to reduce the error of the Trapezoidal rule can be reduced by applying mean averaging toward Trapezoidal rule called as Trapezoidal rule with mean averaging, TRMEAN. The arithmetic mean, geometric mean, Harmonic mean, Heronian mean, Contra harmonic, Root-mean-square mean, and Centroidal mean will be implemented. Hence, we will be experimented as examples before concluding the results. The results of this study showed that the error from TRMEAN is smaller as compared to TR. The findings indicate that TRMEAN presents a promising improvement in accuracy and reliability for this task over the original TR method. However, further research and validation are essential to firmly establish these conclusions and assess their applicability to diverse datasets and situations.},
    issn = {0094-243X},
    doi = {10.1063/5.0294833},
    url = {https://doi.org/10.1063/5.0294833},
    eprint = {https://pubs.aip.org/aip/acp/article-pdf/doi/10.1063/5.0294833/20840220/040010_1_5.0294833.pdf},
}
//
Cubspl
/////////////////////////////////
@article{jarre2025cubicsplinefunctionsrevisited,
title={Cubic spline functions revisited}, 
author={{Jarre, F.}},
year={2025},
journal={arXiv},
eprint={2507.05083},
archivePrefix={arXiv},
primaryClass={math.NA},
url={https://arxiv.org/abs/2507.05083},
doi={10.48550/arXiv.2507.05083}
}
@article{JARRE2026117240,
title = {Cubic spline functions revisited},
journal = {Journal of Computational and Applied Mathematics},
volume = {478},
pages = {117240},
year = {2026},
issn = {0377-0427},
doi = {10.1016/j.cam.2025.117240},
url = {https://www.sciencedirect.com/science/article/pii/S037704272500754X},
author = {{Jarre, F.}},
keywords = {Cubic spline, Natural spline, Error estimate, Condition number},
abstract = {In this paper a fourth order asymptotically optimal error bound for a new cubic interpolating spline function, denoted by Q-spline, is derived for the case that only function values at given points are used but not any derivative information. The bound seems to be stronger than earlier error bounds for cubic spline interpolation in such setting such as the not-a-knot spline. A brief analysis of the conditioning of the end conditions of cubic spline interpolation leads to a modification of the not-a-knot spline, and some numerical examples suggest that the interpolation error of this revised not-a-knot spline generally is comparable to the near optimal Q-spline and lower than for the not-a-knot spline when the mesh size is small.}
}
@Inbook{Sarfraz2008,
author={{Sarfraz, M.}},
title="Visualization of Shaped Data by Cubic Spline Interpolation",
bookTitle="Interactive Curve Modeling: With Applications to Computer Graphics, Vision and Image Processing",
year="2008",
publisher="Springer London",
address="London",
pages="157--172",
abstract="This chapter reiterates the subject of the previous chapter. Instead of a rational cubic model, a polynomial cubic spline has been presented here for the same objective. A piecewise cubic spline has been introduced to preserve the shape of the data when it is convex, monotone or positive. The spline representation is interpolatory and applicable to the scalar valued data. The shape parameters, in the description of the cubic, have been constrained in such a way that they control the shape of the curve to avoid any noise. As far as visual smoothness is concerned, the curve scheme under discussion is GC1. Thus the continuity constraints have been relaxed from C1 to GC1.",
isbn="978-1-84628-871-5",
doi="10.1007/978-1-84628-871-5_8",
url="https://doi.org/10.1007/978-1-84628-871-5_8"
}
@InProceedings{abd10.1007/978-3-031-27199-1_3,
author={{Abduganiev, M., Azimov, R., & Muydinov, L.}},
editor={{Zaynidinov, H., Singh, M., Tiwary, U. S., & Singh, D.}},
title="Digital Processing Algorithms of Biomedical Signals Using Cubic Base Splines",
booktitle="Intelligent Human Computer Interaction",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="18--26",
abstract="In this article, the issues of digital processing and restoration of electroencephalogram (EEG) signals from biomedical signals are considered, the location of 21 sensors in the EEG apparatus along the brain, the naming of the sensors, their connection types, the use of bipolar coupling in the detection of disease symptoms, interpolation of received signals, disease symptoms. The processes of separating parts into scales have been studied. During the work, the B-spline function was selected as the most convenient mathematical model for digital processing of EEG signals, and the construction of the B-spline function was presented. Based on the constructed mathematical model, an algorithm for restoring the electroencephalogram signals by dividing the problematic parts into scales was developed, and the absolute error in the restoration of EEG signals was estimated.",
isbn="978-3-031-27199-1",
doi={10.1007/978-3-031-27199-1_3}
}
@Inbook{Farin1994,
author={{Farin, G.}},
title="Kubische Spline-Interpolation",
bookTitle="Kurven und Fl{\"a}chen im Computer Aided Geometric Design: Eine praktische Einf{\"u}hrung",
year="1994",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="104--121",
abstract="In diesem Kapitel werden wir uns mit dem popul{\"a}rsten Kurvenschema befassen: C2-stetige kubische interpolierende Splines. Wir haben schon gesehen, da{\ss} Lagrange-Interpolation keine brauchbaren Resultate liefert. Wir haben aber andererseits gesehen, da{\ss} uns durch kubische B-Spline-Kurven ein m{\"a}chtiges Werkzeug zum Modellieren an die Hand gegeben wurde; sie k{\"o}nnen auf einfache Weise komplexe Formen wiedergeben. Diese „Wiedergabe`` geschieht als Approximationsproze{\ss}, man ver{\"a}ndert das Kontrollpolygon solange, bis die gew{\"u}nschte Form gefunden ist. Wir werden sehen, wie man kubische Splines auch zur Interpolation benutzen kann, n{\"a}mlich, eine Splinekurve zu finden, die durch einen gegebenen Satz von Punkten verl{\"a}uft. Kubische Spline-Interpolation ist in die CAGD-Literatur von J. Ferguson [184] im Jahre 1964 eingef{\"u}hrt worden, w{\"a}hrend die zugrundeliegende Mathematik in der Approximationstheorie (siehe de Boor [112] oder Holladay [266]) entwickelt wurde. Eine {\"U}bersicht {\"u}ber die Geschichte der Splines ist bei Schumaker [426] zu finden.",
isbn="978-3-663-10602-9",
doi="10.1007/978-3-663-10602-9_9",
url="https://doi.org/10.1007/978-3-663-10602-9_9"
}
@Inbook{Agarwal1993,
author={{Agarwal, R. P., & Wong, P. J. Y.}},
title="Spline Interpolation",
bookTitle="Error Inequalities in Polynomial Interpolation and Their Applications",
year="1993",
publisher="Springer Netherlands",
address="Dordrecht",
pages="281--362",
abstract="Spline interpolation is an improvement over piecewise --- polynomial interpolation. It uses less information of the given function, yet furnishes smoother interpolates. The plan of this chapter is as follows: In Section6.2 we shall define the spline space Sm,$\tau$($\Delta$), and for a given function x (t) the spline and Lidstone --- spline interpolates Sm,$\tau$$\Delta$and LSm,2m−2$\Delta$x(t),respectively. Here, we shall also show that to acquire the bounds for {\textbardbl} Dk(x− Sm,$\tau$$\Delta$) {\textbardbl}∞ and {\textbardbl} Dk(x− LSm,2m−2$\Delta$x) {\textbardbl}∞ in terms of the derivatives of x(t) it is necessary to estimate several terms. While some of these terms can be estimated by the results of Chapter 5, other terms which require a different analysis for each m and $\tau$, need to be bounded. In Sections6.3,6.4 and 6.6 respectively, we shall consider the cases m = 2, $\tau$ = 2; m = 3, $\tau$ = 4 and m = 3, $\tau$ = 3. These cases correspond to cubic in the class C(2) [a, b], and quintic in the classes C(4)[a, b] and C(3)[a, b] spline interpolates. For the cases m = 3, $\tau$ = 4 and m = 3, $\tau$ = 3 in Sections 6.5 and 6.7 respectively, we shall discuss the construction of approximated quintic splines, and for these interpolates we will provide explicit error bounds in L∞ --- norm. For the cubic and quintic Lidstone --- spline interpolates error bounds in L∞ --- norm are obtained in Sections 6.8 and 6.9, respectively. In Section6.10 we shall extend Theorems 5.3.16 and 5.3.17 for the spline interpolate Sm,$\tau$$\Delta$x(t)",
isbn="978-94-011-2026-5",
doi="10.1007/978-94-011-2026-5_6",
url="https://doi.org/10.1007/978-94-011-2026-5_6"
}
@article{CHENG1991700,
title = {Interproximation: interpolation and approximation using cubic spline curves},
journal = {Computer-Aided Design},
volume = {23},
number = {10},
pages = {700-706},
year = {1991},
issn = {0010-4485},
doi = {10.1016/0010-4485(91)90023-P},
url = {https://www.sciencedirect.com/science/article/pii/001044859190023P},
author = {{Cheng, F., & Barsky, B. A.}},
keywords = {splines, interpolation, uncertain data},
abstract = {An algorithm for the construction of a cubic spline curve with relatively good shape that interpolates specified data points at some knots and passes through specified regions at some other knots is presented. The curve constructed by the algorithm has minimum energy on each of its components. This algorithm has applications in various fields, such as the reconstruction of natural phenomena where data points cannot be sampled exactly, or computer-aided modeling where some of the fitting points cannot be explicitly specified.}
}
@article{PERRIN198775,
title = {Mapping of scalp potentials by surface spline interpolation},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {66},
number = {1},
pages = {75-81},
year = {1987},
issn = {0013-4694},
doi = {10.1016/0013-4694(87)90141-6},
url = {https://www.sciencedirect.com/science/article/pii/0013469487901416},
author = {{Perrin, F., Pernier, J., Bertnard, O., Giard M. H.,  & Echallier, J. F.}},
keywords = {interpolation, spline},
abstract = {Evoked potentials and EEGs record punctuate electrical activity at electrode sites. To represent the overall potential distribution on the entire scalp it is necessary to interpolate between these sampled values. Surface splines are mathematical tools for interpolating functions of two variables. In comparison to the classical methods of interpolation, based on linear combination of the potentials of the 4 nearest electrodes, spline methods are smoother, give more precisely located extrema and converge faster toward the ‘true’ potential surface when the number of recording electrodes is increased. These advantages are at the expense of lengthier computation time.
Résumé
Les potentiels évoqués comme l'électroencéphalographie n'enregistrent l'activité électrique du scalp qu'en des points précis correspondant à l'emplacement des électrodes. Pour avoir une représentation globale de la distribution du potentiel sur le scalp, il est nécessaire d'interpoler entre ces points de mesure. Les surfaces splines permettent d'effectuer de telles interpolations bi-dimensionnelles. Par comparaison avec la méthode d'interpolation couramment utilisée basée sur la combinaison linéaire des potentiels des 4 plus proches électrodes la méthode des splines donne des surfaces plus régulières, les extrêmes sont mieux localisés et quand on augmente le nombre d'électrodes, les surfaces obtenues convergent plus rapidement vers la ‘vraie’ surface. En contrepartie le calcul de ces fonctions est plus long.}
}
@article{SOUFFLET1991393,
title = {A statistical evaluation of the main interpolation methods applied to 3-dimensional EEG mapping},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {79},
number = {5},
pages = {393-402},
year = {1991},
issn = {0013-4694},
doi = {10.1016/0013-4694(91)90204-H},
url = {https://www.sciencedirect.com/science/article/pii/001346949190204H},
author = {{Soufflet, L., Toussaint, M., Luthringer, R., Gresser, J., Minot, R., & Macher, J. P.}},
keywords = {EEG, 3D interpolation, Image synthesis},
abstract = {This paper is a review of the main interpolation methods applicable to 3-dimensional EEG mapping. The use of simple statistical comparison methods on recorded EEG maps allowed us to evaluate the qualities of interpolation methods belonging to 3 mathematical families (barycentric, polynomial, spline). A combination of a 3-dimensional representation of EEG maps and a reliable interpolation method makes it possible to obtain better spatial resolution than with standard planar mapping.}
}
@INPROCEEDINGS{5360654,
  author={{Jiang, H., & Zhao, Y.}},
  booktitle={2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery}, 
  title={The Study of Interpolation Algorithm Based on Cubic Spline in Marching Cubes Method}, 
  year={2009},
  volume={5},
  number={},
  pages={80-83},
  keywords={Interpolation;Spline;Isosurfaces;Boundary conditions;Software algorithms;Convergence;Hardware;Costs;Biomedical image processing;Rendering (computer graphics);Cubic spline;MC;interpolation},
  doi={10.1109/FSKD.2009.591}
}
@article{TOROK2025129411,
title = {Explicit forms of interpolating cubic splines and data smoothing},
journal = {Applied Mathematics and Computation},
volume = {500},
pages = {129411},
year = {2025},
issn = {0096-3003},
doi = {10.1016/j.amc.2025.129411},
url = {https://www.sciencedirect.com/science/article/pii/S0096300325001389},
author = {{Török, C., Hudák, J., Pristaš, V., & Antoni, L.}},
keywords = {Data smoothing, Cubic splines, Interpolation, Linear regression},
abstract = {We express the interpolating cubic splines of class C2 in their new, explicit forms. We construct the desired forms, the spline's Hermitian and B-spline representations for both equidistant and arbitrary nodes. During this process we demonstrate an innovative way to compute the inverse of a special class of tridiagonal matrices. Afterward, we propose the corresponding interpolating spline based linear regression models with easily interpretable coefficients suitable for smoothing data of complex structures.}
}
@article{MOON2001251,
title = {An explicit solution for the cubic spline interpolation for functions of a single variable},
journal = {Applied Mathematics and Computation},
volume = {117},
number = {2},
pages = {251-255},
year = {2001},
issn = {0096-3003},
doi = {10.1016/S0096-3003(99)00178-2},
url = {https://www.sciencedirect.com/science/article/pii/S0096300399001782},
author = {{Moon, B. S.}},
keywords = {Explicit solution, Cubic spline interpolation, B-splines, Inverse matrix},
abstract = {An algorithm for computing the cubic spline interpolation coefficients without solving the matrix equation involved is presented in this paper. It requires only O(n) multiplication or division operations for computing the inverse of the matrix compared to O(n2) or larger number of operations in the Gauss elimination method.}
}
@article{SUN2023115039,
title = {Cubic spline interpolation with optimal end conditions},
journal = {Journal of Computational and Applied Mathematics},
volume = {425},
pages = {115039},
year = {2023},
issn = {0377-0427},
doi = {10.1016/j.cam.2022.115039},
url = {https://www.sciencedirect.com/science/article/pii/S0377042722006379},
author = {{Sun, M., Lan, L., Zhu C. G., & Lei, F.}},
keywords = {Cubic spline interpolation, End conditions, Not-a-knot end condition, Interpolation error estimation},
abstract = {Traditional end conditions for cubic spline interpolation consist of values, the first or the second derivatives of interpolated functions on the boundary interpolation knots. The not-a-knot end condition proposed by de Boor (1985) is a kind of end condition of cubic spline interpolation for the practical application without the requirements of the derivatives at the end knots. However, a significant disadvantage of such end condition is that there is a sharp decrease in the accuracy of the interpolation at boundary intervals. In this paper, by changing the locations of two spline knots in not-a-knot end condition, we propose the optimal arrangement of shifted spline knots for cubic spline interpolation. The proposed scheme leads to an approximately 3.4 times increasing in the accuracy of the interpolation compared to the de Boor’s not-a-knot end condition. Furthermore, we also present the optimal end conditions of cubic spline interpolation to approximate the first and the second derivatives of interpolated functions. The representative examples show the effectiveness and the superiority of the proposed method.}
}
@article{Congedo01092002,
author = {{Congedo, M., Özen, C., & Sherlin, L.}},
title = {Notes on EEG Resampling by Natural Cubic Spline Interpolation},
journal = {Journal of Neurotherapy},
volume = {6},
number = {4},
pages = {73--80},
year = {2002},
publisher = {Routledge},
doi = {10.1300/J184v06n04_08},
abstract = {Resampling of digitized electroencephalographic data allows changing the sampling rate with minimal distortion of the signal. Useful applications of the procedure include compatibility among diverse hardware and software and the customization of data analysis. The natural cubic spline interpolation procedure is introduced in a discursive fashion. A formal presentation is provided in the appendix.}
}
@article{roy2017,
author={{Roy, V., & Shukla, S.}},
year={2017},
title={Effective EEG Motion Artifacts Elimination Based on Comparative Interpolation Analysis},
journal={Wireless Personal Communications},
pages={6441--6451},
volume={97},
number={4},
abstract={Electroencephalogram (EEG) signal is usually suffered from motion artifacts, generated randomly during signal acquisition timings. These artifacts sturdily affect the investigation and therefore, diagnosis of neural diseases from EEG signal. The artifact removal may cause loss of important information from the signal. Therefore, it is required to remove the motion artifacts and simultaneously preserve the desired information, which makes EEG artifact removal a vital task. Enhanced Empirical Mode Decomposition (EEMD) is the most widespread method used for artifact removal, as it is a data-driven based feature extraction method. In this research work the efficiency of various EEMD with different interpolation based artifact removal method have been compared. The EEMD is used to convert input single channel EEG signal to a multichannel signal, and in order to remove the randomness of motion artifact, CCA and DWT filtering were used successively. The performance of different interpolation based artifact removal methods have evaluated and results indicate that the proposed algorithm is suitable for use as a supplement to algorithms currently in use as it offers improvements in DSNR and various other performance parameters.},
doi={10.1007/s11277-017-4846-3}
}
@article{FREDENHAGEN1999182,
title = {On the Construction of Optimal Monotone Cubic Spline Interpolations},
journal = {Journal of Approximation Theory},
volume = {96},
number = {2},
pages = {182-201},
year = {1999},
issn = {0021-9045},
doi = {10.1006/jath.1998.3247},
url = {https://www.sciencedirect.com/science/article/pii/S0021904598932476},
author = {{Fredenhagen, S., Oberle, H. J., & Opfer, G.}},
abstract = {In this paper we derive necessary optimality conditions for an interpolating spline function which minimizes the Holladay approximation of the energy functional and which stays monotone if the given interpolation data are monotone. To this end optimal control theory for state-restricted optimal control problems is applied. The necessary conditions yield a complete characterization of the optimal spline. In the case of two or three interpolation knots, which we call thelocalcase, the optimality conditions are treated analytically. They reduce to polynomial equations which can very easily be solved numerically. These results are used for the construction of a numerical algorithm for the optimal monotone spline in the general (global) case via Newton's method. Here, the local optimal spline serves as a favourable initial estimation for the additional grid points of the optimal spline. Some numerical examples are presented which are constructed by FORTRAN and MATLAB programs.}
}
//
Newton
/////////////////////////////////
@Inbook{Rutishauser1990,
author={{Rutishauser, H.}},
editor={{Gutknecht, M.}},
title="Interpolation",
bookTitle="Lectures on Numerical Mathematics",
year="1990",
publisher="Birkhäuser Boston",
address="Boston, MA",
pages="128--174",
abstract="Interpolation is the art of reading between the lines of a mathematical table. It can be used to express nonelementary functions approximately in terms of the four basic arithmetic operations, thus making them accessible to computer evaluation.",
isbn="978-1-4612-3468-5",
doi="10.1007/978-1-4612-3468-5_6",
url="https://doi.org/10.1007/978-1-4612-3468-5_6"
}
@Inbook{Scherer2010,
author={{Scherer, P. O.J.}},
title="Interpolation",
bookTitle="Computational Physics: Simulation of Classical and Quantum Systems",
year="2010",
publisher="Springer",
address="Berlin, Heidelberg",
pages="15--27",
abstract="Experiments usually produce a discrete set of data points. If additional data points are needed, for instance, to draw a continuous curve or to change the sampling frequency of audio or video signals, interpolation methods are necessary. But interpolation is also helpful to develop more sophisticated numerical methods for the calculation of numerical derivatives or integrals. Polynomial interpolation is discussed in large detail together with its drawbacks. The methods by Lagrange and Newton are discussed, as well as the Neville method, which allow efficient determination and evaluation of the interpolating polynomial. For interpolation over a larger range, larger number of data spline interpolation is very useful which does not show the oscillatory behavior characteristic of polynomial interpolation. In a computer experiment both these approaches are compared. Multivariate interpolation is a necessary tool to process multidimensional data sets, for instance, for image processing. A computer experiment compares bilinear interpolation and bicubic spline interpolation.",
isbn="978-3-642-13990-1",
doi="10.1007/978-3-642-13990-1_2",
url="https://doi.org/10.1007/978-3-642-13990-1_2"
}
@article{https://doi.org/10.1155/2020/9020541,
author = {{Zou, L., Song, L., Wang, X., Weise, T., Chen, Y., & Zhang, C.}},
title = {A New Approach to Newton-Type Polynomial Interpolation with Parameters},
journal = {Mathematical Problems in Engineering},
volume = {2020},
number = {1},
pages = {9020541},
doi = {10.1155/2020/9020541},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2020/9020541},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2020/9020541},
abstract = {Newton’s interpolation is a classical polynomial interpolation approach and plays a significant role in numerical analysis and image processing. The interpolation function of most classical approaches is unique to the given data. In this paper, univariate and bivariate parameterized Newton-type polynomial interpolation methods are introduced. In order to express the divided differences tables neatly, the multiplicity of the points can be adjusted by introducing new parameters. Our new polynomial interpolation can be constructed only based on divided differences with one or multiple parameters which satisfy the interpolation conditions. We discuss the interpolation algorithm, theorem, dual interpolation, and information matrix algorithm. Since the proposed novel interpolation functions are parametric, they are not unique to the interpolation data. Therefore, its value in the interpolant region can be adjusted under unaltered interpolant data through the parameter values. Our parameterized Newton-type polynomial interpolating functions have a simple and explicit mathematical representation, and the proposed algorithms are simple and easy to calculate. Various numerical examples are given to demonstrate the efficiency of our method.},
year = {2020}
}
@inbook{doi:10.1002/9781119604570.ch4,
author={{Epperson, J. F.}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781119604570},
title = {Interpolation and Approximation},
booktitle = {An Introduction to Numerical Methods and Analysis},
chapter = {4},
pages = {101-148},
doi = {10.1002/9781119604570.ch4},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119604570.ch4},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119604570.ch4},
year = {2021},
keywords = {divided differences, Hermite interpolation, interpolation error, inverse quadratic interpolation, Lagrange interpolation, Muller's method, Newton interpolation, piecewise polynomial interpolation, tension Splines},
abstract = {Summary This chapter presents the information of interpolation and approximation for solving problems of mathematical analysis. It contains exercises and solutions that present an introduction to key concepts, a calculus review, and an updated primer on Lagrange interpolation, Newton interpolation and divided differences, interpolation error, Muller's method and inverse quadratic interpolation, Hermite interpolation, piecewise polynomial interpolation, Splines, tension Splines, and least squares concepts in approximation. The chapter features new and updated material reflecting new trends and applications in numerical methods and analysis. It is the perfect for upper-level undergraduate students in mathematics, science, and engineering courses, as well as for courses in the social sciences, medicine, and business with numerical methods and analysis components.}
}
@article{tsao1977,
author={{Tsao, N.-K.}},
year={1977},
title={Newton interpolation is efficient for approximation of linear functionals},
journal={Numerische Mathematik},
page={115--122},
volume={29},
number={1},
abstract={The Newton interpolation approach is developed for approximation of linear functionals. It is shown that in numerical interpolation and numerical differentiation, the Newton interpolation approach is more efficient than solving the Vandermonde systems.},
doi={10.1007/BF01389317}
}
//
Matrix
/////////////////////////////////
@book{saff2025,
title={Matrix Fundamentals},
subtitle={From Equation Solving to Signal Processing},
author={{Saff, E. B., & Snider, A. D.}},
doi={10.1007/978-3-031-97222-5},
publisher={Springer},
address="Cham",
edition={2},
year={2025}
}
@book{gentle2024,
title={Matrix Algebra},
subtitle={Theory, Computations and Applications in Statistics},
author={{Gentle, J. E.}},
series={Springer Texts in Statistics},
doi={10.1007/978-3-031-42144-0},
publisher={Springer},
address="Cham",
year={2024},
edition={3}
}
@Inbook{Shores2018,
author={{Shores, T. S.}},
title="MATRIX ALGEBRA",
bookTitle="Applied Linear Algebra and Matrix Analysis",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="65--180",
abstract="In Chapter 1 we used matrices and vectors as simple storage devices. In this chapter matrices and vectors take on a life of their own. We develop the arithmetic of matrices and vectors. Much of what we do is motivated by a desire to extend the ideas of ordinary arithmetic to matrices.",
isbn="978-3-319-74748-4",
doi="10.1007/978-3-319-74748-4_2",
url="https://doi.org/10.1007/978-3-319-74748-4_2"
}
@Inbook{Karpfinger2022a,
author={{Karpfinger, C.}},
title="L R-Zerlegung einer Matrix",
bookTitle="H{\"o}here Mathematik in Rezepten: Begriffe, S{\"a}tze und zahlreiche Beispiele in kurzen Lerneinheiten",
year="2022",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="107--117",
abstract="Wir betrachten das Problem, zu einer invertierbaren Matrix {\$}{\$}A {\backslash}in {\backslash}mathbb {\{}R{\}}^{\{}n{\backslash}times n{\}}{\$}{\$}A∈Rn{\texttimes}nund einem Vektor {\$}{\$}b {\backslash}in {\backslash}mathbb {\{}R{\}}^n{\$}{\$}b∈Rneinen Vektor {\$}{\$}x {\backslash}in {\backslash}mathbb {\{}R{\}}^n{\$}{\$}x∈Rnmit {\$}{\$}A {\backslash}, x = b{\$}{\$}Ax=bzu bestimmen; kurz: Wir l{\"o}sen das lineare Gleichungssystem {\$}{\$}A x = b{\$}{\$}Ax=b. Formal erh{\"a}lt man die L{\"o}sung durch {\$}{\$}x = A^{\{}-1{\}} b{\$}{\$}x=A-1b. Aber die Berechnung von {\$}{\$}A^{\{}-1{\}}{\$}{\$}A-1ist bei einer gro{\ss}en Matrix A aufwendig. Die Cramer'sche Regel (siehe Rezept in Abschn. 12.3) ist aus numerischer Sicht zur Berechnung der L{\"o}sung x ungeeignet. Tats{\"a}chlich liefert das Gau{\ss}'sche Eliminationsverfahren, das wir auch in Kap. 9 zur h{\"a}ndischen L{\"o}sung eines LGS empfohlen haben, eine Zerlegung der Koeffizientenmatrix A, mit deren Hilfe es m{\"o}glich ist, ein Gleichungssystem der Form {\$}{\$}A {\backslash}, x = b{\$}{\$}Ax=bmit invertierbarem A zu l{\"o}sen. Diese sogenannte {\$}{\$}L{\backslash}, R{\$}{\$}LR-Zerlegung ist zudem numerisch gutartig. Gleichungssysteme mit bis zu etwa 10000 Zeilen und Unbekannten lassen sich auf diese Weise vorteilhaft l{\"o}sen. F{\"u}r gr{\"o}{\ss}ere Gleichungssysteme sind iterative L{\"o}sungsverfahren zu bevorzugen (siehe Kap. 71).",
isbn="978-3-662-63305-2",
doi="10.1007/978-3-662-63305-2_11",
url="https://doi.org/10.1007/978-3-662-63305-2_11"
}
@Inbook{Karpfinger2022b,
author={{Karpfinger, C.}},
title="Calculating with Matrices",
bookTitle="Calculus and Linear Algebra in Recipes: Terms, phrases and numerous examples in short learning units",
year="2022",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="87--100",
abstract="We have already used matrices to solve systems of linear equations: Matrices have been a helpful tool here to represent linear systems of equations economically and clearly. Matrices also serve as a tool in other, manifold ways. This is one reason to consider matrices in their own right, and to clearly illustrate and practice all kinds of manipulations that are possible with them: We will add, multiply, multiply, exponentiate, transpose, and invert matrices. But everything in order.",
isbn="978-3-662-65458-3",
doi="10.1007/978-3-662-65458-3_10",
url="https://doi.org/10.1007/978-3-662-65458-3_10"
}
@article{GRCAR2011163,
title = {How ordinary elimination became Gaussian elimination},
journal = {Historia Mathematica},
volume = {38},
number = {2},
pages = {163-218},
year = {2011},
issn = {0315-0860},
doi = {10.1016/j.hm.2010.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0315086010000376},
author = {{Grcar, J. F.}},
keywords = {Algebra before 1800, Gaussian elimination, Human computers, Least squares method, Mathematics education},
abstract = {Newton, in notes that he would rather not have seen published, described a process for solving simultaneous equations that later authors applied specifically to linear equations. This method — which Euler did not recommend, which Legendre called “ordinary,” and which Gauss called “common” — is now named after Gauss: “Gaussian” elimination. Gauss’s name became associated with elimination through the adoption, by professional computers, of a specialized notation that Gauss devised for his own least-squares calculations. The notation allowed elimination to be viewed as a sequence of arithmetic operations that were repeatedly optimized for hand computing and eventually were described by matrices.
Zusammenfassung
In Aufzeichnungen, die Newton lieber nicht der Veröffentlichung preisgegeben hätte, beschreibt er den Prozess für die Lösung von simultanen Gleichungen, den spätere Autoren speziell für lineare Gleichungen anwandten. Diese Methode — welche Euler nicht empfahl, welche Legendre “ordinaire” nannte, und welche Gauß “gewöhnlich” nannte — wird nun nach Gauß benannt: Gaußsches Eliminationsverfahren. Die Verbindung des Gaußschen Namens mit Elimination wurde dadurch hervorgebracht, dass professionelle Rechner eine Notation übernahmen, die Gauß speziell für seine eigenen Berechnungen der kleinsten Quadrate ersonnen hatte, welche zuließ, das Elimination als eine Sequenz von arithmetischen Rechenoperationen betrachtet wurde, die wiederholt für Handrechnungen optimisiert wurden und schließlich durch Matrizen beschrieben wurden.}
}