// F chi² //
//
@Inbook{Jolicoeur1999,
author={{Jolicoeur, P.}},
title="The distribution of $\chi$2 (chi squared)",
bookTitle="Introduction to Biometry",
year="1999",
publisher="Springer US",
address="Boston, MA",
pages="38--39",
abstract="The distribution of $\chi$2(chi squared) is a continuous and asymmetrical distribution which ranges from 0 to + ∞ and is followed by a sum of squares of independent standardized normal variates. The $\chi$2distribution and its application to frequency tables (chapter 15) were discovered by the British biometrician Karl Pearson (1857--1936), who was considered as the ``father of biometry'' and was one of the founders of the periodical Biometrika. The $\chi$2distribution is exact when hypotheses must be tested or confidence intervals must be determined (chapter 10) concerning the parametric variance $\sigma$2of a continuous variate X which follows a normal distribution (chapter 5). Moreover, the $\chi$2distribution may be used as an approximation in many cases, including Bartlett's test of the homogeneity of variance (section 12.7), tests of hypotheses concerning frequency tables (chapter 15), tests of goodness of fit (chapters 16, 17 and 18), the binomiality (chapter 17) and Poissonianity (chapter 18) tests, etc.. Paradoxically, the approximate utilizations of the $\chi$2distribution are perhaps better known than the exact ones.",
isbn="978-1-4615-4777-8",
doi="10.1007/978-1-4615-4777-8_8",
url="https://doi.org/10.1007/978-1-4615-4777-8_8"
}
@misc{das2025newmethodscomputegeneralized,
      title={New methods to compute the generalized chi-square distribution}, 
author={{Das, A.}},
year={2025},
eprint={2404.05062},
archivePrefix={arXiv},
primaryClass={stat.CO},
url={https://arxiv.org/abs/2404.05062}, 
}
@Inbook{Chattamvelli2021,
author={{Chattamvelli, R., & Shanmugam, R.}},
title={{F Distribution}},
bookTitle="Continuous Distributions in Engineering and the Applied Sciences -- Part II",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="227--234",
isbn="978-3-031-02435-1",
doi="10.1007/978-3-031-02435-1_7",
url="https://doi.org/10.1007/978-3-031-02435-1_7"
}
@Inbook{Herrmann1984,
author={{Herrmann, D.}},
title={{F-Verteilung}},
bookTitle="Wahrscheinlichkeitsrechnung und Statistik --- 30 BASIC-Programme",
year="1984",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="40--41",
abstract="Die letzte der drei wichtigsten Test-Verteilungen ist die F-Verteilung mit der Verteilungsfunktion Die Bedeutung der F-Verteilung liegt darin, da{\ss} mit zwei $\chi$2 -verteilten Gr{\"o}{\ss}en X2 und Y2 mit den Freiheitsgraden f1 bzw. f2F-verteilt ist. Die F-Verteilung hat somit zwei Freiheitsgrade. Sie ist eine sehr allgemeine Funktion, denn sie geht f{\"u}r f1 = 1, t = {\textsurd}F in die t-Verteilung f1 = 1, f2 = ∞, z = {\textsurd}F in die Normalverteilung {\"u}ber.",
isbn="978-3-322-96320-8",
doi="10.1007/978-3-322-96320-8_21",
url="https://doi.org/10.1007/978-3-322-96320-8_21"
}
@Inbook{Behnke2006,
author={{Behnke, J., & Behnke, N.}},
title="Verteilungen, die von der Standardnormalverteilung abgeleitet werden k{\"o}nnen",
bookTitle="Grundlagen der statistischen Datenanalyse: Eine Einf{\"u}hrung f{\"u}r Politikwissenschaftler",
year="2006",
publisher="VS Verlag f{\"u}r Sozialwissenschaften",
address="Wiesbaden",
pages="344--355",
abstract="Die Standardnormalverteilung ist ohne Zweifel die wichtigste und grundlegendste Verteilungsform in der Statistik. Sie ist von fundamentaler Bedeutung f{\"u}r die Sch{\"a}tzung von Parametern, aber auch zur Konstruktion von Teststatistiken. Nicht alle Teststatistiken aber k{\"o}nnen so konstruiert werden, dass sie normalverteilt sind. Die Teststatistiken von wichtigen statistischen Tests haben oft auch andere Verteilungsformen, die allerdings aus der Standardnormalverteilung abgeleitet werden k{\"o}nnen. Die wichtigsten dieser weiteren Verteilungen sind die so genannte Chi -Verteilung und die F-Verteilung. Die Chi2 -Verteilung wird unter anderem bei der Untersuchung von Zusammenh{\"a}ngen zwischen nominalskalierten Variablen eingesetzt, aber sie kann in einem wesentlich weiteren Sinn in vielen F{\"a}llen f{\"u}r den Vergleich einer theoretischen Verteilung mit einer empirischen Verteilung herangezogen werden. Die F-Verteilung spielt vor allem f{\"u}r den Vergleich von Varianzen eine herausragende Rolle. Schon bekannt ist uns au{\ss}erdem die T-Verteilung, die im Prinzip {\"u}berall dort Verwendung findet, wo auch die Standard-normalverteilung eingesetzt werden kann, wo aber die Standardabweichung der Grundgesamtheit erst mit Hilfe der Standardabweichung der Stichprobe gesch{\"a}tzt werden muss. Wir greifen die T-Verteilung in diesem Kapitel noch einmal kurz auf, um ihre Beziehung zur Standardnormalverteilung noch etwas genauer zu kl{\"a}ren.",
isbn="978-3-531-90003-2",
doi="10.1007/978-3-531-90003-2_26",
url="https://doi.org/10.1007/978-3-531-90003-2_26"
}
@Inbook{Stange1970,
author={{Stange, K.}},
title={{Die F-Verteilung}},
bookTitle="Angewandte Statistik: Erster Teil Eindimensionale Probleme",
year="1970",
publisher="Springer",
address="Berlin, Heidelberg",
pages="357--372",
abstract="Vorgelegt seien zwei Normalverteilungen 1 und 2, die beispielsweise zu zwei Erzeugungsvorg{\"a}ngen geh{\"o}ren, mit unbekannten Varianzen $\sigma$12und $\sigma$22. Man will entscheiden, ob beide Vorg{\"a}nge mit der gleichen Varianz f{\"u}r die Merkmalwerte ablaufen. Mit anderen Worten: Es soll die Hypothese $\sigma$12= $\sigma$22getestet werden. Verf{\"u}gbar ist aus jeder Verteilung eine Zufallsprobe der Gr{\"o}{\ss}e n1 bzw. n2 mit der Varianz s12bzw. s22. Als Pr{\"u}fgr{\"o}{\ss}e, mit der man die Entscheidung f{\"a}llt, w{\"a}hlt man das Verh{\"a}ltnis s12/s22der Stichprobenvarianzen.",
isbn="978-3-642-85602-0",
doi="10.1007/978-3-642-85602-0_11",
url="https://doi.org/10.1007/978-3-642-85602-0_11"
}
@Inbook{Hafner1992,
author={{Hafner, R.}},
title="Der Chi-Quadrat-Test",
bookTitle="Statistik: f{\"u}r Sozial- und Wirtschaftswissenschaftler",
year="1992",
publisher="Springer",
address="Vienna",
pages="164--170",
abstract="Die in den Kapiteln 11, 12 und 13 behandelten statistischen Hypothesen waren Parameterhypothesen in dem Sinne, da{\ss} Aussagen {\"u}ber einzelne Verteilungsparameter (z.B.:  gepr{\"u}ft werden sollten. H{\"a}ufig hat man aber allgemeinere Aussagen {\"u}ber die den Daten zugrundeliegende Verteilung zu testen. Man m{\"o}chte zum Beispiel wissen, ob die Annahme, da{\ss} die Daten normal-verteilt sind, haltbar ist, oder ob zwei Verteilungen deutlich oder kaum verschieden sind.",
isbn="978-3-7091-3420-7",
doi="10.1007/978-3-7091-3420-7_14",
url="https://doi.org/10.1007/978-3-7091-3420-7_14"
}
@article{https://doi.org/10.1002/cem.2734,
author = {{Brereton, R. G.}},
title = {{The F distribution and its relationship to the chi squared and t distributions}},
journal = {Journal of Chemometrics},
volume = {29},
number = {11},
pages = {582--586},
doi = {10.1002/cem.2734},
url = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.2734},
eprint = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/pdf/10.1002/cem.2734},
year = {2015}
}

@inbook{doi:https://doi.org/10.1002/9781118445112.stat05856,
author = {{Selvin, S.}},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781118445112},
title = {{F Distributions}},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {},
doi = {10.1002/9781118445112.stat05856},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat05856},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat05856},
year = {2014},
keywords = {-distribution, variance, probability distribution},
abstract = {Abstract The F-distribution is fundamental to much of statistical analysis, particularly the analysis of variance. The article entitled “F-Distribution” is a brief but complete description of this probability distribution. Its properties are first discussed, followed by a description of the relationship between the F-distribution and other common statistical distributions (e.g. beta, t-, chi-square, and binomial distributions). The article concludes with a typical application of this distribution to a small set of sampled observations.}
}
@article{CANAL2005803,
title = {A normal approximation for the chi-square distribution},
journal = {Computational Statistics & Data Analysis},
volume = {48},
number = {4},
pages = {803--808},
year = {2005},
issn = {0167-9473},
doi = {10.1016/j.csda.2004.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167947304001069},
author = {{Canal, L.}},
keywords = {Chi-square distribution, Maximum absolute error, Normal approximation, Power transformation},
abstract = {An accurate normal approximation for the cumulative distribution function of the chi-square distribution with n degrees of freedom is proposed. This considers a linear combination of appropriate fractional powers of chi-square. Numerical results show that the maximum absolute error associated with the new transformation is substantially lower than that found for other power transformations of a chi-square random variable for all the degrees of freedom considered (1⩽n⩽1000).}
}
@article{a71e7dcf-67b3-3b81-a0c2-17153341fab1,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2335780},
 abstract = {The distribution of the χ2 test under the null hypothesis is studied, when the parameters are estimated by the method of moments. A general formula, applicable also to other situations, is given. Three examples are studied in more detail and numerical results are given, indicating how unsafe it can be to use a χ2 distribution with a number of degrees of freedom smaller than or equal to the number of cells.},
 author = {{Molinari, L.}},
 journal = {Biometrika},
 number = {1},
 pages = {115--121},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Distribution of the Chi-Squared Test in Nonstandard Situations},
 urldate = {2026-01-13},
 volume = {64},
 year = {1977}
}
@article{b3bc6554-760a-35aa-92dd-1ef8068d343e,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2985223},
 abstract = {A procedure for interpolating in tables of the F-distribution is proposed. This procedure is already known by many statisticians, although its original source has been forgotten. A table gives an upper bound for the relative error in the interpolation between some tabulated values. An example is given.},
 author = {{Zinger, A.}},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {51--53},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {{On Interpolation in Tables of the F-Distribution}},
 urldate = {2026-01-13},
 volume = {13},
 year = {1964}
}
@article{b71582f9-54b0-3efd-95a7-a60d1005dd3b,
 ISSN = {00278424, 10916490},
 URL = {http://www.jstor.org/stable/86022},
 author = {{Wilson, E. B., & Hilferty, M. M.}},
 journal = {Proceedings of the National Academy of Sciences of the United States of America},
 number = {12},
 pages = {684--688},
 publisher = {National Academy of Sciences},
 title = {The Distribution of Chi-square},
 urldate = {2026-01-13},
 volume = {17},
 year = {1931}
}
@article{c9bb1879-be2e-3251-9489-81b3cdab8ab2,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2282687},
 abstract = {A relation is derived between the percentile points of a t-distribution with n degrees of freedom and those of an F-distribution with n and n degrees of freedom. In effect, the t-percentiles can be obtained by a simple transformation from the "diagonal" entries of an F-table.},
 author = {{Cacoullos, T.}},
 journal = {Journal of the American Statistical Association},
 number = {310},
 pages = {528--531},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {{A Relation Between t and F-Distributions}},
 urldate = {2026-01-13},
 volume = {60},
 year = {1965}
}
@article{Saunders_Moran_1978, 
title={{On the quantiles of the gamma and F distributions}}, 
volume={15}, 
DOI={10.2307/3213414}, 
number={2}, 
journal={Journal of Applied Probability}, author={{Saunders, I. W., & Moran, P. A. P.}}, year={1978}, 
pages={426--432}
}
@article{e800b171-dd42-3678-b651-8375c1fde110,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2332542},
 author = {{Patnaik, P. B.}},
 journal = {Biometrika},
 number = {1/2},
 pages = {202--232},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {{The Non-Central χ2- and F-Distribution and their Applications}},
 urldate = {2026-01-13},
 volume = {36},
 year = {1949}
}
// Fisher Z //
//
@article{CARBONELL2009780,
title = {{On the Fisher’s Z transformation of correlation random fields}},
journal = {Statistics & Probability Letters},
volume = {79},
number = {6},
pages = {780--788},
year = {2009},
issn = {0167-7152},
doi = {10.1016/j.spl.2008.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167715208005154},
author = {{Carbonell, F., Worsley, K. J., & Trujillo-Barreto, N. J.}},
abstract = {One of the most interesting problems studied in Random Field Theory (RFT) is to approximate the distribution of the maximum of a random field. This problem usually appears in a general hypothesis testing framework, where the statistics of interest are the maximum of a random field of a known distribution. In this paper, we use the RFT approach to compare two independent correlation random fields, R1 and R2. Our statistics of interest are the maximum of a random field G, resulting from the difference between the Fisher’s Z transformation of R1 and R2, respectively. The Fisher’s Z transformation guarantees a Gaussian distribution at each point of G but, unfortunately, G is not transformed into a Gaussian random field. Hence, standard results of RFT for Gaussian random fields are not longer available for G. We show here that the distribution of the maximum of G can still be approximated by the distribution of the maximum of a Gaussian random field, provided there is some correction by its spatial smoothness. Indeed, we present a general setting to obtain this correction. This is done by allowing different smoothness parameters for the components of G. Finally, the performance of our method is illustrated by means of both numerical simulations and real Electroencephalography data, recorded during a face recognition experimental paradigm.}
}
@article{67d2aa04-e169-3e9d-9c2d-31d37c595ef4,
 ISSN = {00220973, 19400683},
 URL = {http://www.jstor.org/stable/20156574},
 author = {{Hjelm, H. F., & Norris, R. C.}},
 journal = {The Journal of Experimental Education},
 number = {3},
 pages = {269--277},
 publisher = {Taylor & Francis, Ltd.},
 title = {{Empirical Study of the Efficacy of Fisher's Z-Transformation}},
 urldate = {2026-01-13},
 volume = {30},
 year = {1962}
}
@article{Bond_Richardson_2004, 
title={{Seeing the Fisher Z-transformation}}, volume={69}, 
DOI={10.1007/BF02295945}, 
number={2}, 
journal={Psychometrika}, 
author={{Bond, C. F., & Richardson, K.}}, 
year={2004}, 
pages={291–303}
}
@article{Mendoza_1993, 
title={Fisher Transformations for Correlations Corrected for Selection and Missing Data}, volume={58}, 
DOI={10.1007/BF02294830}, 
number={4}, 
journal={Psychometrika}, 
author={{Mendoza, J. L.}}, 
year={1993}, 
pages={601–615}
}
@article{fcdfa5ac-5e28-3f56-8f5f-82b584b870a3,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2332676},
 author = {{David, F. N.}},
 journal = {Biometrika},
 number = {3/4},
 pages = {394--403},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {{The Moments of the z and F Distributions}},
 urldate = {2026-01-13},
 volume = {36},
 year = {1949}
}
@article{Yang2013,
author = {{Yang, Z., Duan, Z., Wang, J., Wang, T., Song, Y., & Zhang, J.}},
year={2013},
title = {{Quadratic radical function better than fisher Z transformation}},
journal = {Transactions of Tianjin University},
pages =  {381--384},
volume = {19},
volume = {5},
abstract={A new explicit quadratic radical function is found by numerical experiments, which is simpler and has only 70.778% of the maximal distance error compared with the Fisher z transformation. Furthermore, a piecewise function is constructed for the standard normal distribution: if the independent variable falls in the interval (−1.519, 1.519), the proposed function is employed; otherwise, the Fisher z transformation is used. Compared with the Fisher z transformation, this piecewise function has only 38.206% of the total error. The new function is more exact to estimate the confidence intervals of Pearson product moment correlation coefficient and Dickinson best weights for the linear combination of forecasts},
doi={10.1007/s12209-013-1978-8}
}
 
