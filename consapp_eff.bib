@book{Cohen1977,
    author={{Cohen, J.}},
    year={1977},
    title={Statistical Power Analysis for the Behavioral Science},
    address={Amsterdam},
    publisher={Elsevier Academic Press},
    ISBN={978-0-12-179060-8},
    url={https://doi.org/10.1016/C2013-0-10517-X}
}
@book{Cohen1988,
    author={{Cohen, J.}},
    year={1988},
    title={Statistical Power Analysis for the Behavioral Science},
    edition={2},
    address={Hillsdale, NJ},
    publisher={Lawrence Erlbaum Associates}, 
    doi={10.4324/9780203771587}
}
@article{Cohen1992,
    author={{Cohen, J.}},
    year={1992},
    title={A power primer},
    journal={Psychological Bulletin},
    volume={112},
    number={1},
    pages={155--159},
    url={https://doi.org/10.1037/0033-2909.112.1.155}
}
@Inbook{Janczyk2013,
author={{Janczyk, M., & Pfister, R.}},
title="Fehlertypen, Effektst{\"a}rken und Power",
bookTitle="Inferenzstatistik verstehen: Von A wie Signifikanztest bis Z wie Konfidenzintervall",
year="2013",
publisher="Springer",
address="Berlin, Heidelberg",
pages="77--90",
abstract="Obwohl signifikante Ergebnisse oft gew{\"u}nscht werden, sagt -- wie in diesem Kapitel aufgezeigt wird -- die blo{\ss}e Signifikanz eines Tests nichts {\"u}ber die St{\"a}rke eines Effekts aus, und man kann sich berechtigt fragen: Bedeutet statistische Signifikanz auch immer ˝inhhaltliche Relevanz˝ bzw. ˝praktische Bedeutsamkeit˝? In diesem Kapitel werden diejenigen Konzepte eingef{\"u}hrt, die zur Beantwortung dieser Frage ben{\"o}tigt werden, und begonnen wird dabei mit einer systematischen Betrachtung statistischer Entscheidungen. Im Anschluss wird Cohen's d als Ma{\ss} der Effektst{\"a}rke f{\"u}r die bisher behandelten t-Tests betrachtet. Das Konzept der Effektst{\"a}rke f{\"u}hrt schlie{\ss}lich zur Power (Testst{\"a}rke) eines Signifikanztests und der Frage nach dem optimalen Stichprobenumfang.",
isbn="978-3-642-34825-9",
doi="10.1007/978-3-642-34825-9_7",
url="https://doi.org/10.1007/978-3-642-34825-9_7"
}
@article{Brandmaier2025,
author ={{Brandmaier, A. M.}},
title ={Stop Calling Cohen's d an Effect Size},
journal ={PsyArXiv},
year ={2025},
doi={10.31234/osf.io/n9ta7_v1}
}
@article{Fritz2012,
author ={{Fritz, C. O., Morris, P. E., & Richler, J. J.}},
year ={2012},
title ={Effect size estimates: Current use, calculations, and interpretation},
journal ={Journal of Experimental Psychology: General},
volume={141},
number={1},
pages={2--18},
doi={10.1037/a0024338}
}
@ARTICLE{10.3389/fpsyg.2019.00813,
AUTHOR={{Schäfer, T., & Schwarz, M. A.}},
TITLE={The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases},
JOURNAL={Frontiers in Psychology},
VOLUME={Volume 10 - 2019},
YEAR={2019},
URL={https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.00813},
DOI={10.3389/fpsyg.2019.00813},
ISSN={1664-1078},
ABSTRACT={Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes—when is an effect small, medium, or large?—has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = .36) were much larger than effects from the latter (median r = .16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.}
}
@article{BOWRING2021117477,
title = {Confidence Sets for Cohen’s d effect size images},
journal = {NeuroImage},
volume = {226},
pages = {117477},
year = {2021},
issn = {1053-8119},
doi = {10.1016/j.neuroimage.2020.117477},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920309629},
author = {{Bowring, A., Telschow, F. J. E., Schwartzman, A., & Nichols, T. E.}},
keywords = {Confidence sets, fMRI, Task fmri, Cohen’s , Effect sizes},
abstract = {Current statistical inference methods for task-fMRI suffer from two fundamental limitations. First, the focus is solely on detection of non-zero signal or signal change, a problem that is exacerbated for large scale studies (e.g. UK Biobank, N=40,000+) where the ‘null hypothesis fallacy’ causes even trivial effects to be determined as significant. Second, for any sample size, widely used cluster inference methods only indicate regions where a null hypothesis can be rejected, without providing any notion of spatial uncertainty about the activation. In this work, we address these issues by developing spatial Confidence Sets (CSs) on clusters found in thresholded Cohen’s d effect size images. We produce an upper and lower CS to make confidence statements about brain regions where Cohen’s d effect sizes have exceeded and fallen short of a non-zero threshold, respectively. The CSs convey information about the magnitude and reliability of effect sizes that is usually given separately in a t-statistic and effect estimate map. We expand the theory developed in our previous work on CSs for %BOLD change effect maps (Bowring et al., 2019) using recent results from the bootstrapping literature. By assessing the empirical coverage with 2D and 3D Monte Carlo simulations resembling fMRI data, we find our method is accurate in sample sizes as low as N=60. We compute Cohen’s d CSs for the Human Connectome Project working memory task-fMRI data, illustrating the brain regions with a reliable Cohen’s d response for a given threshold. By comparing the CSs with results obtained from a traditional statistical voxelwise inference, we highlight the improvement in activation localization that can be gained with the Confidence Sets.}
}
@article{MILLER2011144,
title = {Exact method for computing absolute percent change in a dichotomous outcome from meta-analytic effect size: Improving impact and cost-outcome estimates},
journal = {Value in Health},
volume = {14},
number = {1},
pages = {144--151},
year = {2011},
issn = {1098-3015},
doi = {10.1016/j.jval.2010.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S1098301510000148},
author = {{Miller, T. R., Hendrie, D., & Derzon, J.}},
keywords = {Absolute change, BESD, Effect size, Meta-analysis},
abstract = {Objectives
Meta-analyses typically compute a treatment effect size (Cohen's d), which is readily converted to another common measure, the binomial effect size display (BESD). BESD is the correlation coefficient and represents a percentage difference in outcome attributable to an intervention. Both d and BESD are in arbitrary units; neither measures the absolute change resulting from intervention. The method used to estimate absolute change from BESD assumes both a 50-50 split of the outcome and a balanced design. Consequently, inaccurate assumptions underpin most meta-analytic estimates of the gain resulting from an intervention (and of its cost effectiveness). This article develops an exact formula without these assumptions.
Methods
The formula is developed algebraically from 1) the formula for the correlation coefficient represented as a 2-by-2 contingency table constructed from the relative size of the treatment and control groups and the percentage of people who would have the condition absent intervention, and 2) the BESD correlation coefficient formula showing change in success probability with treatment.
Results
Simulation reveals that BESD only approximates the reduction in the outcome an intervention might well achieve when the problem outcome occurs in 35%-65% of cases. For less common outcomes, BESD substantially overestimates the impact of an intervention. Even when BESD accurately estimates the likely percentage change in outcome, it paints a misleading picture of the proportion of cases that will achieve a positive outcome.
Conclusion
It is time to retire BESD. Our equations can also guide effect size estimation from difficult articles.}
}
@article{abb9e70f-419e-3a38-8754-462a7041475a,
 ISSN = {00220973, 19400683},
 URL = {https://www.jstor.org/stable/26594399},
 abstract = {Given the long history of discussion of issues surrounding statistical testing and effect size indices and various attempts by the American Psychological Association and by the American Educational Research Association to encourage the reporting of effect size, most journals in education and psychology have witnessed an increase in effect size reporting since 1999. Yet, effect size was often reported in three indices, namely, the unadjusted R², Cohen’s d, and η² with a simple labeling of small, medium, or large, according to Cohen’s (1969) criteria. In this article, the authors present several alternatives to Cohen’s d to help researchers conceptualize effect size beyond standardized mean differences for between-subject designs with two groups. The alternative effect size estimators are organized into a typology and are empirically contrasted with Cohen’s d in terms of purposes, usages, statistical properties, interpretability, and the potential for meta-analysis. Several sound alternatives are identified to supplement the reporting of Cohen’s d. The article concludes with a discussion of the choice of standardizers, the importance of assumptions, and the possibility of extending sound alternative effect size indices to other research contexts.},
 author = {{Peng, C.-Y. J., & Chen, L.-T.}},
 journal = {The Journal of Experimental Education},
 number = {1},
 pages = {22--50},
 publisher = {Taylor & Francis, Ltd.},
 title = {Beyond Cohen’s d: Alternative Effect Size Measures for Between-Subject Designs},
 urldate = {2026-01-14},
 volume = {82},
 year = {2014}
}
@article{d04b907b-1c2b-329b-902f-ade87a9fd109,
 ISSN = {00220973, 19400683},
 URL = {http://www.jstor.org/stable/20157436},
 abstract = {Reporting effect size plays an integral role in educational and psychological research and is required by many journals. Certainly, the best-known measure of effect size is Cohen's d, which represents a substantial improvement over using p values. But Cohen's d is known to suffer from some fundamental concerns. The author's goal was to illustrate some graphical methods aimed at addressing those concerns. These methods can be applied in a wide range of situations, including situations in which the Wilcoxon-Mann-Whitney test is used.},
 author = {{Wilcox, R. R.}},
 journal = {The Journal of Experimental Education},
 number = {4},
 pages = {353--367},
 publisher = {Taylor & Francis, Ltd.},
 title = {Graphical Methods for Assessing Effect Size: Some Alternatives to Cohen's d},
 urldate = {2026-01-14},
 volume = {74},
 year = {2006}
}
@article{Groß2024,
author = {{Groß, J., & Möller, A.}},
year = {2024},
title = {Some additional remarks on statistical properties of Cohen’s d in the presence of covariates},
journal = {Statistical Papers},
pages = {3971--3979},
volume = {65},
number = {6},
abstract = {The size of the effect of the difference in two groups with respect to a variable of interest may be estimated by the classical Cohen’s d. A recently proposed generalized estimator allows conditioning on further independent variables within the framework of a linear regression model. In this note, it is demonstrated how unbiased estimation of the effect size parameter together with a corresponding standard error may be obtained based on the non-central t distribution. The portrayed estimator may be considered as a natural generalization of the unbiased Hedges’ g. In addition, confidence interval estimation for the unknown parameter is demonstrated by applying the so-called inversion confidence interval principle. The regarded properties collapse to already known ones in case of absence of any additional independent variables. The stated remarks are illustrated with a publicly available data set.},
doi = {10.1007/s00362-023-01527-9}
}
@article{Groß2023,
author = {{Groß, J., & Möller, A.}},
year = {2023},
title = {A Note on Cohen’s d From a Partitioned Linear Regression Model},
journal = {Journal of Statistical Theory and Practice},
pages = {22},
volume = {17},
number = {2},
abstract = {In this note, we introduce a generalized formula for Cohen’s d under the presence of additional independent variables, providing a measure for the size of a possible effect concerning the size of a difference location effect of a variable in two groups. This is done by employing the so-called Frisch–Waugh–Lovell theorem in a partitioned linear regression model. The generalization is motivated by demonstrating the relationship to appropriate t and F statistics. Our discussion is further illustrated by inference about a publicly available data set.},
doi = {10.1007/s42519-023-00323-w}
}
@article{Li2016,
author={{Li, J. C.-H.}},
year = {2016},
title = {Effect size measures in a two-independent-samples case with nonnormal and nonhomogeneous data},
journal = {Behavior Research Methods},
pages = {1560--1574},
volume = {48},
number = {4},
abstract = {In psychological science, the “new statistics” refer to the new statistical practices that focus on effect size (ES) evaluation instead of conventional null-hypothesis significance testing (Cumming, Psychological Science, 25, 7–29, 2014). In a two-independent-samples scenario, Cohen’s (1988) standardized mean difference (d) is the most popular ES, but its accuracy relies on two assumptions: normality and homogeneity of variances. Five other ESs—the unscaled robust d (dr*; Hogarty & Kromrey, 2001), scaled robust d (dr; Algina, Keselman, & Penfield, Psychological Methods, 10, 317–328, 2005), point-biserial correlation (rpb; McGrath & Meyer, Psychological Methods, 11, 386–401, 2006), common-language ES (CL; Cliff, Psychological Bulletin, 114, 494–509, 1993), and nonparametric estimator for CL (Aw; Ruscio, Psychological Methods, 13, 19–30, 2008)—may be robust to violations of these assumptions, but no study has systematically evaluated their performance. Thus, in this simulation study the performance of these six ESs was examined across five factors: data distribution, sample, base rate, variance ratio, and sample size. The results showed that Awand drwere generally robust to these violations, and Awslightly outperformed dr. Implications for the use of Awand drin real-world research are discussed.},
doi = {10.3758/s13428-015-0667-z}
}
