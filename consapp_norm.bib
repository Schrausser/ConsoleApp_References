@Inbook{Leaver1974,
author={{Leaver, R. H.,  & Thomas, T. R.}},
title="Normal Distribution",
bookTitle="Analysis and Presentation of Experimental Results",
year="1974",
publisher="Macmillan Education UK",
address="London",
pages="30--37",
abstract="There are many different frequency distributions in statistics, some of which we shall meet later in the book, but one in particular applies to the theory of errors. This is the normal or Gaussian distribution. The word `normal' is here used in its original sense of `ideal', for it was believed for a long time that this distribution was a reflection of a universal and fundamental law of nature. If certain assumptions are made it can be `proved' that the accumulation of a large number of small independent random deviations follows a normal law, but belief in the rigour of this proof is no longer widespread. There is a well-known quotation on the subject: `Everybody believes in the Gaussian law of errors; the experimenters because they think it will be proved by mathematics; and the mathematicians because they believe it has been established by observation.' However, for our present purposes we can assume that the Gaussian distribution is the best possible representation of the variation of small random errors.",
isbn="978-1-349-01942-7",
doi="10.1007/978-1-349-01942-7_4",
url="https://doi.org/10.1007/978-1-349-01942-7_4"
}
@Inbook{Mulholland1968a,
author={{Mulholland, H., & Jones, C. R.}},
title="Elementary Probability",
booktitle="Fundamentals of Statistics",
year="1968",
publisher="Springer US",
address="Boston, MA",
pages="32--58",
abstract="The phrases `it is probable that', `it is very likely that', `there is virtually no chance' are often used in conversation. They are used to describe events which could possibly happen. They represent our estimates of the probability of the events happening. They are sometimes based on personal feelings, but more often they are based on relative frequency, i.e. the proportion of times such an event has happened previously. These subjective or intuitive ideas of probability are basic and fundamental in life and they give rise to two definitions of probability which are now discussed.",
isbn="978-1-4899-6507-3",
doi="10.1007/978-1-4899-6507-3_3",
url="https://doi.org/10.1007/978-1-4899-6507-3_3"
}
@Inbook{Mulholland1968b,
author={{Mulholland, H., & Jones, C. R.}},
title="The Normal Distribution",
booktitle="Fundamentals of Statistics",
year="1968",
publisher="Springer US",
address="Boston, MA",
pages="122--138",
abstract="The normal distribution is probably the most widely studied distribution in statistics because :(a)Distributions which are approximately normal are frequently encountered, e.g. most sets of random errors follow the normal distribution.(b)The normal distribution is important as a `limiting distribution', i.e. it can be used as an approximation to other distributions (see sections 8.7 and 8.8).(c)The normal distribution is easy to use.(d)It has been shown that the results obtained by assuming a non-normal population to be normally distributed are reasonably accurate when the departure from normality is not too severe.(e)The central limit theorem shows that the means of samples of size n from any population are approximately normally distributed. The approximation improves as n gets bigger.",
isbn="978-1-4899-6507-3",
doi="10.1007/978-1-4899-6507-3_8",
url="https://doi.org/10.1007/978-1-4899-6507-3_8"
}
@Inbook{Sachs1993,
author={{Sachs, L.}},
title="Normalverteilung",
bookTitle="Statistische Methoden: Planung und Auswertung",
year="1993",
publisher="Springer",
address="Berlin, Heidelberg",
pages="41--54",
abstract="Bedeutung der Normalverteilung: Die Normalverteilung ist ein wichtiges Modell: es beschreibt die Streuung von Me{\ss}werten um ihren Mittelwert. Me{\ss}fehler sind als zuf{\"a}llige Fehler [unvermeidbar, symmetrisch, meist klein] oft angen{\"a}hert normalverteilt. Der Praktiker mu{\ss} sich damit abfinden, da{\ss} es, streng genommen, in der Empirie keine Normalverteilung gibt. Indessen lassen sich viele mehr oder weniger symmetrisch-eingipflig verteilte Beobachtungen zumindest in ihrem mittleren Bereich als angen{\"a}hert normalverteilt auffassen.",
isbn="978-3-642-77717-2",
doi="10.1007/978-3-642-77717-2_4",
url="https://doi.org/10.1007/978-3-642-77717-2_4"
}
@Inbook{Saneii2024,
author={{Saneii, S. H., & Doosti, H.}},
title="Normal Distribution",
bookTitle="Practical Biostatistics for Medical and Health Sciences",
year="2024",
publisher="Springer",
address="Singapore",
pages="115--137",
isbn="978-981-97-3083-4",
doi="10.1007/978-981-97-3083-4_6",
url="https://doi.org/10.1007/978-981-97-3083-4_6"
}
@inbook{Wesolowski2018,
author = {{Wesolowski, B., & Musselwhite Thompson, D.}},
booktitle={The SAGE Encyclopedia of Educational Research, Measurement, and Evaluation},
editor={{Frey, B. B.}},
publisher={University of Kansas},
address={USA},
year = {2018},
title = {Normal Distribution},
doi = {10.4135/9781506326139.n476}
}
@article{Benzon2021,
author = {{Benzon, B.}},
year = {2021},
month = {07},
pages = {1--16},
title = {The normal distribution, an epistemological view},
volume = {2},
journal = {St open},
doi = {10.48188/so.2.6}
}
@article{https://doi.org/10.1002/cem.1382,
author = {{Tóth, G.}},
title = {Destruction of normal distribution in small samples by centering and scaling},
journal = {Journal of Chemometrics},
volume = {25},
number = {5},
pages = {247--253},
keywords = {centering, scaling, studentization, standardization, normal distribution},
doi = {10.1002/cem.1382},
url = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.1382},
eprint = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/pdf/10.1002/cem.1382},
abstract = {Abstract It is less emphasized in scientific literature that centering and scaling of data may drastically change the original distribution of the data for small samples. The destruction of the original distribution depends on the source of the estimation of the mean (centering) and the divisor (scaling) where the latter is connected to the spread of data. In our comparative study we focus on cases, where the sample is taken from normally distributed data; the means and the standard deviations are population or sample based. We discuss six cases in transforming the data or the sample means. Most of them are studied previously, but some of them have not been theoretically investigated. The transformed data follow normal distribution in three cases, if the scaling is performed with population standard deviation. In one case, the final distribution is related to β-distribution with astonishing density functions for N = 2 Dirac-delta, N = 3 Viking helmet like and N = 4 uniform distributions. Another case is the well-known t-distribution. For one of the transformed data, we were not able to identify the general form. Here we obtained only numerical results for 3 ≤ N. The effect of the transformations was tested on experimental data representing more or less normally distributed variables. We found that transformations using the sample standard deviation were significantly less normally distributed-like than the original data for small samples, but the other transformations enhanced the normal distribution-like feature. The results point out that centering and especially scaling require consideration for small samples up to questioning the reality of subsequent data evaluation processes where normal distribution is assumed. Copyright © 2011 John Wiley \& Sons, Ltd.},
year = {2011}
}
@article{https://doi.org/10.1002/cem.2655,
author = {{Brereton, R. G.}},
title = {The normal distribution},
journal = {Journal of Chemometrics},
volume = {28},
number = {11},
pages = {789--792},
doi = {10.1002/cem.2655},
year = {2014}
}
@article{WANG2011903,
title = {A simple characterization of Student’s distributions and normal distributions},
journal = {Statistics & Probability Letters},
volume = {81},
number = {8},
pages = {903--906},
year = {2011},
issn = {0167-7152},
doi = {10.1016/j.spl.2011.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167715211001283},
author = {{Wang, J.}},
keywords = {Characterization, Order statistics, Regressional property},
abstract = {Let X1:3≤X2:3≤X3:3 be the order statistics of the independent and identically distributed random variables X1, X2, and X3. Using the regressional property E[(aX2:3−X1:3)2|X2:3=x]=E[(X3:3−aX2:3)2|X2:3=x], we characterize some distributions, including Student’s tν distributions (ν>2) and normal distributions.}
}
@incollection{199847,
title = {Chapter 3 The normal distribution},
editor = {{Massart, D. L., Vandeginste, B. G. M., Buydens, L. M. C., De Jong, S., Lewi, P. J., & Smeyers-Verbeke, J.}},
series = {Data Handling in Science and Technology},
publisher = {Elsevier},
volume = {20},
pages = {47--72},
year = {1998},
booktitle = {Handbook of Chemometrics and Qualimetrics: Part A},
issn = {0922-3487},
doi = {10.1016/S0922-3487(97)80033-0},
url = {https://www.sciencedirect.com/science/article/pii/S0922348797800330},
abstract = {Publisher Summary
This chapter focuses on the concept of normal distribution. It also describes population parameters and their estimators. To summarize the characteristics of a distribution, its moments can be used. The best known probability distribution is the normal distribution. Distributions can be non-normal and procedures or tests are needed to detect this departure from normality. A graphical procedure is described in the chapter that permits to indicate whether a distribution is normal or not. The graphical procedure applied is called the “rankit procedure.” A bacteriological example provides a clue to the way to make non-normal distributions normal— namely, by transformation. The transformation carried out in the chapter is called the “log-transformation.” Log-normal distributions are frequently found in nature, particularly when the variable studied has a natural zero (such as weight, length, etc.). In this case, simple normality around the mean could include negative values.}
}
@article{doi:10.1093/bjps/axs046,
author = {{Lyon, A.}},
title = {Why are Normal Distributions Normal?},
journal = {The British Journal for the Philosophy of Science},
volume = {65},
number = {3},
pages = {621--649},
year = {2014},
doi = {10.1093/bjps/axs046},
abstract = {It is usually supposed that the central limit theorem explains why various quantities we find in nature are approximately normally distributed—people's heights, examination grades, snowflake sizes, and so on. This sort of explanation is found in many textbooks across the sciences, particularly in biology, economics, and sociology. Contrary to this received wisdom, I argue that in many cases we are not justified in claiming that the central limit theorem explains why a particular quantity is normally distributed, and that in some cases, we are actually wrong. 1 Introduction2 Normal Distributions and the Central Limit Theorem  2.1 Normal distributions  2.2 The central limit theorem  2.3 Terminology3 Explaining Normality  3.1 Loaves of bread  3.2 Varying variances and probability densities  3.3 Tensile strengths and problems with summation  3.4 Products of factors and log-normal distributions  3.5 Transforming factors and sub-factors  3.6 Transformations of quantities  3.7 Quantitative genetics  3.8 Inference to the best explanation4 Maximum Entropy Explanations5 Conclusion }
}
@article{2e210a69-a2e7-3fa7-96e2-b30efac9ed56,
 ISSN = {0025570X, 19300980},
 URL = {http://www.jstor.org/stable/27642916},
 author = {{Stahl, S.}},
 journal = {Mathematics Magazine},
 number = {2},
 pages = {96--113},
 publisher = {Mathematical Association of America},
 title = {The Evolution of the Normal Distribution},
 urldate = {2026-01-14},
 volume = {79},
 year = {2006}
}
@article{8ccd5f0b-f3d0-3894-a051-95d6e3d4ed98,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2333749},
 author = {{Breitenberger, E.}},
 journal = {Biometrika},
 number = {1/2},
 pages = {81--88},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Analogues of the Normal Distribution on the Circle and the Sphere},
 urldate = {2026-01-14},
 volume = {50},
 year = {1963}
}
@article{57fba293-9d42-3fd4-9780-f77567f507d7,
 ISSN = {09366784},
 URL = {http://www.jstor.org/stable/40986007},
 author = {{Thome, H.}},
 journal = {Historical Social Research / Historische Sozialforschung. Supplement},
 number = {3},
 pages = {1--275},
 publisher = {GESIS - Leibniz-Institute for the Social Sciences, Center for Historical Social Research},
title = {GRUNDKURS STATISTIK FÜR HISTORIKER TEIL II: INDUKTIVE STATISTIK UND REGRESSIONSANALYSE},
 urldate = {2026-01-14},
 year = {1990}
}
@article{adfd1cdd-be2e-33b1-bf94-693f4754181e,
 ISSN = {02664666, 14694360},
 URL = {http://www.jstor.org/stable/43948011},
 abstract = {We study the asymptotic covariance function of the sample mean and quantile, and derive a new and surprising characterization of the normal distribution: the asymptotic covariance between the sample mean and quantile is constant across all quantiles, if and only if the underlying distribution is normal. This is a powerful result and facilitates statistical inference. Utilizing this result, we develop a new omnibus test for normality based on the quantile-mean covariance process. Compared to existing normality tests, the proposed testing procedure has several important attractive features. Monte Carlo evidence shows that the proposed test possesses good finite sample properties. In addition to the formal test, we suggest a graphical procedure that is easy to implement and visualize in practice. Finally, we illustrate the use of the suggested techniques with an application to stock return datasets.},
 author = {{Bera, A. K., Galvao, A. F., Wang, L., & Xiao, Z.}},
 journal = {Econometric Theory},
 number = {5},
 pages = {1216--1252},
 publisher = {Cambridge University Press},
 title = {{A New Characterisazion Of The Normal Distribution And Test For Normality}},
 urldate = {2026-01-14},
 volume = {32},
 year = {2016}
}
@inbook{Storch_Zwiers_1999, 
place={Cambridge}, 
title={Normal Density and Cumulative Distribution Function}, 
booktitle={Statistical Analysis in Climate Research}, publisher={Cambridge University Press}, 
author={{von Storch, H., & Zwiers, F. W.}}, 
year={1999}, 
doi={10.1017/CBO9780511612336},
pages={419--420}
} 


